# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/library/datasets/saddle-sphere-ablations.ipynb.

# %% auto 0
__all__ = ['SadSpheres', 'metric']

# %% ../nbs/library/datasets/saddle-sphere-ablations.ipynb 5
from .core import get_adaptive_graph
from .datasets import rejection_sample_from_saddle, sphere, plane
from .self_evaluating_datasets import SelfEvaluatingDataset, metric
from fastcore.all import *
import xarray as xr
import inspect
import pandas as pd

import sklearn
import scipy.stats
import numpy as np
import matplotlib.pyplot as plt


class SadSpheres(SelfEvaluatingDataset):
    def __init__(self,
                 dimension:list = [2], # Dimension of saddles and spheres. If a list is supplied, computed sadspheres for each
                 num_pointclouds = 100, # num pointclouds per dataset per dimension
                 num_points = 2000, # num points per pointclouds
                 noise_level = 0, # from 0 to 1. 1 is all noise.
                 include_planes = False, # if True, includes randomly sampled planes as a sanity check.
                ):
        store_attr()
        if isinstance(dimension, int):
            dimension = [dimension]

        
        datalist = []
        names = []
        
        for d in dimension:
            for i in range(num_pointclouds):
                X_saddle, ks_saddle = rejection_sample_from_saddle(self.num_points, d)
                datalist.append(
                    { 'X' : X_saddle, 'ks' : ks_saddle, 'd':d}
                )
                names.append(f'{d}-Saddle')
                
                X_sphere, ks_sphere = sphere(self.num_points, d)
                datalist.append(
                    { 'X' : X_sphere, 'ks' : ks_sphere[0], 'd':d}
                )
                names.append(f'{d}-Sphere')

                if self.include_planes:
                    X_plane = plane(self.num_points, d) 
                    X_plane = np.hstack([X_plane, np.zeros(self.num_points)[:,None]])
                    datalist.append(
                        { 'X' : X_plane, 'ks' : 0, 'd':d}
                    )
                    names.append(f'{d}-Plane')

            super().__init__(
                datalist, names, ['ks']
            )

                
    def get_item(self, idx):
        return self.DS[idx].obj['X']

    def get_truth(self, result_name, idx):
        truth = self.DS[idx].obj['ks']
        return truth

    def plot_by_dimension(self):
        names = self.names
        labels = self.labels['ks']
        # Extract unique dimensions
        dimensions = sorted(set(name[0] for name in names))
        
        # Number of methods
        methods = list(labels.keys())
        
        # Create a grid of plots
        fig, axs = plt.subplots(len(dimensions), len(methods), figsize=(5 * len(methods), 5 * len(dimensions)))
        if len(dimensions) == 1 or len(methods) == 1:
            axs = np.array(axs).reshape(len(dimensions), len(methods))
        
        # Define a color map based on unique dataset names without dimension prefix
        unique_names = sorted(set(name[2:] for name in names))  # Strip dimension prefix
        colors = plt.cm.get_cmap('viridis', len(unique_names))
        name_to_color = {name: colors(i) for i, name in enumerate(unique_names)}  # Correct mapping
        
        # Plotting
        for i, dim in enumerate(dimensions):
            for j, method in enumerate(methods):
                # Filter data for the current dimension
                data = [(labels[method][k], name[2:]) for k, name in enumerate(names) if name.startswith(dim)]  # Use name without prefix
        
                # Define bins for histogram
                all_values = [val for val, _ in data]
                bins = np.linspace(min(all_values), max(all_values), 51)  # 50 bins
        
                # Create histogram for each dataset
                ax = axs[i, j]
                for label in unique_names:
                    dataset_values = [val for val, name in data if name == label]  # Compare without dimension prefix
                    if dataset_values:  # Check if there are any values for this dataset
                        counts, _ = np.histogram(dataset_values, bins=bins)
                        bin_centers = 0.5 * (bins[:-1] + bins[1:])
                        ax.bar(bin_centers, counts, width=(bins[1] - bins[0]) * 0.9, color=name_to_color[label], label=label, alpha=0.75)
        
                ax.set_title(f'Dimension {dim} - {method}')
                ax.set_xlabel('Value Range')
                ax.set_ylabel('Count')
                if i == 0 and j == 0:  # Add legend only to the first subplot for clarity
                    ax.legend(title='Dataset Name')
        
        # Adjust layout
        plt.tight_layout()
        plt.show()
        

    

    @metric
    def pearson_r(self, a, b):
        return scipy.stats.pearsonr(a,b)[0]

    @metric
    def sign_score(self, 
                   a, # prediction
                   b # target
                  ):
        a = np.array(a)
        b = np.array(b)
        # measures classification accuracy of signs
        # First, get rid of zeros in ground truth curvatures; we don't want to classify the planes.
        nz = np.nonzero(b)[0]
        nonzero_preds = np.sign(a[nz])
        nonzero_targets = np.sign(b[nz])
        acc = np.sum((nonzero_preds == nonzero_targets).astype(int))/len(nz)
        return acc
        
            


# %% ../nbs/library/datasets/saddle-sphere-ablations.ipynb 6
from .core import get_adaptive_graph
from .datasets import rejection_sample_from_saddle, sphere, plane
from fastcore.all import *
import xarray as xr
import inspect
import pandas as pd

import sklearn
import scipy.stats
import numpy as np
import matplotlib.pyplot as plt

def metric(func):
    setattr(func, 'tag', 'metric')
    return func

class SadSpheres():
    def __init__(self,
                 dimension:list = 2, # Dimension of saddles and spheres. If a list is supplied, computed sadspheres for each
                 num_pointclouds = 100, # num pointclouds to make in total
                 num_points = 2000, # num points per pointclouds
                 noise_level = 0, # from 0 to 1. 1 is all noise.
                 include_planes = False, # if True, includes randomly sampled planes as a sanity check.
                ):
        store_attr()
        self.DS = xr.Dataset()
        self.idx = -1
        self.dnum = 3 if self.include_planes else 2
        if isinstance(dimension, list):
            dimensions = dimension
        else:
            dimensions = [dimension]
        for d in dimensions:
            for i in range(num_pointclouds//self.dnum):
                X_saddle, ks_saddle = rejection_sample_from_saddle(self.num_points, self.dimension)
                self.DS[self.dnum*i] = xr.DataArray(X_saddle, dims=['n', 'd'], attrs={'ks':ks_saddle, 'name':'Saddle'})
                X_sphere, ks_sphere = sphere(self.num_points, self.dimension)
                self.DS[self.dnum*i+1] = xr.DataArray(X_sphere, dims=['n', 'd'], attrs={'ks':ks_sphere[0], 'name':'Sphere'})
                if self.include_planes:
                    X_plane = plane(self.num_points, self.dimension) 
                    X_plane = np.hstack([X_plane, np.zeros(self.num_points)[:,None]])
                    self.DS[self.dnum*i + 2] = xr.DataArray(X_plane, dims=['n', 'd'], attrs = {'ks':0, 'name':'Plane'})
    
    def __iter__(self):
        return self

    def __len__(self):
        return len(self.DS)

    def __next__(self):
        self.idx += 1
        if self.idx >= self.__len__():
            raise StopIteration
        result = self.DS[self.idx].to_numpy()
        return result

    def update(self,
               result,
               method_name='computed'):
        """
        Store the result of the curvature computation by passing the computed curvature of the center (first) point.
        """
        self.DS[self.idx].attrs[method_name] = result

    def compute_metrics(self):
        self._aggregate_labels()
        metrics = self._get_metrics()
        self.metric_table = {}
        for metric in metrics:
            self.metric_table[metric.__name__] = {}
            for method_name in self.method_names:
                self.metric_table[metric.__name__][method_name] = self.compute(metric=metric, method_name=method_name)
        self.metric_table = pd.DataFrame(self.metric_table)
            
    def compute(self, metric, method_name):
        # Overwrite this class with your logic. It implements the computation of a single metric for a single method
        return metric(self.labels[method_name], self.labels['ks'])
    

    def _aggregate_labels(self):
        self.method_names = list(self.DS.data_vars[0].attrs.keys())
        self.method_names.remove('name')
        self.labels = {}
        for m in self.method_names:
            self.labels[m] = np.array([self.DS.data_vars[i].attrs[m] for i in range(self.__len__())])

    def plot(self, title = None):
        if title is None: title = f"In dimension {self.dimension}"
        # for each computed method on this dataset, we plot the histogram of saddles vs spheres
        self._aggregate_labels()
        # get the idxs for each type of dataset
        dataset_names = [self.DS.data_vars[i].attrs['name'] for i in range(len(self.DS))]
        unique_names = list(set(dataset_names))
        idxs_by_name = {n: [i for i, name in enumerate(dataset_names) if name == n] for n in unique_names}        
        for m in self.method_names: 
            if m != 'ks' and m != 'name':
                for dname in unique_names:
                    plt.hist(self.labels[m][idxs_by_name[dname]], bins=50, label = dname, edgecolor='none', linewidth=5)
                plt.legend()
                plt.xlabel(m)
                plt.title(title)
                plt.show()

    def table(self):
        self.compute_metrics()
        return self.metric_table

    def _get_metrics(self):
        tagged_functions = []
        for name, member in inspect.getmembers(self, predicate=inspect.ismethod):
            if hasattr(member, 'tag') and getattr(member, 'tag') == 'metric':
                tagged_functions.append(member)
        return tagged_functions

    @metric
    def pearson_r(self, a, b):
        return scipy.stats.pearsonr(a,b)

    @metric
    def sign_score(self, 
                   a, # prediction
                   b # target
                  ):
        print(a, b)
        # measures classification accuracy of signs
        # First, get rid of zeros in ground truth curvatures; we don't want to classify the planes.
        nz = np.nonzero(b)[0]
        print(nz)
        nonzero_preds = np.sign(a[nz])
        nonzero_targets = np.sign(b[nz])
        acc = np.sum((nonzero_preds == nonzero_targets).astype(int))/len(nz)
        return acc
        
        
    
