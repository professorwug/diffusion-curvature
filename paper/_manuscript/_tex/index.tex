% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Diffusion Curvature for Fast, Point-wise, Noise-Resistant Geometric Featurization of Graphs and Pointclouds},
  pdfauthor={Kincaid MacDonald; Dhananjay Bhaskar; Yanlei Zhang; Ian Adelstein; Smita Krishnaswamy},
  pdfkeywords={Manifold Learning, Geometric Deep Learning, Graph
Curvature, Point Clouds},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Diffusion Curvature for Fast, Point-wise, Noise-Resistant
Geometric Featurization of Graphs and Pointclouds}
\author{Kincaid MacDonald \and Dhananjay Bhaskar \and Yanlei
Zhang \and Ian Adelstein \and Smita Krishnaswamy}
\date{2024-07-30}

\begin{document}
\maketitle
\begin{abstract}
For a number of years now work has been proceeding in order to bring to
perfection the crudely conceived idea of a machine that would not only
supply inverse reactive current for use in unilateral phase detractors,
but would also be capable of automatically synchronizing cardinal
grammeters. Such a machine is the ``Turbo-Encabulator.''
\end{abstract}


\newcommand\Kaly[1]{\textcolor{blue}{Kaly: [[[#1]]]}}

\section{Introduction}\label{introduction}

In this age of overwhelmingly abundant data, those racing to make sense
of it all have developed an appreciation for theories and metaphors from
the once hopelessly abstract mathematical fields of topology and
Riemannian geometry. Endowing noisy, sparse, high-dimensional real-world
data with geometric and topological features is helpful in two ways: it
empirically improves the performance of black-box machine learning
models; and, in cases like cellular differentiation, it also helps
\emph{us} interpret the data. What's more, by anchoring these models and
their inputs to well-established theories, it can bring analytical light
to a theoretically-impoverished field.

As a case in point, consider the emerging subfield of \emph{Topological
Data Analysis} (TDA). While graph neural networks struggle with problems
of over-smoothing and over-squashing, TDA endows graph or pointcloud
data with global topological features with provable guarantees. (LIST
SUCCESSES OF TDA.) Behind this success is \emph{persistence homology}, a
sort of `Rosetta Stone' between the mathematical theories of
analytically-specified manifolds, and the messy realities of actual
data. Persistence homology gives a framework for extracting the most
salient topological features from amidst the noise and sparsity of
real-world point clouds. (MORE SUCCESSES)

We await the geometric equivalent of this topological Renaissance. While
TDA can describe the shape of the data in its entirety, there's a need
for more local descriptions of data geometry. Forget the data as a
whole; what's happening around \emph{this point}, in \emph{this
neighborhood}? Topological analysis only assumes hierarchies of sets.
But for many types of point cloud data, e.g.~those arising from
time-varying systems like cellular development or neuronal activity,
notions of angle, distance, and volume have meaning. In other words, in
our field's famous `Manifold Hypothesis' we're often assuming a
\emph{Riemannian} manifold: samples from some continuous underlying
object that can be analyzed at infinitesimal scales.

However, translating the language of Riemannian geometry to the
discrete, sampled realm is troublesome. Nowhere is this more evident
than curvature. It's among the most basic of geometric properties,
underlying volumes, distances, and geodesics. Where TDA captures global
structure, curvature epitomizes the description of local structure.
Classical definitions, however, are hard to reconcile with reality; many
involve some infinitesimal shrinking, e.g.~by fitting osculating circles
of radius limiting to zero, or comparing volumes of balls converging to
a point. Other definitions involve even more complex objects, like
tangent bundles or local parameterizations of the manifold. In the
discrete, sampled realm, taking such a local limit is impossible -- one
can't `zoom in' past the known samples. Moreover, in contrast to
Riemannian manifolds, noisy samplings are \emph{least} trustworthy at
the local level -- which is precisely where we wish to measure geometric
information.

There have been several attempts to bridge curvature to the discrete
realm. (BRIEF DESCRIPTIONS OF PREVIOUS WORK) Ollivier-Ricci Curvature
and Forman Ricci curvature use optimal transport to define an edge-wise
Ricci curvature at any given scale. Hickok \& Blumberg define a point
cloud curvature by estimating graph volumes over time, and fitting a
quadratic to recover a scalar curvature. Sritharan et al.~and, recently,
Zhang et al.~(CITATIONS PLEASE) approximate tangent planes with
localized PCA, and track the movement of these planes to recover
sectional curvatures. These curvatures have been found to exceed the
WL-expressivity of graph neural networks, and when used to augment GNNs,
improve their performance (CITE BASTION). They have also been used to
study cellular differentiation, image data, etc\ldots{}

With so many existing curvature methods, why introduce another one?
Indeed, we came to this area interested only in using curvature to aid
our study of complex high-dimensional systems like neural networks' loss
landscapes. However, our experimentation with existing methods suggested
that, while existing methods have many elegant ideas, their techniques
are in practice unreliable. The optimal-transport curvatures, like
Ollivier Ricci, were designed for networks, and have poor resilience to
the noise ubiquitous in point-cloud data. Many techniques rely on
shortest-path distances, which are themselves easily corrupted by noise.
And all techniques have parameters which must be carefully tuned, and --
as our experiments found -- give not just noisy, but \emph{backwards}
results if mistuned. And how \emph{can} one tune these parameters, when
the ground truth curvature of interest is unknown? The success of TDA is
partially attributable to how easily a domain practitioner can apply it
to their problem of interest; persistence homology has few parameters
and tunes itself. The field of graph curvature has no such Rosetta Stone
for adapting geometric theory to the messy world of practice.

What might such a Rosetta Stone be? One candidate, with a long
track-record of producing robust geometric features from
high-dimensional, noisy data is \emph{diffusion geometry}. The basic
object in this framework is the random walk, or diffusion operator. When
iterated, this operator endows each point with a probability
distribution which is extremely robust to noise and spurious
connections. In this way, a point cloud can be lifted into a
\emph{statistical manifold}: the original, unreliable sampling
coordinates supplanted with diffusion probabilities. By manipulating
these diffusion probabilities in various ways, one can define several
varieties of `diffusion distances'. These distances can give rise to
low-dimensional embedding and visualization techniques, like PHATE and
HeatGeo. One can even bring in the Riemannian tool of pullback metrics
to endow the point cloud with a Riemannian metric. Unsurprisingly,
diffusion probabilities also contain information about curvature. In
(CITE ORIGINAL PAPER), we presented a preliminary technique for
extracting curvature from the `laziness' of the diffusion operator. This
fledgling method proved extremely robust to noise, though still somewhat
sensitive to its parameters, and being an unsigned correlate, was unable
to differentiate positive from negative curvature.

Here, we describe a significantly refined version of these ideas, which
we christen \emph{Diffusion Curvature}. It inherits the noise resilience
of diffusion geometry, and, in a style reminiscent of persistence
homology, further improves this resilience by incorporating many scales
of diffusion into one measure. The result is far easier for the domain
practitioner to use, having only one parameter, to which it is extremely
robust. Moreover, we prove connections between diffusion curvature and
the field's bulwark, Ollivier-Ricci curvature, positioning diffusion
curvature as a point-wise adaptation of Ollivier-Ricci curvature
especially suitable for pointcloud data. We also describe a new
technique for comparing diffusions across differently-sampled graphs and
point clouds, with which diffusion curvature can distinguish between
negative, flat, and positively curved spaces. Indeed, in our
experiments, diffusion curvature appears both the most resilient to
noise and the most capable of distinguishing negative from positive
curvatures in high dimensions. This motivates the hypothesis that
diffusion geometry is a uniquely capable `Rosetta Stone' for translating
Riemannian geometry into the sampled realm.

In sum, our contributions are: 1. \emph{Diffusion Curvature}, a fast,
noise-resistant and signed scalar curvature for point cloud data which
is also natively differentiable. 2. Theoretical connections between the
diffusion geometry underlying diffusion curvature and Ollivier-Ricci
curvature, which enables adaptation of Ollivier's results in metric
measure theory to other realms of diffusion geometry. 3. \emph{Diffusion
Trajectory Normalization}, a technique for comparing diffusions across
manifolds.

\section{Background}\label{background}

\subsection{Curvature in the Continuous
Setting}\label{curvature-in-the-continuous-setting}

There are many definitions of curvature on Riemannian manifolds. In this
work, we focus on discrete analogs to the \emph{Ricci} curvature
\(\text{Ric}(x,y)\), which we'll briefly motivate in the continuous
setting.

Imagine two spheres of equal radius centered at \(x\) and \(y\) in a
Riemannian manifold \(\mathcal{M}\). Intuitively, the Ricci curvature
between \(x\) and \(y\) measures the difference between the distance of
these midpoints, and the average distance between corresponding points
in each sphere. In an area of positive Ricci curvature, points in
spheres are, on average, closer than their midpoints; in negative Ricci
curvature, the points in the sphere are further, on average, than the
midpoints. This is formally described by first defining the
\emph{sectional curvature} as the contraction of length incurred by
parallel transport through a 2-plane of the manifold, then defining the
Ricci curvature as an average of the sectional curvatures between two
points. For the present work, we needn't reproduce the full definitions,
but we will recall some properties of manifolds with Ricci curvature
bounded from below by some \(k\).

First, recall the relation between curvature and volume. In spaces of
high positive curvature, the volume of a ball is \emph{smaller} than a
ball of the same radius in a flat space, and even smaller than a ball of
the same radius in a hyperbolic space. This is formally expressed by the
Bishop Gromov inequality ({``Bishop--{Gromov} Inequality''} 2021).

\begin{theorem}[Bishop-Gromov]\protect\hypertarget{thm-bishop-gromov}{}\label{thm-bishop-gromov}

Let \(\mathcal{M}\) be a complete \(d\)-dimensional Riemannian manifold
with \(Ric(x,y) > (d - 1)k\) for all \(x,y \in \mathcal{M}\) and
\(k \in \mathbb{R}\). Let \(M_{K}^d\) be the complete \(d\)-dimensional
simply connected space of constant sectional curvature \(k\). Denote by
\(B(x,k)\) the ball of radius \(k\) centered at \(x\). Then for any
\(x \in \mathcal{M}\) and \(x_{k} \in M_{K}^d\), the function

\[
\phi(r) = \frac{\operatorname{Vol}B(x,r)}{\operatorname{Vol}B(x_{k},r)}
\] is non-increasing on \((0,\infty)\).

\end{theorem}

This phenomenon of diminishing volume in positive curvature is related
to the convergence of geodesic rays. In the plane (or saddle), two
geodesic rays extending from the same point in different directions will
never intersect. But on the sphere, they \emph{will} meet again -- at
the opposite pole. This convergence constrains the maximum diameter a
space of positive curvature may have, as expressed in the Bonnet-Myers
Theorem (Ollivier 2009):

\begin{theorem}[Bonnet-Myers]\protect\hypertarget{thm-bonnet-myers}{}\label{thm-bonnet-myers}

Let \(X\) be an \(d\)-dimensional Riemannian manifold. Let
\(\inf \operatorname{Ric}(X)\) be the infimum of the Ricci curvature
\(\operatorname{Ric}(v, v)\) over all unit tangent vectors \(v\). Let
\(S^d \subset \mathbb{R}^{d+1}\) be the unit sphere of the same
dimension as \(X\). Then, if
\(\inf \operatorname{Ric}(X) \geqslant \inf \operatorname{Ric}\left(S^d\right)\)
then \(\operatorname{diam} X \leqslant \operatorname{diam} S^d\).

\end{theorem}

We'll encounter versions of Bishop-Gromov and Bonnet-Myers in our
discrete setting. But first, let's describe the construction and
properties of our specific discrete setting.

\subsection{The Discrete Setting}\label{the-discrete-setting}

Within the ambient setting of points \(x_{i} \in \mathbb{R}^D\), the
Euclidean distances between the points in our point cloud are not very
useful. To perform geometric analysis, we want the manifold's
\emph{geodesic} distances between \(x_{i}, x_{j} \in \mathcal{M}\),.
However, manifolds are locally euclidean, so within a sufficiently small
neighborhood of \(x_{i} \in \mathcal{M}\) , the euclidean distances are
accurate. This is the basis of graph construction: retain only the
trustworthy local distances, discard the rest, and then ``integrate''
over the local neighborhoods to recover features of the global geometry.

A graph \(G = (V, E)\) is a collection of \(n\) vertices \(v_{i} \in V\)
connected by (possibly weighted) edges \(e_{ij} \in E\) . It is
efficiently represented by a single \emph{adjacency} (or
\emph{affinity}) matrix \(A \in \mathbb{R}^{n \times n}\), where
\(A_{ij}\) expresses the degree of connection between the vertices
\(v_{i}\) and \(v_{j}\). In a binary adjacency matrix, \(A_{ij}=1\) iff
there is an edge between \(v_{i}\) and \(v_{j}\). In a weighted affinity
matrix, \(0<A_{ij}<1\) with a higher affinity indicating a closer
connection between the nodes.

One can construct an affinity matrix from a point cloud with the
following algorithm: 1. Compute the matrix \(D\) of pairwise euclidean
distances between points, so that \(D_{ij}=\|x_{i}-x_{j}\|_{2}\). 2.
Apply a kernel \(\kappa\) to the distances to construct the affinity
matrix, where \(A_{ij} = \kappa(D_{ij})\). This is typically the
gaussian kernel: \[
\kappa(y) = \frac{1}{\sqrt{ 2\pi }\sigma}\exp\left( -\frac{y}{\sigma^2} \right)
\] There are a variety of heuristics for selecting an appropriate kernel
bandwidth \(\sigma\). In this paper, we use an adaptive kernel
bandwidth, in which, when computing \(k(D_{ij})\), \(\sigma\) is set to
the mean distance from the points \(x_{i}\) and \(x_{j}\) to their
\(k\)-th nearest neighbor.

After building our graph affinity matrix \(A\), we created a new
representation of the point cloud \(X\) -- turning it from an
\(n \times D\) matrix of unwieldy ambient coordinates into an
\(n \times n\) matrix of pairwise connections between points. The
challenge is now to reassemble this information of local connectivity to
recover the features of \(\mathcal{M}\). Graph diffusion does precisely
this.

\subsection{Graph Diffusion}\label{graph-diffusion}

By row-normalizing \(A\), one obtains the graph diffusion matrix
\(P = D^{-1}A\), which is a commonly-used method of ``integrating'' the
local connectivity of the graph \(A\) into global geometric descriptors
of \(\mathcal{M}\). Coifman and Lafon (Coifman and Lafon 2006) proved a
correspondence between iterated graph diffusion \(P^t\) and the Neumann
heat kernel on \(\mathcal{M}\). Their technique, \emph{Diffusion Maps},
uses the Euclidean distances between eigencoordinates of \(P\) to
approximate the geodesic distances on \(\mathcal{M}\). The visualization
technique \(PHATE\) (Moon et al. 2019) constructs a low-dimensional
embedding of a point cloud \(X\) such that a distance between the
transition probabilities \(P\) of \(X\) is preserved in the embedding.
(More on properties of phate, trajectory preservation.) \emph{Diffusion
Earth Mover's Distance} (A. Y. Tong et al. 2021) efficiently
approximates the transportation distance between distributions on a
graph using multi-scale wavelet transform obtained by applying different
scales of diffusion. \emph{LEGSNet}`s ``learnable geometric scattering''
computes tunable scales of diffusion with a graph neural network and
achieves state of the art performance on biochemistry graph
classification (A. Tong et al. 2021). These are but a few of the many
manifold learning techniques based in diffusion.

Constructing the diffusion matrix from the affinity matrix \(A\) is
straightforward: you simply row-normalize \(A\), with an optional step
to normalizing by density.

Here is the algorithm presented in Coifman and Lafon (Coifman and Lafon
2006):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Optional) Compute an \emph{anisotropic density normalization} on
  \(A\), obtaining the anisotropic adjacency matrix \(A_{\star}\).
\item
  Construct the degree matrix \(D\), whose diagonal entries are the
  rowsums of \(A\), i.e.~\(D_{ii} = \sum_{j}A_{ij}\).. The other entries
  are zeros.
\item
  Define \(P = D^{-1} A\), the graph diffusion matrix.
\end{enumerate}

\begin{itemize}
\tightlist
\item[$\square$]
  Clean this up: get anisotropic equation, and clarify the role of the
  self affinity. When is it removed? When is laziness added?
\end{itemize}

\(P\) has several nice properties. The rows \(P[i]\) give the transition
probabilities of a single step random walk starting at point \(x_{i}\);
each row \(P[i]\) can be viewed as a probability distribution centered
at \(x_{i}\). This is preserved under powers of the matrix. The rows of
\(P^t\) still sum to 1, and \(P^t[i]\) now gives the probability
distribution of a \(t\)-step random walk starting at \(x_{i}\).

Although \(P\) is not symmetric, it is conjugate to a symmetric matrix,
via \(D^{0.5}PD^{-0.5} = D^{-0.5}AD^{-0.5}\), granting it a full basis
of real-valued eigenvectors and eigenvalues. These eigenvectors are
shared with the normalized graph Laplacian
\(L = I - D^{-0.5}AD^{-0.5}\). The eigenvalues of \(P\) have magnitude
less than or equal to 1. Powering the matrix \(P^t\) thus corresponds to
powering the eigenvalues \(\lambda_{i}^t\) of \(P\), via diagonalization
\[
P^t = \Psi \Lambda^t \Psi^T
\] This is similar to applying a low-pass filter to the graph. As \(t\)
increases, the smallest eigenvalues decay fastest under repeated
powering, and their corresponding eigenvector vanishes from the
eigenbasis -- leaving only the largest \(\lambda_{i}\), whose
eigenvectors trace global geometric features.

This is a remarkable feature of the diffusion matrix: the ability to
``denoise'' itself by iterating the random walk over larger time scales.
Intuitively, the paths through the data most robustly trafficked by
random walkers are those supported by multiple high-probability
connections from independent starting points.

\subsection{Ollivier-Ricci Curvature}\label{ollivier-ricci-curvature}

Developed by Yann Ollivier in 2007, \emph{Coarse Ricci Curvature} (or
sometimes, ``Ollivier Ricci Curvature'') is a direct translation of
Ricci curvature to discrete metric spaces like graphs (Ollivier 2009).
Several classical properties of Ricci curvature can be extended to the
graph setting using Coarse Ricci Curvature. Ollivier has, for instance,
proven versions of concentration inequalities, Bonnet Myers (more).
Coarse Ricci Curvature has, in this way, become something of a bridge
between continuous and coarse geometry. The basis of this bridge is
optimal transport, and specifically, the 1-Wasserstein distance.

In the Riemannian setting, Ricci curvature captures the phenomenon that,
in positive curvature, ``small spheres are closer (in transportation
distance) than their centers are'' (Ollivier 2009). On the sphere, for
instance, imagine two circles centered on the north and south poles:
every point in each circle is closer to the corresponding point in the
opposite circle than are the circles' centers. This effect diminishes as
one moves the circles closer together, but never reaches equality. In
negatively curved spaces, the discrepancy reverses, while in a flat
space, the average distance between the points of the circles is the
distance between the centers.

Coarse Ricci Curvature captures a similar phenomenon on graphs. Instead
of spheres, it uses locally-centered probability distributions defined
by random walks. And to measure the distance between these walks, it
uses the 1-Wasserstein (or Earth Mover's) distance. We'll briefly define
each.

The 1-Wasserstein distance is a measure of the distance between
probability distributions. Given distributions \(\mu_{x}\) and
\(\mu_{y}\) over some shared space \(X\), the Wasserstein distance
quantifies the smallest amount of ``work'' needed to transform one
distribution into another, by transporting probability ``mass'' between
pairs of points over the ground metric \(d(x,y)\):

\begin{definition}[1-Wasserstein
Distance]\protect\hypertarget{def-1-wasserstein}{}\label{def-1-wasserstein}

The 1-Wasserstein distance between distributions \(\mu_{x}\) and
\(\mu_{y}\) is
\[ W_{1}(\mu_{x},\mu_{y}) := \inf_{\xi \in \Pi(\mu_{x},\mu_{u})} \int \int d(x,y) \, d\xi(x,y) \]
where the ``transportation plan'' \(\xi\) is drawn from the space
\(\Pi(\mu_{x},\mu_{y})\) of joint probability distributions over
\(X \times X\) which project onto \(\mu_{x}\) and \(\mu_{y}\). In the
discrete setting, this translates naturally into an infimum over a
summation.
\[W_{1}(\mu_{x},\mu_{y}) := \inf_{\xi \in \Pi(\mu_{x},\mu_{y})} \sum_{x \in X} \sum_{y \in X} d(x,y) \xi(x,y)\]

\end{definition}

What is the analog on a graph of a ``small sphere'' around a point?
Ollivier replaces spheres with a family of measures \(m_{x}(\cdot)\)
defined for each point \(x\), where

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each \(\mu_{x}(\cdot)\) depends measurably on \(x\), i.e.~the map
  \(x \to \mu_{x}\) is measurable.
\item
  Each \(\mu_{x}(\cdot)\) has finite first moment, or \emph{Jump},
  i.e.~for some \(o \in X\) \(\int d(o,y) \mu_{x}(y) \, dx < \infty\).
\end{enumerate}

In graphs, Ollivier defines these \(\mu_x\) as the probability
distributions created by a single-step random walk from the point \(x\).

With a transition probability \(\alpha\), and equal probability of
moving to each of \(x\)'s neighbors on the graph,
\(\mu_{x}(x) = (1-\alpha)\) and \(m_{x}(y) = \alpha\) if \(y \in N(x)\)
or \(0\) otherwise.

This is analogous to defining \(m_{x} = P e_{x}\), if \(P\) is the
diffusion matrix created from a binary adjacency matrix. Note, however,
that there is nothing limiting us to binary adjacency matrices, or even
single steps of diffusion; the two conditions above are equally
satisfied by weighted adjacency matrices and \(t\)-step diffusions, and
in sparse or noisy graphs, this may be desirable.

\begin{definition}[Coarse Ricci
Curvature]\protect\hypertarget{def-coarse-ricci-curvature}{}\label{def-coarse-ricci-curvature}

The \emph{Coarse Ricci Curvature} between \(x\) and \(y\) is
\[\kappa(x, y):=1-\frac{W_1\left(m_x, m_y\right)}{d(x, y)}\]

\end{definition}

There are a number of provisos attached to this definition, which tries
to approximate a continuous phenomenon within discrete constraints.
These constraints, and the relationship between Ricci and Ollivier's
coarse Ricci curvature are illustrated Ollivier's Example 2.6 (Ollivier
2009):

\begin{example}[]\protect\hypertarget{exm-ollivier-example-2.6}{}\label{exm-ollivier-example-2.6}

Let \((X,d)\) be a smooth Riemannian manifold of dimension \(d\) and let
\(\text{vol}\) be the Riemannian volume measure. Let \(\epsilon>0\)
small enough and consider the ball of radius \(\epsilon\) around each
point \(x\). Let \(x,y \in X\) be two sufficiently close points. Let
\(v\) be the unit tangent vector at \(x\) directed towards \(y\). The
coarse Ricci curvature along \(v\) is then
\[\kappa(x,y) = \frac{\epsilon^2 \text{Ric}(v,v)}{2(d+2)}+o(\epsilon^3 + \epsilon^2d(x,y))\]

\end{example}

Hence the coarse Ricci curvature applied to a manifold recovers the
Ricci curvature, up to a scaling factor contingent on dimension, and
plus an error term that grows with the radius of ball and distance
between points.

Ollivier's choice not to scale \(\kappa(x,y)\) by dimension is
interesting, and likely motivated by his application of coarse Ricci
curvature to graph-like spaces for which dimension isn't clearly
defined, like social networks. Within our domain of point-cloud data,
incorporating dimension may be desirable; without it, spaces of high
dimension can be conflated with spaces of lower dimension but higher
negative curvature.

\begin{itemize}
\tightlist
\item[$\square$]
  Ollivier actually does define a local diffusion-based dimension
\end{itemize}

A result on coarse Ricci curvature which will prove useful concerns the
\emph{contraction (or expansion) of measure} that occurs under diffusion
in spaces of positive (or negative) curvature. This is Ollivier (2009)'s
Proposition 20:

\begin{proposition}[\(W_1\) Contraction of
Measure]\protect\hypertarget{prp-ollivier-contraction-of-measure}{}\label{prp-ollivier-contraction-of-measure}

Let \((X,d,m)\) be a metric space with a random walk. Let
\(\kappa \in \mathbb{R}\). Then we have \(\kappa(x,y) \geq \kappa\) for
all \(x,y \in X\) iff for any two probability distributions
\(\mu, \mu' \in \mathcal{P}(X)\) one has

\[
W_{1}(\mu \star m, \mu' \star m) \leq (1-k)W_{1}(\mu, \mu')
\] Where \[
 \mu \star m := \int_{{x \in X}} d\mu(x)m_{x} \, dx
\]

\end{proposition}

\section{Methods}\label{methods}

We consider a measure space \((X,p)\) equipped with a random walk,
e.g.~a point cloud and a diffusion operator derived as above. In this
section, we define a new notion of curvature deriving from this random
walk.

The core intuition of Diffusion Curvature is that the \emph{laziness} of
random walks on a graph is a proxy for the curvature of the underlying
manifold. Picture a ``random walker'' drunkenly traversing a sphere. If
he manages, over the course of several steps, to wander to the opposite
pole, he has many ways of getting back to where he started. By contrast,
if he begins on the top of a (negatively-curved) saddle and wanders down
one side, any path aside from exactly retracing his steps incurs a steep
penalty in extra distance. On the sphere, the random walker is more
likely to find his way home: his walks are ``lazier''.

Our previous paper (Bhaskar et al. 2022) measured this laziness
directly, as the return probability within a k-neighborhood of the
starting point. This required tuning the parameter \(k\), and neglected
the information provided by probabilities outside of this neighborhood.

We refine our previous definition of diffusion laziness by replacing the
neighborhood-sum with a distributional distance measure \(D\) between a
dirac \(\delta_{x}\) and its \(t\)-step diffusion \(p_{x}^X(t)\). This
provides a more sensitive and parameter-free measure of how `spread out'
the diffusion is.

\begin{definition}[Diffusion Energy
Snapshot]\protect\hypertarget{def-energy-of-diffusion}{}\label{def-energy-of-diffusion}

Given some distributional distance \(D\), the \(D\)-energy snapshot of a
diffusion \(p_{x}^X(t)\) is \[
l_{X}(x,t) := D\left(\delta_x, p_x^X(t)\right)
\]

\end{definition}

By \(p_{x}^X(t)\), we indicate the \(t\)-step diffusion of the dirac
\(\delta_{x}\) centered at \(x \in X\), using the diffusion operator for
the measure space \(X\). We use the term `Diffusion Energy' in reference
to our previous work (CITE), which measured curvature via `Diffusion
Laziness'. That laziness measurement increased with positive curvature;
diffusion energy does the opposite, increasing with more negative
curvature.

The distributional distance \(D\) may be the Wasserstein-1 distance, as
with Ollivier-Ricci curvature; it may be the Jensen-Shannon Distance,
evoking the classical use of entropy to measure the diffusion of heat;
or it may be some other domain-specific form chosen for a particular
metric measure space.

This measure of diffusion energy provides a snapshot of curvature at
single diffusion time. However, this curvature is unsigned, and
sensitive to the choice of \(t\). Moreover, as we'll discuss,
differences in graph construction cause the times associated with the
diffusion operators from different spaces to change at different speeds.
This makes it hard to compare the geometric information gleaned from the
diffusion operator across manifolds.

Instead of these static and fickle snapshots of a diffusion process, we
consider the process in its entirety. As \(t\) ranges from 0 to
\(\infty\), the diffusions \(p_{x}^X(t)\) can be viewed as a trajectory
on the \(n\)-dimensional probability simplex, where \(n\) is the number
of points in \(X\). Given a distributional distance and its generating
Riemannian metric, we can define Diffusion Curvature as the path
integral of diffusion energy along this trajectory.

\begin{definition}[Diffusion
Energy]\protect\hypertarget{def-diffusion-energy-path-integral}{}\label{def-diffusion-energy-path-integral}

Given a metric \(g_{p}\) on the statistical manifold of diffusion
probabilities, and the distributional distance \(D\) it generates the
Diffusion Energy of a point \(x \in X\) is

\[
l_{X}(x, d)= \int_{t=0}^{T(d)} D\left(\delta_x, p_x^X(t)\right) g_p\left(\frac{d p_x^X(t)}{d t}, \frac{ d p_x^X(t) }{d t} \right)
\]

where \(T(d)\) is a function that converts between the calculated
\emph{Diffusion Trajectory Distances} \(d\) and diffusion times \(t\).

\end{definition}

In practice, we don't have an infinitesimal parameterization of
\(p_{x}^X\), so approximate this with the algorithm in BOX WAWA: we
compute \(T\) scales of diffusion (e.g.~50), sum the distributional
distances between each scale to estimate the path integral, and then use
a trapezoidal approximation of the integral of \(L_{x}(t)\) with respect
to the distances along the trajectory.

By viewing the energy integrated along a diffusion trajectory, we
eliminate many of the differences between diffusions on different
spaces. This enables the computation of a \emph{signed} curvature, by
taking the difference of diffusion energies from a given space and a
comparison space of known Euclidean curvature and equal dimension to
\(X\). In practice, we construct the comparison space as uniform samples
from \(\mathbb{R}^d\), where \(d\) is the intrinsic dimension of \(X\).

\begin{definition}[Diffusion
Curvature]\protect\hypertarget{def-diffusion-curvature}{}\label{def-diffusion-curvature}

Given points \(X \subseteq \mathcal{M}\), where \(\mathcal{M}\) is a
Riemannian manifold of dimension \(d\), the \emph{Diffusion Curvature}
of \(x \in X\) is

\[
k_(x, d) = l_{X}(x, d) - l_{X}(x, d)
\] where \(E \subseteq \mathbb{R}^d\) is a collection of uniformly
sampled points from \(\mathbb{R}^d\).

\end{definition}

The usefulness of this definition rests upon two claims:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  That diffusion energy is indeed a measure of (unsigned) curvature
\item
  That our integration along diffusion trajectories enables this energy
  to be compared across point clouds with different diffusion processes.
\end{enumerate}

The next two sections will analyze this first claim. We first analyze
diffusion energy within Ollivier's framework of metric measure theory,
and bound the diffusion energy from above by the Ollivier-Ricci
curvature. We then provide a motivating example recovering a
Bishop-Gromov type volume comparison from the diffusion curvature with
an entropic distance.

Finally, we support the second claim with illustrations of point clouds
which diffusion trajectories can uniquely compare. We also describe how
common techniques of constructing graphs from point clouds can obscure
geometric information, and propose a `curvature-agnostic kernel' to
facilitate more geometrically-faithful graph construction.

\subsection{Diffusion Energy is bounded by Ollivier-Ricci
curvature}\label{diffusion-energy-is-bounded-by-ollivier-ricci-curvature}

Ollivier Ricci curvature measures the extent to which `spheres are
closer than their centers', using graph diffusions to create spheres,
and optimal transport to measure distances. Diffusion energy uses both
ingredients. But whereas Ollivier Ricci curvature takes the Wasserstein
distance between spheres of the same size, Diffusion energy takes the
distance between successive sizes of the same sphere.

This simplification makes Diffusion energy more localized and much
computationally lighter. Here we demonstrate that it also retains the
theoretical guarantees of Ollivier Ricci curvature, and might be though
of as a natural node-wise adaptation of Ollivier Ricci curvature.
\begin{proposition}[Coarse Ricci Curvature Bounds Diffusion
Energy]\protect\hypertarget{prp-coarse-ricci-curvature-bounds-diffusion-energy}{}\label{prp-coarse-ricci-curvature-bounds-diffusion-energy}

Let \((X,d,m)\) be a metric space equipped with a random walk, with
coarse Ricci curvature bounded from below by some \(k\) such that
\(\kappa(x,y) \geq k\) for all \(x,y \in X\). The diffusion energy of a
\(t\) step diffusion in \(X\) is bounded above by \[
W_{1}(\delta_{x}, m_{x}^t) \leq W_1\left(\delta_x,m_x\right)\frac{(1-(1-k)^t)}{k}
\]

In particular, if \(k>0\) then
\(W_{1}(\delta_{x},m_{x}^t) \leq \frac{W_1\left(\delta_x,m_x\right)}{k}\),
and if \(k=0\), then
\(W_{1}(\delta_{x},m_{x}^t) \leq tW_1\left(\delta_x,m_x\right)\).

\end{proposition}

To prove this, first we bound \(W_{1}(m_{x}^t,m_{x}^{t+1})\) using
Proposition~\ref{prp-ollivier-contraction-of-measure}. The proposition
states that a lower bound on curvature, such as we have, implies that \[
W_{1}(\mu \star m, \mu' \star m) \leq (1-k)W_{1}(\mu, \mu')
\] where here \(\mu,\mu'\) are two probability distributions and \(m\)
is a random walk. This provides an easy lemma:

\begin{lemma}[Inductive Contraction of
Measure]\protect\hypertarget{lem-inductive-contraction-of-measure}{}\label{lem-inductive-contraction-of-measure}

Let \((X,d,m)\) be a metric space with a random walk. Suppose there is
some \(k \in \mathbb{R}\) such that the coarse Ricci curvature
\(\kappa(x,y) \geq k\) for all \(x,y \in X\). Then: \[
W_1\left(m_x^t, m_x^{t+1}\right) \leq(1-k)^t W_1\left(\delta_x,m_x\right)
\]

\end{lemma}

\begin{proof}
We proceed by induction. For \(t=0\), the above is true, as
\(W_{1}(m_{x}^0, m_{x}^{1}) =W_{1}(\delta_x, m_{x}^{1}) = W_1\left(\delta_x,m_x\right)\).
Suppose it holds for \(t-1\), e.g. \[
W_1\left(m_x^{t-1}, m_x^{t}\right) \leq(1-k)^{t-1} W_1\left(\delta_x,m_x\right)
\]

Consider \(W_1\left(m_x^{t-1}\star m, m_x^{t}\star m\right)\), the
application of another step of diffusion. By Ollivier's Proposition 20,
this distance is bounded above by \[
W_1\left(\mu_1 \star m, \mu_2 \star m\right) \leq(1-k) W_1\left(\mu_1, \mu_2\right)
\] So \[
W_1\left(m_x^{t-1}\star m, m_x^{t}\star m\right) \leq  (1-k)W_1\left(m_x^{t-1}, m_x^{t}\right)
\]

which, since the statement holds for \(t-1\), yields \[
W_1\left(m_x^t, m_x^{t+1}\right) \leq(1-k)^t W_1\left(\delta_x,m_x\right)
\]
\end{proof}

We can now use this lemma to decompose our \(t\)-step diffusion into a
sum of single-step diffusions, allowing an easy proof of the
proposition.

\begin{proof}
By the triangle inequality, \[
W_{1}(\delta_{x},m_{x}^t) \leq W_1\left(\delta_x,m_x\right) + W_{1}(m_{x},m_{x}^2) + \dots + W_{1}(m_{x}^{t-1}, m_{x}^t)
\]

By Lemma~\ref{lem-inductive-contraction-of-measure}, \[
\leq W_1\left(\delta_x,m_x\right)\left(1+(1-k)+(1-k)^2+\ldots+(1-k)^{t-1}\right)
\]

This truncated series is equal to
\(\frac{1-(1-k)^t}{1-(1-k)} = \frac{(1-(1-k)^t)}{k}\). If \(k>0\), then
as \(t \to \infty\), the infinite sum \(\sum_{i=0}^t (1-k)^i\) converges
to the geometric series \(\frac{1}{1-(1-k)} = \frac{1}{k}\). Because the
sum is monotonically increasing with \(t\), the partial sum is upper
bounded by the infinite sum. It follows that \[
W_{1}(\delta_{x},m_{x}^t) \leq \frac{W_1\left(\delta_x,m_x\right)}{k}
\]

If \(k=0\), then obviously \(\sum_{i}^t (1-k)^i = t\).
\end{proof}

The above shows that the diffusion energy \(l_{W_1}\) has an upper-bound
given by the Ollivier Ricci curvature. We expect that the higher a
manifold's curvature, the lower \(l_{W_{1}}\) should be. This is
formalized by Proposition 1: the higher the Ollivier-Ricci curvature of
a manifold, the smaller its diffusion energy. This result motivates the
use of diffusion curvature as a point-wise adaptation of Ollivier-Ricci
curvature.

\subsection{A Motivating Example: Entropic Diffusion Curvature Recovers
a Bishop-Gromov Volume
Comparison}\label{a-motivating-example-entropic-diffusion-curvature-recovers-a-bishop-gromov-volume-comparison}

Among the most obvious manifestations of curvature is differences in
volume: the higher the curvature, the smaller the volume of geodesic
balls of the same radius. In the continuous setting, this is expressed
by Theorem~\ref{thm-bishop-gromov}.

One route to discrete curvature is thus estimating volumes; this is the
route taken by Hickok and Blumberg (2023). However, standard methods of
estimating volumes in discrete, sampled spaces like point clouds or
graphs rely on more complex geometric quantities, like density
estimation or approximated geodesic distances, which are not only
susceptible to noise, but often themselves dependent on the
curvature!\footnote{Consider kernel density estimation, where one places
  gaussians on each data point and sums up the probability assigned to
  each area. The higher the curvature, the higher the reported density,
  since at the end of an ellipsoid the gaussians have greater overlap
  with each other than in its center -- even though the intrinsic
  density of the surface is uniform.} One wonders: can we use the tools
of diffusion geometry to robustly estimate volume?

Indeed, this is one way to interpret diffusion energy: as an inverse
volume measurement. Huguet et al. (2023) motivates this with a result
from Saloff Costes et al. (Saloff-Coste 2010). For manifold satisfying
the Parabolic Harnock Inequality (e.g.~all manifolds of positive
curvature), heat diffusion \(m_{x}^t\) on the manifold is bounded by

\[
\frac{c_1}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_2 t}\right) \leq m_{x}^t(y) \leq \frac{c_3}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_4 t}\right)
\]

In other words, heat diffusion on non-Euclidean manifolds behaves
approximately like Euclidean heat diffusion scaled by the local volume.

Powering the diffusion matrix approximates heat diffusion {[}cite
Coifman{]}. We can thus relate our measure of the `spread' or `energy'
of this diffusion to the manifold's volume.

The Shannon entropy of the diffusion \(m_{x}\) over the bounded
neighborhood of \(X\) with non-zero diffusion mass, can be written \[
H\left(m_x^t\right)=-\int_{y_\epsilon x} m_x^t(y) \ln \left(m_x^t(y)\right)
\] Here we use Shannon's differential entropy.Though this formulation
encounters pathologies on unbounded domains, it approximates the entropy
a signal on a sampled region as the number of samples goes to infinity.

We'll simplify the notation by combining the integral and distribution
into an expected value over \(m_{x}^t\) \[
=-\mathbb{E}_{m_x} \ln \left(m_x(y)\right) d y
\] Recalling the above result, there are constants \(c_{1}\dots c_{4}\)
such that

\[
\frac{c_1}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_2 t}\right) \leq m_{x}^t(y) \leq \frac{c_3}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_4 t}\right)
\] hence we can approximate \(H(m_{x}^t)\) by

\[
=-\mathbb{E}_{m_{x}^t} \ln \left(\frac{1}{V(y, \sqrt{t})} \exp \left(\frac{-d(x, y)^2}{4 t}\right)\right)
\] \[
=-\mathbb{E}_{m_x^t} \ln \left(\frac{1}{V(y, \sqrt{6})}\right)-\frac{d(x, y)^2}{4 t}
\] \[
=\mathbb{E}_{m_x^t} \ln (V(y, \sqrt{t}))+\mathbb{E}_{m_{x}^t} \frac{d(x, y)^2}{4 t}
\]

By assumption, the volumes \(V(y,\sqrt{ t })\) equal some constant
\(V_{m}(\sqrt{ t })\) over the support of \(m_{x}^t\). Also note that
the right hand term is precisely
\(\frac{1}{4t}W_{2}(\delta_{x}, m_{x}^t)\), so \[
H(m_{x}^t) \simeq \ln(V_{m}(\sqrt{ t })) + \frac{1}{4t}W_{2}(d_{x},m_{x}^t)
\] The Diffusion Curvature is then \[
\frac{1}{t}(H(m_{E}^t) - H(m_{x}^t)) \simeq \frac{1}{t}\ln(V_{E}(\sqrt{ t })) - \frac{1}{t}\ln(V_{m}(\sqrt{ t })) + \frac{1}{4t^2}( W_{2}(d_{x},m_{E}^t) - W_{2}(d_{x},m_{x}^t))
\]

This presents us with two comparisons: one between the average volume
within \(N_{t}(x)\) both in Euclidean space and on the manifold, and one
between the Wasserstein spread of diffusion in Euclidean space and on
the manifold. Note that both exhibit the same inverse relationship to
curvature. If the curvature of \(m\) is greater than in euclidean space
(e.g.~positive), the volume of a ball is smaller -- just as the spread
of diffusion is diminished. In negative curvature, we have the opposite.

\subsection{Comparing Diffusion Across Point
Clouds}\label{comparing-diffusion-across-point-clouds}

\subsubsection{Problem Setting \&
Motivation}\label{problem-setting-motivation}

For a single graph, the energy of diffusion is linked to graph volume
and theoretically bounded by curvature. Within a given graph, the lower
the curvature, the higher the diffusion energy. As shown above, we can
draw analogy to Bishop-Gromov to convert this unsigned
magnitude-of-curvature estimation to a signed curvature via a comparison
between spaces.

A comparison like this is at the heart of many discrete curvatures.
Ollivier-Ricci compares the transport distance between two distributions
with the transport distance on a flat space. Hickok and Blumberg compare
the approximate the volume of balls on a given point cloud with the
Euclidean volume of balls with the same radius. In these definitions,
the use of more complex geometric constructs (distances, volumes,
densities) grants the Euclidean comparison an analytic form. Our
definition makes a different trade-off: it doesn't depend on the
error-prone approximations of those higher-order phenomena, but it
doesn't benefit from their theoretical simplicity.

To put it bluntly, there's no analytical form of, e.g.~the diffusion
energy snapshot in \(\mathbb{R}^8\) at \(t=25\) because there's no
single translation between \(\mathbb{R}^8\) and one of the ``discrete
measure space with a random walk'' our definition operates on. There are
many different ways of uniformly sampling \(N\) points from
\(\mathbb{R}^d\) -- and there are many ways of turning those points into
a graph. But different samplings, and different modes of graph
construction result in different \emph{speeds of diffusion} on the
graph, rendering direct comparison of diffusion energies difficult.

For example, imagine constructing two \(k\)-nearest neighbor graphs with
a different value of \(k\) for each pointcloud. In the graph constructed
with a larger \(k\), diffusion will happen more quickly because
everything is more tightly connected. The diffusion energy of a point on
this graph, at any given \(t\), will appear much higher than its sister
graph -- even if both were constructed from the same space! The vagaries
of graph construction have obscured the underlying geometry of the
manifold.

An immediate workaround is to try to bypass this obstruction by
constructing graphs in \emph{exactly} the same manner from both our
target points and comparison points. One might use a \(k\)nn graph with
identical \(k\)'s, or an adaptive kernel which sets the bandwidth
locally based on the distance to each nearest neighbor. We experimented
with this approach extensively, and came to the unexpected conclusion
that the interplay between curvature and distance makes it extremely
hard to create a kernel with the same relative bandwidth in spaces of
differing curvatures. Specifically, the local Euclidean distance used by
standard graph construction techniques like the k-neighbor adaptive
kernel becomes less reliable the greater the magnitude of curvature.
Thus, not only are comparisons between spaces compromised, but even the
geometric information within a space is obscured by a gratuitously
fluctuating kernel bandwidth. We explore this in more detail in the next
section, and propose a simple change which lessens these symptoms.

However, the underlying `disease' remains: although we assume our point
clouds are sampled manifolds, there's no one-to-one translation between
these manifolds and the measure spaces with random walks diffusion
geometry operates on. Different samplings and different means of graph
construction all create different random walks. Trying to homogenize
these random walks via careful graph construction relies on geometric
information, like distances, that we don't (yet) have -- and tries to
wage war against the notoriously fickle process of sampling. Far better
than trying to homogenize these random walks on the level of \emph{graph
construction} would be a method that can reliably compare diffusions
\emph{across} graphs.

This problem, of reliably comparing diffusions across graphs, has wide
applicability. \ldots{} research needed here \ldots{} Other techniques
partially address this by considering \emph{multiscale diffusions}. The
\emph{multiscale diffusion distance} takes a weighted average across
powers of diffusion. Diffusion Earth Mover's Distance extends this to an
optimal transport measure based on the differences distributions
diffused at multiple scales. The \emph{Graph scattering transform}
produces a rich set of features for comparing graphs derived from
differences between powers of diffusion. And many graph neural networks
are built upon flavors of message passing which resemble graph
diffusion, while others have been modeled on graph scattering. All of
these methods come with a caveat: if compared across graphs,
discrepancies in graph construction will manifest as spurious
differences in graph geometry.

\subsubsection{Diffusion Trajectory
Alignment}\label{diffusion-trajectory-alignment}

We consider a new approach to this problem, which replaces the diffusion
time \(t\) -- so heavily dependent on the vagaries of graph construction
-- with a distance along the \emph{diffusion trajectory}. To obtain
this, we view scales of diffusion as points along a trajectory on the
\(N\)-dimensional probability simplex, where \(N\) is the number of
nodes on the graph.\footnote{This simplex corresponds to the `diffusion
  coordinates' used by Coifman, as well as the setting of Fasina's
  diffusion-based Fisher Information metric (CITE).}This trajectory has
the advantage of erasing any differences caused by the speed of
diffusion. Instead, we can use the distance along that trajectory
\emph{Diffusion Trajectory Distance}) to align the scales of diffusion
between two graphs. As illustrated in Figure~\ref{fig-curvature-curves},
by comparing the diffusion energies of several graphs across their
diffusion trajectory distances. Making the same comparison across times
is uninformative - and misleadingly suggests that the sphere is the most
negatively curved. \emph{Diffusion Trajectory Alignment}, however,
reveals the correct relation between the surfaces.

\begin{figure}[H]

\centering{

\includegraphics{index_files/figure-latex/..-nbs-3d-diffusion-trajectory-distance-normalization-fig-curvature-curves-output-2.png}

}

\caption{\label{fig-curvature-curves}Diffusion energies across multiple
scales on a 4-dimensional saddle, plane, and sphere (all 3-manifolds).
When comparing directly between times, there is no clear separation of
curvatures. Aligning diffusions by diffusion trajectory distance reveals
curvature.}

\end{figure}%

\textsubscript{Source:
\href{https://professorwug.github.io/diffusion-curvature//Users/boreas/Pumberton/Workshop/21-SUMRY-Curvature/diffusion-curvature/nbs/3d-diffusion-trajectory-distance-normalization.ipynb.html\#cell-fig-curvature-curves}{Standard
libraries}}

Applying diffusion trajectory alignment to our diffusion energies also
has the happy effect of focusing our measurement on the most relevant
scales of diffusion. When using diffusion times, one must be careful to
select a time that is neither too small nor too large. Make \(t\) too
small, and the diffusion might have barely differentiated itself from a
dirac. Make \(t\) too large, and the diffusion might have progressed to
the uninformative steady-state distribution. With the diffusion
trajectory distance, all of the undesirably large \(t\)'s are compressed
into a narrow range of distances -- the diffusion trajectory limits to
the steady state distribution, and the diffusion trajectory distances
that correspond to it are all within some \(\epsilon\) of the maximum
diffusion trajectory distance. Hence, any diffusion trajectory distance
between its minimum and maximum values corresponds to an interesting
scale of diffusion.

With the alignment help of the diffusion trajectory distance, we can now
motivate our path-integral definition of diffusion energy
(Definition~\ref{def-diffusion-energy-path-integral}): \[
l_{X}(x, d)= \int_{t=0}^{T(d)} D\left(\delta_x, p_x^X(t)\right) g_p\left(\frac{d p_x^X(t)}{d t}, \frac{ d p_x^X(t) }{d t} \right)
\]

Recall that \(D(\delta_{x}, p_{x}^X(t))\) is the snapshot diffusion
energy (Definition~\ref{def-energy-of-diffusion}) at time \(t\). This is
integrated with respect to the diffusion trajectory distance, here
formulated via a Riemannian metric on the probability simplex. The
parameter \(T\) controls how local the measurement of diffusion energy
is. As \(T\) approaches \(\infty\), the diffusion approaches the steady
state across the entire graph, and the diffusion energy measures a
global curvature. Smaller \(T\)'s yield more localized measurements of
curvature. In practice, \(T\) is set via some \(T(d)\) that translates
diffusion trajectory distances into the corresponding times.

The diffusion energy can be thought of as an integral of the geodesic
distance from a distribution to a dirac along the distributions in a
given diffusion trajectory. Imagine a long, bent noodle; while traveling
along the noodle, we integrate the distances from each point on the
noodle to its base point. If these geodesic distances corresponded to
travel back along the diffusion trajectory, or back down the noodle --
e.g.~if the diffusion trajectory took the shortest path between the
dirac and each subsequent distribution -- then the diffusion energy
would be equal across all manifolds. The extent to which this fails, and
the diffusion trajectory takes a `lazier' path than optimal, reveals the
curvature.

This measurement of diffusion energy can now be converted into a signed
\emph{diffusion curvature} via Definition~\ref{def-diffusion-curvature}:

\[
k_(x, d) = l_{X}(x, d) - l_{X}(x, d)
\] This corresponds to taking the area under each of the curves in Fig.
WAWA, up to some trajectory distance \(d\), and subtracting them.

In practice, we don't have access to a metric \(g_{p}\) on the
probability simplex; this space is too high-dimensional, and we lack a
continuous parameterization of graph diffusion. Instead, we approximate
this integral by measuring the distances between subsequent diffusions
(performed for every integer \(1 \leq t \leq T(d)\)). This approximates
the diffusion trajectory distances for each scale of diffusion. We then
use the trapezoidal rule to approximate the integral of diffusion
energies with respect to these trajectory distances. This is summarized
by Algorithm 1 REFERENCE WAWA.

\subsubsection{The Curvature-Agnostic Kernel \& Automatic Bandwidth
Tuning}\label{the-curvature-agnostic-kernel-automatic-bandwidth-tuning}

Diffusion trajectory alignment significantly reduces the interference
caused by graph construction, but does not completely eliminate it. Due
to our lack of access to a diffusion coordinate space metric, and the
approximations used for the diffusion trajectory distance, we find the
robustness of our algorithm is significantly improved by a few
adjustments to standard graph construction techniques to make them more
curvature agnostic.

To turn a point cloud into a graph, one places a kernel (e.g.~a
gaussian) at each point, and assigns edges to surrounding points with
weights given by the kernel. The main parameter here is the bandwidth of
each kernel: make it too high, and everything is connected; too low, and
points are isolated. Moreover, the distances between a point and its
neighbors may vary. Where the points are sparse, one wants a higher
bandwidth. Where dense, a lower bandwidth.

A standard technique for accommodating the variable densities of point
cloud data is a \(k\)-neighbor \emph{adaptive kernel}. Here, the
bandwidth at each point is set to the squared distance to the \(k\)-th
nearest neighbor, averaged in some way between the two inputs. Here's
one standard adaptive kernel:

\[
\begin{aligned}
k(x, y) & =\frac{e^{-\frac{d(x, y)^2}{d(x, N_{k}(x))^2}}}{d(y, N_{k}(y))}
+\frac{e^{-\frac{d(x, y)^2}{d(y, N_{k}(x))^2}}}{d(x, N_{k}(y)}
\end{aligned}
\]

where \(N_{k}(x)\) gives the \(k\)th nearest neighbor to \(x\).

Note that that parameter \(k\) plays roles in two kinds of adaptation.
By setting the bandwidth to some Euclidean distance \(d(x,N_{k}(x))\)
between data points, it adapts the kernel to the \emph{right scale} for
the point cloud, concentrating its probability within a local
neighborhood. It also controls the \emph{size} of that local
neighborhood; typically set between 5 and 15.

Unfortunately, this adaptive process hinges on the correspondence
between the Euclidean distances \(d(x,N_{k}(x))\), and the manifold
distances -- a correspondence that degrades \emph{faster} the higher the
magnitude of curvature! Consider a highly-curved paraboloid. The
Euclidean distance from its center point to its nearest few neighbors is
roughly the geodesic distance. But as the neighborhood widens, and the
neighbors move higher up the paraboloid, the Euclidean distance now
underestimates the true geodesic distance: it cuts across the manifold.
This happens in spaces of high positive or high negative curvature.

As a result, the adaptive kernel constructed in highly curved spaces has
a smaller bandwidth than in flat spaces, even using the same kernel
parameters. If one directly compares the laziness of diffusion on two
such graphs, it will appear lazier than on the flat manifold.

To remedy this, we make a simple adjustment to the standard adaptive
kernel. The key is separating the parameter which controls neighborhood
size from neighborhood scale. In the adaptive kernel, both are
controlled by the parameter \(k\); as discussed above, in spaces of high
curvature, the same \(k\) creates different scales. Instead, we always
set \(k = 1\) and use another parameter -- \(\nu\), the neighbor-size --
to control the variance of the kernel.

\[
\begin{aligned}
k(x, y) & =\frac{e^{-\frac{d(x, y)^2}{d(x, N_{k}(x))^2\nu^2}}}{d(y, N_{k}(y))}
+\frac{e^{-\frac{d(x, y)^2}{d(y, N_{k}(x))^2\nu^2}}}{d(x, N_{k}(y)}
\end{aligned}
\]

\(\nu\) behaves like the kernel bandwidth in the regular fixed Gaussian
kernel. We call this the \emph{Curvature-Agnostic Kernel}.

\begin{itemize}
\tightlist
\item[$\square$]
  Figure comparing the adaptive to curvature agnostic kernels with
  diffusion trajectory normalization. Use a stacked image.
\end{itemize}

Figure WAWA illustrates the increased robustness diffusion trajectory
alignment gains from using this curvature-agnostic kernel. The relative
ordering of graphs is still achieved by the traditional adaptive kernel,
but is significantly denoised by these tweaks.

The main parameter of the curvature-agnostic kernel is the ``neighbor
scale'' \(\nu\), which sets the kernel bandwidth to the specified
multiple of the average distance to the \(k\)th-nearest neighbor. As a
single parameter, this can be tuned easily. We use as a heuristic the
desired median number of points with non-negligible single-step
diffusion probability, and perform a binary search over values of
\(\nu\) until this number is within a desired tolerance.

We suspect the curvature-agnostic kernel may be generally useful in
manifold learning. In any downstream analysis that leverages the graph
geometry -- be it distance estimation, {[}CITE HUGUET{]} or metric
learning {[}CITE US{]} -- this analysis is made easier by a kernel whose
bandwidth does not gratuitously fluctuate in regions of high curvature.

\begin{itemize}
\tightlist
\item[$\square$]
  Can we provide experimental results here? Does PHATE or HeatGeo
  perform better with this kernel?
\end{itemize}

\section{Results}\label{results}

Curvature is an easy quantity to find -- indeed, at least two of the
many recent curvature methods were defined \emph{accidentally}
(Steinerberger 2022; Bhaskar et al. 2022)! The challenge is to make
one's definition robust to the bogeymen of real-world data, especially
1) noise and 2) high dimensions. Our benchmarks of existing methods,
including Hickok \& Blumberg's, Ollivier Ricci Curvature, Adal-PCA
{[}and Sritharan{]}, {[}citations{]} reveal that all deteriorate quickly
under noise, and most \emph{fail entirely} in high-dimensional data. The
dimension-independent, noise-resilient properties of the diffusion
operator endow diffusion curvature with good performance on both of
these axes.

But first, a sanity check. Does diffusion curvature recover the Gaussian
curvature of basic 2-manifolds? Figure~\ref{fig-2-manifolds} shows the
diffusion curvature of a Torus, Saddle, and Ellipsoid, along with a
scatter plot correlating diffusion curvature and Gaussian curvature.
Figure~\ref{fig-2-manifolds-visual-comparison} places this in the
context of existing methods.

\begin{figure}[H]

\centering{

\includegraphics{index_files/figure-latex/..-nbs-2a-Diffusion-Curvatures-of-Toy-Manifolds-fig-2-manifolds-output-1.png}

}

\caption{\label{fig-2-manifolds}Diffusion Curvature vs Gaussian
Curvature on 2-Manifolds.}

\end{figure}%

\textsubscript{Source:
\href{https://professorwug.github.io/diffusion-curvature//Users/boreas/Pumberton/Workshop/21-SUMRY-Curvature/diffusion-curvature/nbs/2a-Diffusion-Curvatures-of-Toy-Manifolds.ipynb.html\#cell-fig-2-manifolds}{Standard
libraries}}

Though the correlation is strong -- passing the `sniff test' -- this
low-dimensional validation highlights two subtleties of our method.
First, diffusion curvature, being an intrinsic, graph-based measurement,
is susceptible to edge effects to a greater degree than extrinsic
methods. When diffusion hits the edges of the saddle, it rebounds,
creating the false appearance of positive curvature.

\begin{figure}[H]

\centering{

\includegraphics{index_files/figure-latex/..-nbs-2-Toy-Manifolds-Benchmark-fig-2-manifolds-visual-comparison-output-1.png}

}

\caption{\label{fig-2-manifolds-visual-comparison}Diffusion Curvature vs
Gaussian Curvature on 2-Manifolds.}

\end{figure}%

\textsubscript{Source:
\href{https://professorwug.github.io/diffusion-curvature//Users/boreas/Pumberton/Workshop/21-SUMRY-Curvature/diffusion-curvature/nbs/2-Toy-Manifolds-Benchmark.ipynb.html\#cell-fig-2-manifolds-visual-comparison}{Standard
libraries}}

The second subtly emerges in the context of related methods. Everything
pictured does an excellent job of coloring the toy manifolds. But
looking beyond this, \emph{what matters?} Most methods don't care about
the precise magnitude -- or even exactly matching the correlations, as
in the ellipsoid\ldots{} \%\% mean vs gaussian curvature, and uniqueness
of definitions\%\%

All of these methods in Figure~\ref{fig-2-manifolds-visual-comparison}
perform well on well-sampled noiseless toy manifolds. In Table
\textbf{?@fig-2-manifolds-noise-table}, we see the results of adding
Gaussian noise to each manifold.

\subsubsection{Differentiating Sign in High
Dimensions}\label{differentiating-sign-in-high-dimensions}

Most existing methods only quantify their performance in high dimensions
on one or two test cases.

\begin{figure}[H]

\centering{

\includegraphics{index_files/figure-latex/..-nbs-5-sign-prediction-tests-fig-sadspheres-output-1.png}

}

\caption{\label{fig-sadspheres}Predicted curvatures of Saddles and
Spheres in dimensions 2-6. Diffusion Curvature robustly distinguishes
between the signs of the data, even in high dimensions, and with
relative sparsity.}

\end{figure}%

\textsubscript{Source:
\href{https://professorwug.github.io/diffusion-curvature//Users/boreas/Pumberton/Workshop/21-SUMRY-Curvature/diffusion-curvature/nbs/5-sign-prediction-tests.ipynb.html\#cell-fig-sadspheres}{Standard
libraries}}

\subsection{Loss Landscapes}\label{loss-landscapes}

\subsection{Curvature as a TDA
Filtration}\label{curvature-as-a-tda-filtration}

\section{Related Work}\label{related-work}

\subsection{Foreman Ricci Curvature}\label{foreman-ricci-curvature}

\subsection{Hickock \& Blumberg's Volume Comparison
Curvature}\label{hickock-blumbergs-volume-comparison-curvature}

\subsection{Sritharan}\label{sritharan}

\section{Conclusion}\label{conclusion}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bhaskar2022DiffusionCurvatureEstimating}
Bhaskar, Dhananjay, Kincaid MacDonald, Oluwadamilola Fasina, Dawson
Thomas, Bastian Rieck, Ian Adelstein, and Smita Krishnaswamy. 2022.
{``Diffusion Curvature for Estimating Local Curvature in High
Dimensional Data.''} \emph{Advances in Neural Information Processing
Systems} 35: 21738--49.
\url{https://proceedings.neurips.cc/paper_files/paper/2022/hash/88438dc62fc5c8777e2b5f1b4f6d37a2-Abstract-Conference.html}.

\bibitem[\citeproctext]{ref-2021BishopGromovInequality}
{``Bishop--{Gromov} Inequality.''} 2021. In \emph{Wikipedia}.
\url{https://en.wikipedia.org/w/index.php?title=Bishop\%E2\%80\%93Gromov_inequality&oldid=1059331416}.

\bibitem[\citeproctext]{ref-coifman2006DiffusionMaps}
Coifman, Ronald R., and Stphane Lafon. 2006. {``Diffusion Maps.''}
\emph{Applied and Computational Harmonic Analysis}, Special {Issue}:
{Diffusion Maps} and {Wavelets}, 21 (1): 5--30.
\url{https://doi.org/10.1016/j.acha.2006.04.006}.

\bibitem[\citeproctext]{ref-hickok2023IntrinsicApproachScalarCurvature}
Hickok, Abigail, and Andrew J. Blumberg. 2023. {``An {Intrinsic
Approach} to {Scalar-Curvature Estimation} for {Point Clouds}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2308.02615}.

\bibitem[\citeproctext]{ref-huguet2023HeatDiffusionPerspective}
Huguet, Guillaume, Alexander Tong, Edward De Brouwer, Yanlei Zhang, Guy
Wolf, Ian Adelstein, and Smita Krishnaswamy. 2023. {``A {Heat Diffusion
Perspective} on {Geodesic Preserving Dimensionality Reduction}.''} May
30, 2023. \url{https://doi.org/10.48550/arXiv.2305.19043}.

\bibitem[\citeproctext]{ref-maaten2008VisualizingDataUsing}
Maaten, Laurens van der, and Geoffrey Hinton. 2008. {``Visualizing
{Data} Using t-{SNE}.''} \emph{Journal of Machine Learning Research} 9
(86): 2579--2605. \url{http://jmlr.org/papers/v9/vandermaaten08a.html}.

\bibitem[\citeproctext]{ref-moon2019VisualizingStructureTransitions}
Moon, Kevin R., David van Dijk, Zheng Wang, Scott Gigante, Daniel B.
Burkhardt, William S. Chen, Kristina Yim, et al. 2019. {``Visualizing
Structure and Transitions in High-Dimensional Biological Data.''}
\emph{Nature Biotechnology} 37 (12, 12): 1482--92.
\url{https://doi.org/10.1038/s41587-019-0336-3}.

\bibitem[\citeproctext]{ref-ollivier2009RicciCurvatureMarkov}
Ollivier, Yann. 2009. {``Ricci Curvature of {Markov} Chains on Metric
Spaces.''} \emph{Journal of Functional Analysis} 256 (3): 810--64.
\url{https://doi.org/10.1016/j.jfa.2008.11.001}.

\bibitem[\citeproctext]{ref-saloff-coste2010HeatKernelIts}
Saloff-Coste, Laurent. 2010. {``The Heat Kernel and Its Estimates.''} In
\emph{Advanced {Studies} in {Pure Mathematics}}, 405--36. Kyoto
University, Japan. \url{https://doi.org/10.2969/aspm/05710405}.

\bibitem[\citeproctext]{ref-steinerberger2022CurvatureGraphsEquilibrium}
Steinerberger, Stefan. 2022. {``Curvature on {Graphs} via {Equilibrium
Measures}.''} September 5, 2022.
\url{https://doi.org/10.48550/arXiv.2202.01658}.

\bibitem[\citeproctext]{ref-sturm2006GeometryMetricMeasure}
Sturm, Karl-Theodor. 2006. {``On the Geometry of Metric Measure
Spaces.''} \emph{Acta Mathematica} 196 (1): 65--131.
\url{https://doi.org/10.1007/s11511-006-0002-8}.

\bibitem[\citeproctext]{ref-tong2021DiffusionEarthMovera}
Tong, Alexander Y., Guillaume Huguet, Amine Natik, Kincaid Macdonald,
Manik Kuchroo, Ronald Coifman, Guy Wolf, and Smita Krishnaswamy. 2021.
{``Diffusion {Earth Mover}'s {Distance} and {Distribution
Embeddings}.''} In \emph{Proceedings of the 38th {International
Conference} on {Machine Learning}}, 10336--46. PMLR.
\url{https://proceedings.mlr.press/v139/tong21a.html}.

\bibitem[\citeproctext]{ref-tong2021DatadrivenLearningGeometric}
Tong, Alexander, Frederick Wenkel, Kincaid Macdonald, Smita
Krishnaswamy, and Guy Wolf. 2021. {``Data-Driven Learning of Geometric
Scattering Modules for Gnns.''} In \emph{2021 {IEEE} 31st {International
Workshop} on {Machine Learning} for {Signal Processing} ({MLSP})}, 1--6.
IEEE. \url{https://ieeexplore.ieee.org/abstract/document/9596169/}.

\end{CSLReferences}




\end{document}
