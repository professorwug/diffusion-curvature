<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.358">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kincaid MacDonald">
<meta name="author" content="Dhananjay Bhaskar">
<meta name="author" content="Yanlei Zhang">
<meta name="author" content="Ian Adelstein">
<meta name="author" content="Smita Krishnaswamy">
<meta name="dcterms.date" content="2024-08-12">
<meta name="keywords" content="Manifold Learning, Geometric Deep Learning, Graph Curvature, Point Clouds, Diffusion Geometry, Topological Data Analysis, Neural Loss Landscapes, Single-Cell Data">

<title>Bridging Diffusion Geometry to Curvature</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="citation_title" content="Bridging Diffusion Geometry to Curvature">
<meta name="citation_abstract" content="Modeling data geometry has proven a rich source of interpretable features: quantifying ‘the shape’ of molecules, cellular trajectories, and code enhances downstream classification performance. Yet, creating these features requires a theoretical ‘Rosetta Stone’ to bridge geometric theory into the noisy, discrete world of real data. Various bridging paradigms have been proposed, most prominently *diffusion geometry* – and have given rise to manifold distance estimators, and manifold embedding techniques.However, the estimation of curvature remains relatively unexplored. Curvature, being the most local geometric measure, is among the most challenging concepts to bridge into the discrete realm; for this reason, it is also a sterling test a given paradigm’s ability to translate geometric theory into the sampled data.Here, we introduce *Diffusion Curvature*, a new definition of scalar curvature on point clouds which inherits diffusion geometry’s robustness to noise and sampling. In our benchmarks, diffusion curvature proves itself the *only* method capable of robustly differentiating positive and negative curvature in high dimensions, while its absence of parameters requiring user tuning makes it significantly more user-friendly than other methods.We describe theoretical connections between diffusion curvature and the Ollivier-Ricci curvature, and introduce a general paradigm for comparing diffusions across graphs.We apply diffusion curvature to neural loss landscapes, single-cell RNA data [bit about what we find] motivating the practical utility of our method.">
<meta name="citation_keywords" content="Manifold Learning,Geometric Deep Learning,Graph Curvature,Point Clouds,Diffusion Geometry,Topological Data Analysis,Neural Loss Landscapes,Single-Cell Data">
<meta name="citation_author" content="Kincaid MacDonald">
<meta name="citation_author" content="Dhananjay Bhaskar">
<meta name="citation_author" content="Yanlei Zhang">
<meta name="citation_author" content="Ian Adelstein">
<meta name="citation_author" content="Smita Krishnaswamy">
<meta name="citation_publication_date" content="2024-08-12">
<meta name="citation_cover_date" content="2024-08-12">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-08-12">
<meta name="citation_language" content="en">
<meta name="citation_firstpage" content="undefined">
<meta name="citation_journal_title" content="IEEE TPAMI">
<meta name="citation_reference" content="citation_title=Bishop–Gromov inequality;,citation_abstract=In mathematics, the Bishop–Gromov inequality is a comparison theorem in Riemannian geometry, named after Richard L. Bishop and Mikhail Gromov. It is closely related to Myers’ theorem, and is the key point in the proof of Gromov’s compactness theorem.;,citation_publication_date=2021-12-08;,citation_cover_date=2021-12-08;,citation_year=2021;,citation_fulltext_html_url=https://en.wikipedia.org/w/index.php?title=Bishop%E2%80%93Gromov_inequality&amp;amp;amp;oldid=1059331416;,citation_language=en-US;,citation_journal_title=Wikipedia;">
<meta name="citation_reference" content="citation_title=Quantifying Local Extrinsic Curvature in Neural Manifolds;,citation_abstract=The neural manifold hypothesis postulates that the activity of a neural population forms a low-dimensional manifold within the larger neural state space, whose structure reflects the structure of the encoded task variables. Many dimensionality reduction techniques have been used to study the structure of neural manifolds, but these methods do not provide an explicit parameterization of the manifold, and fail to capture the global structure of topologically nontrivial manifolds. Topological data analysis methods can reveal the shared topological structure between neural manifolds and the task variables they represent, but fail to capture much of the geometric information including distance, angles, and curvature. In this work, we leverage tools from Riemannian geometry and topologically-aware deep generative models to introduce a novel approach for studying the geometry of neural manifolds. This approach (1) computes an explicit parameterization of the manifolds and (2) estimates their local extrinsic curvature. Our approach correctly estimates the geometry of synthetic neural manifolds generated from smooth deformations of circles, spheres, and tori. We expect this approach to open new avenues of inquiry exploring geometric neural correlates of perception and behavior, and provide a new means to compare representations in biological and artificial neural systems.;,citation_author=Francisco Acosta;,citation_author=S. Sanborn;,citation_author=K. D. Duc;,citation_author=Manu S. Madhav;,citation_author=Nina Miolane;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.semanticscholar.org/paper/Quantifying-Local-Extrinsic-Curvature-in-Neural-Acosta-Sanborn/717aa2cd8495ac3681b84f9879ce7dac3e3a1019;">
<meta name="citation_reference" content="citation_title=Latent Space Oddity: On the Curvature of Deep Generative Models;,citation_abstract=Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear &amp;amp;amp;quot;generator&amp;quot; function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.;,citation_author=Georgios Arvanitidis;,citation_author=L. K. Hansen;,citation_author=Søren Hauberg;,citation_publication_date=2017-10-31;,citation_cover_date=2017-10-31;,citation_year=2017;,citation_fulltext_html_url=https://www.semanticscholar.org/paper/Latent-Space-Oddity%3A-on-the-Curvature-of-Deep-Arvanitidis-Hansen/9f696b7156716c978b62a92714e7038a99f7a53c;,citation_journal_title=arXiv: Machine Learning;">
<meta name="citation_reference" content="citation_title=Conformal Flattening by Curvature Prescription and Metric Scaling;,citation_abstract=Abstract We present an efficient method to conformally parameterize 3D mesh data sets to the plane. The idea behind our method is to concentrate all the 3D curvature at a small number of select mesh vertices, called cone singularities, and then cut the mesh through those singular vertices to obtain disk topology. The singular vertices are chosen automatically. As opposed to most previous methods, our flattening process involves only the solution of linear systems of Poisson equations, thus is very efficient. Our method is shown to be faster than existing methods, yet generates parameterizations having comparable quasi‐conformal distortion.;,citation_author=Mirela Ben‐Chen;,citation_author=Craig Gotsman;,citation_author=Guy Bunin;,citation_publication_date=2008-04;,citation_cover_date=2008-04;,citation_year=2008;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2008.01142.x;,citation_issue=2;,citation_doi=10.1111/j.1467-8659.2008.01142.x;,citation_issn=0167-7055, 1467-8659;,citation_volume=27;,citation_language=en-US;,citation_journal_title=Computer Graphics Forum;,citation_journal_abbrev=Computer Graphics Forum;">
<meta name="citation_reference" content="citation_title=Diffusion curvature for estimating local curvature in high dimensional data;,citation_author=Dhananjay Bhaskar;,citation_author=Kincaid MacDonald;,citation_author=Oluwadamilola Fasina;,citation_author=Dawson Thomas;,citation_author=Bastian Rieck;,citation_author=Ian Adelstein;,citation_author=Smita Krishnaswamy;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://proceedings.neurips.cc/paper_files/paper/2022/hash/88438dc62fc5c8777e2b5f1b4f6d37a2-Abstract-Conference.html;,citation_volume=35;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Learning graph geometry and topology using dynamical systems based message-passing;,citation_abstract=In this paper we introduce DYMAG: a message passing paradigm for GNNs built on the expressive power of continuous, multiscale graph-dynamics. Standard discretetime message passing algorithms implicitly make use of simplistic graph dynamics and aggregation schemes which limit their ability to capture fundamental graph topological properties. By contrast, DYMAG makes use of complex graph dynamics based on the heat and wave equation as well as a more complex equation which admits chaotic solutions. The continuous nature of the dynamics are leveraged to generate multiscale (dynamic-time snapshot) representations which we prove are linked to various graph topological and spectral properties. We demonstrate experimentally that DYMAG achieves superior performance in recovering the generating parameters of Erdös-Renyi and stochastic block model random graphs and the persistent homology of synthetic graphs and citation network. Since the behavior of proteins and biomolecules is sensitive to graph topology and exhibits important structure at multiple scales, we find that DYMAG outperforms other methods at predicting salient features of various biomolecules.;,citation_author=Dhananjay Bhaskar;,citation_author=Yanlei Zhang;,citation_author=Charles Xu;,citation_author=Xingzhi Sun;,citation_author=Oluwadamilola Fasina;,citation_author=Guy Wolf;,citation_author=Maximilian Nickel;,citation_author=Michael Perlmutter;,citation_author=Smita Krishnaswamy;,citation_publication_date=2024-07-07;,citation_cover_date=2024-07-07;,citation_year=2024;,citation_fulltext_html_url=http://arxiv.org/abs/2309.09924;,citation_language=en-US;">
<meta name="citation_reference" content="citation_title=Curvature of Hypergraphs via Multi-Marginal Optimal Transport;,citation_abstract=We introduce a novel definition of curvature for hypergraphs, a natural generalization of graphs, by introducing a multi-marginal optimal transport problem for a naturally defined random walk on the hypergraph. This curvature, termed coarse scalar curvature, extends a recent definition of Ricci curvature for Markov chains on metric spaces by Ollivier [Journal of Functional Analysis 256 (2009) 810–864], and is related to the scalar curvature when the hypergraph arises naturally from a Riemannian manifold. We investigate basic theoretical properties of the coarse scalar curvature and obtain several bounds. Empirical experiments demonstrate that coarse scalar curvatures detects “bridges” across connected components in hypergraphs, akin to the behavior of coarse Ricci curvatures on graphs.;,citation_author=Eric Cayeux;,citation_author=Shahab Asoodeh;,citation_author=Shahab Asoodeh;,citation_author=Shahab Asoodeh;,citation_author=Tingran Gao;,citation_author=Tingran Gao;,citation_author=James A. Evans;,citation_author=James A. Evans;,citation_publication_date=2018-12-01;,citation_cover_date=2018-12-01;,citation_year=2018;,citation_doi=10.1109/cdc.2018.8619706;">
<meta name="citation_reference" content="citation_title=Diffusion maps;,citation_abstract=In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.;,citation_author=Ronald R. Coifman;,citation_author=Stéphane Lafon;,citation_publication_date=2006-07-01;,citation_cover_date=2006-07-01;,citation_year=2006;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S1063520306000546;,citation_issue=1;,citation_doi=10.1016/j.acha.2006.04.006;,citation_issn=1063-5203;,citation_volume=21;,citation_language=en-US;,citation_journal_title=Applied and Computational Harmonic Analysis;,citation_journal_abbrev=Applied and Computational Harmonic Analysis;,citation_series_title=Special Issue: Diffusion Maps and Wavelets;">
<meta name="citation_reference" content="citation_title=Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework;,citation_abstract=Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop ORCHID, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that ORCHID curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.;,citation_author=Corinna Coupette;,citation_author=Sebastian Dalleiger;,citation_author=Bastian Rieck;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2210.12048;,citation_doi=10.48550/ARXIV.2210.12048;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Curvature filtrations for graph generative model evaluation | Proceedings of the 37th International Conference on Neural Information Processing Systems;,citation_fulltext_html_url=https://dl.acm.org/doi/10.5555/3666122.3668874;">
<meta name="citation_reference" content="citation_title=Neural FIM for learning Fisher Information Metrics from point cloud data;,citation_abstract=Although data diffusion embeddings are ubiquitous in unsupervised learning and have proven to be a viable technique for uncovering the underlying intrinsic geometry of data, diffusion embeddings are inherently limited due to their discrete nature. To this end, we propose neural FIM, a method for computing the Fisher information metric (FIM) from point cloud data - allowing for a continuous manifold model for the data. Neural FIM creates an extensible metric space from discrete point cloud data such that information from the metric can inform us of manifold characteristics such as volume and geodesics. We demonstrate Neural FIM’s utility in selecting parameters for the PHATE visualization method as well as its ability to obtain information pertaining to local volume illuminating branching points and cluster centers embeddings of a toy dataset and two single-cell datasets of IPSC reprogramming and PBMCs (immune cells).;,citation_author=Oluwadamilola Fasina;,citation_author=Guillaume Huguet;,citation_author=Alexander Tong;,citation_author=Yanlei Zhang;,citation_author=Guy Wolf;,citation_author=Maximilian Nickel;,citation_author=Ian Adelstein;,citation_author=Smita Krishnaswamy;,citation_publication_date=2023-06-11;,citation_cover_date=2023-06-11;,citation_year=2023;,citation_fulltext_html_url=http://arxiv.org/abs/2306.06062;,citation_doi=10.48550/arXiv.2306.06062;">
<meta name="citation_reference" content="citation_title=Non-negative Ollivier curvature on graphs, reverse Poincaré inequality, Buser inequality, Liouville property, Harnack inequality and eigenvalue estimates;,citation_abstract=We prove that for combinatorial graphs with non-negative Ollivier curvature, one has \[ \|P_t \mu - P_t \nu\|_1 \leq \frac{W_1(\mu,\nu)}{\sqrt{t}} \] for all probability measures $\mu,\nu$ where $P_t$ is the heat semigroup and $W_1$ is the $\ell_1$-Wasserstein distance. This turns out to be an equivalent formulation of a version of reverse Poincare inequality. Furthermore, this estimate allows us to prove Buser inequality, Liouville property and the the eigenvalue estimate $\lambda_1 \geq \log(2)/\operatorname{diam}^2$.;,citation_author=Florentin Münch;,citation_author=Florentin Münch;,citation_publication_date=2019-07-31;,citation_cover_date=2019-07-31;,citation_year=2019;,citation_journal_title=arXiv: Differential Geometry;">
<meta name="citation_reference" content="citation_title=A Loss Curvature Perspective on Training Instability in Deep Learning;,citation_abstract=In this work, we study the evolution of the loss Hessian across many classification tasks in order to understand the effect the curvature of the loss has on the training dynamics. Whereas prior work has focused on how different learning rates affect the loss Hessian observed during training, we also analyze the effects of model initialization, architectural choices, and common training heuristics such as gradient clipping and learning rate warmup. Our results demonstrate that successful model and hyperparameter choices allow the early optimization trajectory to either avoid – or navigate out of – regions of high curvature and into flatter regions that tolerate a higher learning rate. Our results suggest a unifying perspective on how disparate mitigation strategies for training instability ultimately address the same underlying failure mode of neural network optimization, namely poor conditioning. Inspired by the conditioning perspective, we show that learning rate warmup can improve training stability just as much as batch normalization, layer normalization, MetaInit, GradInit, and Fixup initialization.;,citation_author=Justin Gilmer;,citation_author=Behrooz Ghorbani;,citation_author=Ankush Garg;,citation_author=Sneha Kudugunta;,citation_author=Behnam Neyshabur;,citation_author=David Cardoze;,citation_author=George Dahl;,citation_author=Zachary Nado;,citation_author=Orhan Firat;,citation_publication_date=2021-10-08;,citation_cover_date=2021-10-08;,citation_year=2021;,citation_fulltext_html_url=http://arxiv.org/abs/2110.04369;">
<meta name="citation_reference" content="citation_title=The Utopia of rules : On technology, stupidity, and the secret joys of bureaucracy;,citation_author=David Graeber;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_isbn=978-1-61219-374-8;,citation_language=en-US;">
<meta name="citation_reference" content="citation_title=An Intrinsic Approach to Scalar-Curvature Estimation for Point Clouds;,citation_abstract=We introduce an intrinsic estimator for the scalar curvature of a data set presented as a finite metric space. Our estimator depends only on the metric structure of the data and not on an embedding in $\mathbb{R}^n$. We show that the estimator is consistent in the sense that for points sampled from a probability measure on a compact Riemannian manifold, the estimator converges to the scalar curvature as the number of points increases. To justify its use in applications, we show that the estimator is stable with respect to perturbations of the metric structure, e.g., noise in the sample or error estimating the intrinsic metric. We validate our estimator experimentally on synthetic data that is sampled from manifolds with specified curvature.;,citation_author=Abigail Hickok;,citation_author=Andrew J. Blumberg;,citation_publication_date=2023-08-04;,citation_cover_date=2023-08-04;,citation_year=2023;,citation_fulltext_html_url=http://arxiv.org/abs/2308.02615;,citation_doi=10.48550/arXiv.2308.02615;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction;,citation_abstract=Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call heat geodesic embeddings. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state of the art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-sequencing datasets with both continuum and cluster structure, where our method enables interpolation of withheld timepoints of data. Finally, we show that parameters of our more general method can be configured to give results similar to PHATE (a state-of-the-art diffusion based manifold learning method) as well as SNE (an attraction/repulsion neighborhood based method that forms the basis of t-SNE).;,citation_author=Guillaume Huguet;,citation_author=Alexander Tong;,citation_author=Edward De Brouwer;,citation_author=Yanlei Zhang;,citation_author=Guy Wolf;,citation_author=Ian Adelstein;,citation_author=Smita Krishnaswamy;,citation_publication_date=2023-05-30;,citation_cover_date=2023-05-30;,citation_year=2023;,citation_fulltext_html_url=http://arxiv.org/abs/2305.19043;,citation_doi=10.48550/arXiv.2305.19043;">
<meta name="citation_reference" content="citation_title=ICML On the Expressive Power of Ollivier-Ricci Curvature on Graphs;,citation_fulltext_html_url=https://icml.cc/virtual/2023/27561;">
<meta name="citation_reference" content="citation_title=Gradient flows of the entropy for finite Markov chains;,citation_abstract=Let K be an irreducible and reversible Markov kernel on a finite set X. We construct a metric W on the set of probability measures on X and show that with respect to this metric, the law of the continuous time Markov chain evolves as the gradient flow of the entropy. This result is a discrete counterpart of the Wasserstein gradient flow interpretation of the heat flow in Rn by Jordan, Kinderlehrer and Otto (1998). The metric W is similar to, but different from, the L2-Wasserstein metric, and is defined via a discrete variant of the Benamou–Brenier formula.;,citation_author=Jan Maas;,citation_author=Jan Maas;,citation_publication_date=2011-10-15;,citation_cover_date=2011-10-15;,citation_year=2011;,citation_issue=8;,citation_doi=10.1016/j.jfa.2011.06.009;,citation_volume=261;,citation_journal_title=Journal of Functional Analysis;">
<meta name="citation_reference" content="citation_title=Entropic Ricci Curvature for Discrete Spaces;,citation_abstract=We give a short overview on a recently developed notion of Ricci curvature for discrete spaces. This notion relies on geodesic convexity properties of the relative entropy along geodesics in the space of probability densities, for a metric which is similar to (but different from) the 2-Wasserstein metric. The theory can be considered as a discrete counterpart to the theory of Ricci curvature for geodesic measure spaces developed by Lott–Sturm–Villani.;,citation_author=Jan Maas;,citation_author=Jan Maas;,citation_publication_date=2017-01-01;,citation_cover_date=2017-01-01;,citation_year=2017;,citation_doi=10.1007/978-3-319-58002-9_5;">
<meta name="citation_reference" content="citation_title=Accelerated Evaluation of Ollivier-Ricci Curvature Lower Bounds: Bridging Theory and Computation;,citation_abstract=Curvature serves as a potent and descriptive invariant, with its efficacy validated both theoretically and practically within graph theory. We employ a definition of generalized Ricci curvature proposed by Ollivier, which Lin and Yau later adapted to graph theory, known as Ollivier-Ricci curvature (ORC). ORC measures curvature using the Wasserstein distance, thereby integrating geometric concepts with probability theory and optimal transport. Jost and Liu previously discussed the lower bound of ORC by showing the upper bound of the Wasserstein distance. We extend the applicability of these bounds to discrete spaces with metrics on integers, specifically hypergraphs. Compared to prior work on ORC in hypergraphs by Coupette, Dalleiger, and Rieck, which faced computational challenges, our method introduces a simplified approach with linear computational complexity, making it particularly suitable for analyzing large-scale networks. Through extensive simulations and application to synthetic and real-world datasets, we demonstrate the significant improvements our method offers in evaluating ORC.;,citation_author=Wonwoo Kang;,citation_author=Heehyun Park;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://arxiv.org/abs/2405.13302;,citation_doi=10.48550/ARXIV.2405.13302;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=On Explicit Curvature Regularization in Deep Generative Models;,citation_abstract=We propose a family of curvature-based regularization terms for deep generative model learning. Explicit coordinate-invariant formulas for both intrinsic and extrinsic curvature measures are derived for the case of arbitrary data manifolds embedded in higher-dimensional Euclidean space. Because computing the curvature is a highly computation-intensive process involving the evaluation of second-order derivatives, efficient formulas are derived for approximately evaluating intrinsic and extrinsic curvatures. Comparative studies are conducted that compare the relative efficacy of intrinsic versus extrinsic curvature-based regularization measures, as well as performance comparisons against existing autoencoder training methods. Experiments involving noisy motion capture data confirm that curvature-based methods outperform existing autoencoder regularization methods, with intrinsic curvature measures slightly more effective than extrinsic curvature measures.;,citation_author=Yonghyeon Lee;,citation_author=Frank Chongwoo Park;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2309.10237;,citation_doi=10.48550/ARXIV.2309.10237;,citation_conference=arXiv;">
<meta name="citation_reference" content="citation_title=Visualizing Data using t-SNE;,citation_abstract=We present a new technique called &amp;amp;amp;quot;t-SNE&amp;quot; that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.;,citation_author=Laurens Maaten;,citation_author=Geoffrey Hinton;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_fulltext_html_url=http://jmlr.org/papers/v9/vandermaaten08a.html;,citation_issue=86;,citation_issn=1533-7928;,citation_volume=9;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Coarse Ricci curvature of hypergraphs and its generalization;,citation_abstract=In the present paper, we introduce a concept of Ricci curvature on hypergraphs for a nonlinear Laplacian. We prove that our definition of the Ricci curvature is a generalization of Lin-Lu-Yau coarse Ricci curvature for graphs to hypergraphs. We also show a lower bound of nonzero eigenvalues of Laplacian, gradient estimate of heat flow, and diameter bound of Bonnet-Myers type for our curvature notion. This research leads to understanding how nonlinearity of Laplacian causes complexity of curvatures.;,citation_author=Masahiro Ikeda;,citation_author=Masahiro Ikeda;,citation_author=Yu Kitabeppu;,citation_author=Yu Kitabeppu;,citation_author=Yuuki Takai;,citation_author=Yuuki Takai;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv: Metric Geometry;">
<meta name="citation_reference" content="citation_title=The heat flow on metric random walk spaces;,citation_abstract=Abstract In this paper we study the Heat Flow on Metric Random Walk Spaces, which unifies into a broad framework the heat flow on locally finite weighted connected graphs, the heat flow determined by finite Markov chains and some nonlocal evolution problems. We give different characterizations of the ergodicity and prove that a metric random walk space with positive Ollivier-Ricci curvature is ergodic. Furthermore, we prove a Cheeger inequality and, as a consequence, we show that a Poincare inequality holds if, and only if, an isoperimetric inequality holds. We also study the Bakry-Emery curvature-dimension condition and its relation with functional inequalities like the Poincare inequality and the transport-information inequalities.;,citation_author=José M. Mazón;,citation_author=Marcos Solera;,citation_author=Julián Toledo;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1016/j.jmaa.2019.123645;,citation_journal_title=Journal of Mathematical Analysis and Applications;">
<meta name="citation_reference" content="citation_title=Curvature of Nonlocal Markov Generators;,citation_abstract=Bakry’s curvature-dimension condition will be extended to certain nonlocal Markov generators. In particular this gives rise to a possible notion of curvature for graphs. 1. Definition of Curvature Let (Ω, μ) be a probability space and L a self-adjoint negative but not necessarily bounded operator on L2(μ) given by Lf(x) := ∫ (f(y)− f(x))K(x, y)μ(dy) (1) where K is a non negative symmetric kernel. Obviously L remains unchanged if we change K on the diagonal. By Pt = e we denote the continuous contraction semigroup on L2(μ) with generator L. We will assume that Pt is ergodic and that there exists an algebra A ⊆ n domL of bounded functions which is a form core of L. Then the Beurling–Deny condition implies that Pt is a symmetric Markov semigroup, i.e., Pt preserves positivity and extends to a continuous contraction semigroup on Lp(μ) for all 1 ≤ p 0. Let us say a a word about the motivation for this definition. Assume L is the Laplacian on a Riemannian manifold, then;,citation_author=Michael Schmuckenschläger;,citation_author=Michael Schmuckenschläger;,citation_publication_date=1998-01-01;,citation_cover_date=1998-01-01;,citation_year=1998;">
<meta name="citation_reference" content="citation_title=Visualizing structure and transitions in high-dimensional biological data;,citation_abstract=The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.;,citation_author=Kevin R. Moon;,citation_author=David Dijk;,citation_author=Zheng Wang;,citation_author=Scott Gigante;,citation_author=Daniel B. Burkhardt;,citation_author=William S. Chen;,citation_author=Kristina Yim;,citation_author=Antonia Elzen;,citation_author=Matthew J. Hirn;,citation_author=Ronald R. Coifman;,citation_author=Natalia B. Ivanova;,citation_author=Guy Wolf;,citation_author=Smita Krishnaswamy;,citation_publication_date=2019-12;,citation_cover_date=2019-12;,citation_year=2019;,citation_fulltext_html_url=https://www.nature.com/articles/s41587-019-0336-3;,citation_issue=12, 12;,citation_doi=10.1038/s41587-019-0336-3;,citation_issn=1546-1696;,citation_volume=37;,citation_language=en-US;,citation_journal_title=Nature Biotechnology;,citation_journal_abbrev=Nat Biotechnol;,citation_publisher=Nature Publishing Group;">
<meta name="citation_reference" content="citation_title=Ricci curvature of Markov chains on metric spaces;,citation_abstract=We define the coarse Ricci curvature of metric spaces in terms of how much small balls are closer (in Wasserstein transportation distance) than their …;,citation_author=Yann Ollivier;,citation_publication_date=2009-02-01;,citation_cover_date=2009-02-01;,citation_year=2009;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S002212360800493X;,citation_issue=3;,citation_doi=10.1016/j.jfa.2008.11.001;,citation_issn=0022-1236;,citation_volume=256;,citation_language=en-US;,citation_journal_title=Journal of Functional Analysis;,citation_publisher=Academic Press;">
<meta name="citation_reference" content="citation_title=[PDF] The Riemannian Geometry of Deep Generative Models | Semantic Scholar;,citation_fulltext_html_url=https://www.semanticscholar.org/paper/The-Riemannian-Geometry-of-Deep-Generative-Models-Shao-Kumar/c6437e7fb44f13a5ff610c9daf8fa7ce4a6c1ec3;">
<meta name="citation_reference" content="citation_title=The heat kernel and its estimates;,citation_abstract=After a short survey of some of the reasons that make the heat kernel an important object of study, we review a number of basic heat kernel estimates. We then describe recent results concerning (a) the heat kernel on certain manifolds with ends, and (b) the heat kernel with the Neumann or Dirichlet boundary condition in inner uniform Euclidean domains.;,citation_author=Laurent Saloff-Coste;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_fulltext_html_url=https://projecteuclid.org/euclid.aspm/1543086330;,citation_doi=10.2969/aspm/05710405;,citation_language=en-US;,citation_conference_title=Advanced Studies in Pure Mathematics;">
<meta name="citation_reference" content="citation_title=Forman’s Ricci Curvature - From Networks to Hypernetworks;,citation_abstract=Networks and their higher order generalizations, such as hypernetworks or multiplex networks are ever more popular models in the applied sciences. However, methods developed for the study of their structural properties go little beyond the common name and the heavy reliance of combinatorial tools. We show that, in fact, a geometric unifying approach is possible, by viewing them as polyhedral complexes endowed with a simple, yet, the powerful notion of curvature - the Forman Ricci curvature. We systematically explore some aspects related to the modeling of weighted and directed hypernetworks and present expressive and natural choices involved in their definitions. A benefit of this approach is a simple method of structure-preserving embedding of hypernetworks in Euclidean N-space. Furthermore, we introduce a simple and efficient manner of computing the well established Ollivier-Ricci curvature of a hypernetwork.;,citation_author=Emil Saucan;,citation_author=Melanie Weber;,citation_editor=Luca Maria Aiello;,citation_editor=Chantal Cherifi;,citation_editor=Hocine Cherifi;,citation_editor=Renaud Lambiotte;,citation_editor=Pietro Lió;,citation_editor=Luis M. Rocha;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=http://link.springer.com/10.1007/978-3-030-05411-3_56;,citation_doi=10.1007/978-3-030-05411-3_56;,citation_isbn=978-3-030-05410-6 978-3-030-05411-3;,citation_volume=812;,citation_conference_title=Complex Networks and Their Applications VII;,citation_conference=Springer International Publishing;">
<meta name="citation_reference" content="citation_title=On the Expressive Power of Ollivier-Ricci Curvature on Graphs;,citation_abstract=Discrete curvature has recently been used in graph machine learning to improve performance, understand message-passing and assess structural differences between graphs. Despite these advancements, the theoretical properties of discrete curvature measures, such as their representational power and their relationship to graph features is yet to be fully explored. This paper studies Ollivier–Ricci curvature on graphs, providing both a discussion and empirical analysis of its expressivity, i.e. the ability to distinguish nonisomorphic graphs.;,citation_author=Joshua Southern;,citation_author=Jeremy Wayland;,citation_author=Michael Bronstein;,citation_author=Bastian Rieck;,citation_language=en-US;">
<meta name="citation_reference" content="citation_title=Curvature on Graphs via Equilibrium Measures;,citation_abstract=We introduce a notion of curvature on finite, combinatorial graphs. It can be easily computed by solving a linear system of equations. We show that graphs with curvature bounded below by $K&amp;amp;amp;gt;0$ have diameter bounded by $\mbox{diam}(G) \leq 2/K$ (a Bonnet-Myers theorem), that $\mbox{diam}(G) = 2/K$ implies that $G$ has constant curvature (a Cheng theorem) and that there is a spectral gap $\lambda_1 \geq K/(2n)$ (a Lichnerowicz theorem). It is computed for several families of graphs and often coincides with Ollivier curvature or Lin-Lu-Yau curvature. The von Neumann minimax theorem features prominently in the proofs.;,citation_author=Stefan Steinerberger;,citation_publication_date=2022-09-05;,citation_cover_date=2022-09-05;,citation_year=2022;,citation_fulltext_html_url=http://arxiv.org/abs/2202.01658;,citation_doi=10.48550/arXiv.2202.01658;">
<meta name="citation_reference" content="citation_title=On the geometry of metric measure spaces;,citation_abstract=We introduce and analyze lower (Ricci) curvature bounds$\underline{{Curv}} {\left( {M,d,m} \right)}$ ⩾ K for metric measure spaces${\left( {M,d,m} \right)}$. Our definition is based on convexity properties of the relative entropy$Ent{\left( { \cdot \left| m \right.} \right)}$regarded as a function on the L2-Wasserstein space of probability measures on the metric space${\left( {M,d} \right)}$. Among others, we show that$\underline{{Curv}} {\left( {M,d,m} \right)}$ ⩾ K implies estimates for the volume growth of concentric balls. For Riemannian manifolds,$\underline{{Curv}} {\left( {M,d,m} \right)}$ ⩾ K if and only if$Ric_{M} {\left( {\xi ,\xi } \right)}$ ⩾ K${\left| \xi \right|}^{2} $for all $\xi \in TM$.;,citation_author=Karl-Theodor Sturm;,citation_publication_date=2006-07-01;,citation_cover_date=2006-07-01;,citation_year=2006;,citation_fulltext_html_url=https://doi.org/10.1007/s11511-006-0002-8;,citation_issue=1;,citation_doi=10.1007/s11511-006-0002-8;,citation_issn=1871-2509;,citation_volume=196;,citation_language=en-US;,citation_journal_title=Acta Mathematica;,citation_journal_abbrev=Acta Math;">
<meta name="citation_reference" content="citation_title=A note on a Bonnet-Myers type diameter bound for graphs with positive entropic Ricci curvature;,citation_abstract=An equivalent definition of entropic Ricci curvature on discrete spaces was given in terms of the global gradient estimate. With a particular choice of the density function $\rho$, we obtain a localized gradient estimate, which in turns allow us to derive a Bonnet-Myers type diameter bound for graphs with positive entropic Ricci curvature. However, the case of the hypercubes indicates that the bound may be not optimal (where $\theta$ is chosen to be logarithmic mean by default). If $\theta$ is arithmetic mean, the Bakry-Emery criterion can be recovered and the diameter bound is optimal as it can be attained by the hypercubes.;,citation_author=Supanat Kamtue;,citation_author=Supanat Kamtue;,citation_author=Supanat Kamtue;,citation_publication_date=2020-03-02;,citation_cover_date=2020-03-02;,citation_year=2020;,citation_journal_title=arXiv: Probability;">
<meta name="citation_reference" content="citation_title=Generative AI for designing and validating easily synthesizable and structurally novel antibiotics;,citation_abstract=The rise of pan-resistant bacteria is creating an urgent need for structurally novel antibiotics. Artificial intelligence methods can discover new antibiotics, but existing methods have notable limitations. Property prediction models, which evaluate molecules one-by-one for a given property, scale poorly to large chemical spaces. Generative models, which directly design molecules, rapidly explore vast chemical spaces but generate molecules that are challenging to synthesize. Here we introduce SyntheMol, a generative model that designs new compounds, which are easy to synthesize, from a chemical space of nearly 30 billion molecules. We apply SyntheMol to design molecules that inhibit the growth of Acinetobacter baumannii, a burdensome Gram-negative bacterial pathogen. We synthesize 58 generated molecules and experimentally validate them, with six structurally novel molecules demonstrating antibacterial activity against A. baumannii and several other phylogenetically diverse bacterial pathogens. This demonstrates the potential of generative artificial intelligence to design structurally novel, synthesizable and effective small-molecule antibiotic candidates from vast chemical spaces, with empirical validation.;,citation_author=Kyle Swanson;,citation_author=Gary Liu;,citation_author=Denise B. Catacutan;,citation_author=Autumn Arnold;,citation_author=James Zou;,citation_author=Jonathan M. Stokes;,citation_publication_date=2024-03;,citation_cover_date=2024-03;,citation_year=2024;,citation_fulltext_html_url=https://www.nature.com/articles/s42256-024-00809-7;,citation_issue=3;,citation_doi=10.1038/s42256-024-00809-7;,citation_issn=2522-5839;,citation_volume=6;,citation_language=en-US;,citation_journal_title=Nature Machine Intelligence;,citation_journal_abbrev=Nat Mach Intell;,citation_publisher=Nature Publishing Group;">
<meta name="citation_reference" content="citation_title=Spectral Properties of Hypergraph Laplacian and Approximation Algorithms;,citation_abstract=The celebrated Cheeger’s Inequality (Alon and Milman 1985; Alon 1986) establishes a bound on the edge expansion of a graph via its spectrum. This inequality is central to a rich spectral theory of graphs, based on studying the eigenvalues and eigenvectors of the adjacency matrix (and other related matrices) of graphs. It has remained open to define a suitable spectral model for hypergraphs whose spectra can be used to estimate various combinatorial properties of the hypergraph. In this article, we introduce a new hypergraph Laplacian operator generalizing the Laplacian matrix of graphs. In particular, the operator is induced by a diffusion process on the hypergraph, such that within each hyperedge, measure flows from vertices having maximum weighted measure to those having minimum. Since the operator is nonlinear, we have to exploit other properties of the diffusion process to recover the Cheeger’s Inequality that relates hyperedge expansion with the “second eigenvalue” of the resulting Laplacian. However, we show that higher-order spectral properties cannot hold in general using the current framework. Since higher-order spectral properties do not hold for the Laplacian operator, we instead use the concept of procedural minimizers to consider higher-order Cheeger-like inequalities. For any k ∈ N, we give a polynomial-time algorithm to compute an O(log r)-approximation to the kth procedural minimizer, where r is the maximum cardinality of a hyperedge. We show that this approximation factor is optimal under the SSE hypothesis (introduced by Raghavendra and Steurer (2010)) for constant values of k. Moreover, using the factor-preserving reduction from vertex expansion in graphs to hypergraph expansion, we show that all our results for hypergraphs extend to vertex expansion in graphs.;,citation_author=T.-H. Hubert Chan;,citation_author=T.-H. Hubert Chan;,citation_author=Anand Louis;,citation_author=Anand Louis;,citation_author=Zhihao Gavin Tang;,citation_author=Zhihao Gavin Tang;,citation_author=Zhihao Gavin Tang;,citation_author=Chenzi Zhang;,citation_author=Chenzi Zhang;,citation_publication_date=2018-03-05;,citation_cover_date=2018-03-05;,citation_year=2018;,citation_issue=3;,citation_doi=10.1145/3178123;,citation_volume=65;,citation_journal_title=Journal of the ACM;">
<meta name="citation_reference" content="citation_title=A new transport distance and its associated Ricci curvature of hypergraphs;,citation_abstract=The coarse Ricci curvature of graphs introduced by Ollivier as well as its modification by Lin-Lu-Yau have been studied from various aspects. In this paper, we propose a new transport distance appropriate for hypergraphs and study a generalization of Lin-Lu-Yau type curvature of hypergraphs. As an application, we derive a Bonnet-Myers type estimate for hypergraphs under a lower Ricci curvature bound associated with our transport distance. We remark that our transport distance is new even for graphs and worthy of further study.;,citation_author=Tomoya Akamatsu;,citation_author=Tomoya Akamatsu;,citation_publication_date=2021-05-16;,citation_cover_date=2021-05-16;,citation_year=2021;,citation_journal_title=arXiv: Metric Geometry;">
<meta name="citation_reference" content="citation_title=Data-driven learning of geometric scattering modules for gnns;,citation_author=Alexander Tong;,citation_author=Frederick Wenkel;,citation_author=Kincaid Macdonald;,citation_author=Smita Krishnaswamy;,citation_author=Guy Wolf;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://ieeexplore.ieee.org/abstract/document/9596169/;,citation_conference_title=2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Diffusion Earth Mover’s Distance and Distribution Embeddings;,citation_abstract=We propose a new fast method of measuring distances between large numbers of related high dimensional datasets called the Diffusion Earth Mover’s Distance (EMD). We model the datasets as distributions supported on common data graph that is derived from the affinity matrix computed on the combined data. In such cases where the graph is a discretization of an underlying Riemannian closed manifold, we prove that Diffusion EMD is topologically equivalent to the standard EMD with a geodesic ground distance. Diffusion EMD can be computed in {Õ}(n) time and is more accurate than similarly fast algorithms such as tree-based EMDs. We also show Diffusion EMD is fully differentiable, making it amenable to future uses in gradient-descent frameworks such as deep neural networks. Finally, we demonstrate an application of Diffusion EMD to single cell data collected from 210 COVID-19 patient samples at Yale New Haven Hospital. Here, Diffusion EMD can derive distances between patients on the manifold of cells at least two orders of magnitude faster than equally accurate methods. This distance matrix between patients can be embedded into a higher level patient manifold which uncovers structure and heterogeneity in patients. More generally, Diffusion EMD is applicable to all datasets that are massively collected in parallel in many medical and biological systems.;,citation_author=Alexander Y. Tong;,citation_author=Guillaume Huguet;,citation_author=Amine Natik;,citation_author=Kincaid Macdonald;,citation_author=Manik Kuchroo;,citation_author=Ronald Coifman;,citation_author=Guy Wolf;,citation_author=Smita Krishnaswamy;,citation_publication_date=2021-07-01;,citation_cover_date=2021-07-01;,citation_year=2021;,citation_fulltext_html_url=https://proceedings.mlr.press/v139/tong21a.html;,citation_issn=2640-3498;,citation_language=en-US;,citation_conference_title=Proceedings of the 38th International Conference on Machine Learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=A Poset-Based Approach to Curvature of Hypergraphs;,citation_abstract=In this contribution, we represent hypergraphs as partially ordered sets or posets, and provide a geometric framework based on posets to compute the Forman–Ricci curvature of vertices as well as hyperedges in hypergraphs. Specifically, we first provide a canonical method to construct a two-dimensional simplicial complex associated with a hypergraph, such that the vertices of the simplicial complex represent the vertices and hyperedges of the original hypergraph. We then define the Forman–Ricci curvature of the vertices and the hyperedges as the scalar curvature of the associated vertices in the simplicial complex. Remarkably, Forman–Ricci curvature has a simple combinatorial expression and it can effectively capture the variation in symmetry or asymmetry over a hypergraph. Finally, we perform an empirical study involving computation and analysis of the Forman–Ricci curvature of hyperedges in several real-world hypergraphs. We find that Forman–Ricci curvature shows a moderate to high absolute correlation with standard hypergraph measures such as eigenvector centrality and cardinality. Our results suggest that the notion of Forman–Ricci curvature extended to hypergraphs in this work can be used to gain novel insights on the organization of higher-order interactions in real-world hypernetworks.;,citation_author=Yasharth Yadav;,citation_author=Areejit Samal;,citation_author=Emil Saucan;,citation_publication_date=2022-02-20;,citation_cover_date=2022-02-20;,citation_year=2022;,citation_fulltext_html_url=https://www.mdpi.com/2073-8994/14/2/420;,citation_issue=2;,citation_doi=10.3390/sym14020420;,citation_issn=2073-8994;,citation_volume=14;,citation_language=en-US;,citation_journal_title=Symmetry;,citation_journal_abbrev=Symmetry;">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">2</span> Background</a>
  <ul class="collapse">
  <li><a href="#curvature-in-the-continuous-setting" id="toc-curvature-in-the-continuous-setting" class="nav-link" data-scroll-target="#curvature-in-the-continuous-setting"><span class="header-section-number">2.1</span> Curvature in the Continuous Setting</a></li>
  <li><a href="#the-discrete-setting" id="toc-the-discrete-setting" class="nav-link" data-scroll-target="#the-discrete-setting"><span class="header-section-number">2.2</span> The Discrete Setting</a></li>
  <li><a href="#graph-diffusion" id="toc-graph-diffusion" class="nav-link" data-scroll-target="#graph-diffusion"><span class="header-section-number">2.3</span> Graph Diffusion</a></li>
  <li><a href="#ollivier-ricci-curvature" id="toc-ollivier-ricci-curvature" class="nav-link" data-scroll-target="#ollivier-ricci-curvature"><span class="header-section-number">2.4</span> Ollivier-Ricci Curvature</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="header-section-number">3</span> Methods</a>
  <ul class="collapse">
  <li><a href="#diffusion-energy-is-bounded-by-ollivier-ricci-curvature" id="toc-diffusion-energy-is-bounded-by-ollivier-ricci-curvature" class="nav-link" data-scroll-target="#diffusion-energy-is-bounded-by-ollivier-ricci-curvature"><span class="header-section-number">3.1</span> Diffusion Energy is bounded by Ollivier-Ricci curvature</a></li>
  <li><a href="#a-motivating-example-entropic-diffusion-curvature-recovers-a-bishop-gromov-volume-comparison" id="toc-a-motivating-example-entropic-diffusion-curvature-recovers-a-bishop-gromov-volume-comparison" class="nav-link" data-scroll-target="#a-motivating-example-entropic-diffusion-curvature-recovers-a-bishop-gromov-volume-comparison"><span class="header-section-number">3.2</span> A Motivating Example: Entropic Diffusion Curvature Recovers a Bishop-Gromov Volume Comparison</a></li>
  <li><a href="#diffusion-trajectory-aligment-comparing-diffusion-across-point-clouds" id="toc-diffusion-trajectory-aligment-comparing-diffusion-across-point-clouds" class="nav-link" data-scroll-target="#diffusion-trajectory-aligment-comparing-diffusion-across-point-clouds"><span class="header-section-number">3.3</span> Diffusion Trajectory Aligment: Comparing Diffusion Across Point Clouds</a>
  <ul class="collapse">
  <li><a href="#problem-setting-motivation" id="toc-problem-setting-motivation" class="nav-link" data-scroll-target="#problem-setting-motivation"><span class="header-section-number">3.3.1</span> Problem Setting &amp; Motivation</a></li>
  <li><a href="#diffusion-trajectory-alignment" id="toc-diffusion-trajectory-alignment" class="nav-link" data-scroll-target="#diffusion-trajectory-alignment"><span class="header-section-number">3.3.2</span> Diffusion Trajectory Alignment</a></li>
  <li><a href="#the-curvature-agnostic-kernel-automatic-bandwidth-tuning" id="toc-the-curvature-agnostic-kernel-automatic-bandwidth-tuning" class="nav-link" data-scroll-target="#the-curvature-agnostic-kernel-automatic-bandwidth-tuning"><span class="header-section-number">3.3.3</span> The Curvature-Agnostic Kernel &amp; Automatic Bandwidth Tuning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#the-curvature-colosseum-a-discrete-curvature-benchmarking-suite" id="toc-the-curvature-colosseum-a-discrete-curvature-benchmarking-suite" class="nav-link" data-scroll-target="#the-curvature-colosseum-a-discrete-curvature-benchmarking-suite"><span class="header-section-number">4</span> The Curvature Colosseum: A Discrete Curvature Benchmarking Suite</a>
  <ul class="collapse">
  <li><a href="#performance-in-low-dimensions-under-noise" id="toc-performance-in-low-dimensions-under-noise" class="nav-link" data-scroll-target="#performance-in-low-dimensions-under-noise"><span class="header-section-number">4.1</span> Performance in Low Dimensions under Noise</a></li>
  <li><a href="#performance-in-high-dimensions-under-noise" id="toc-performance-in-high-dimensions-under-noise" class="nav-link" data-scroll-target="#performance-in-high-dimensions-under-noise"><span class="header-section-number">4.2</span> Performance in High Dimensions under Noise</a></li>
  <li><a href="#sign-differentiation" id="toc-sign-differentiation" class="nav-link" data-scroll-target="#sign-differentiation"><span class="header-section-number">4.3</span> Sign Differentiation</a></li>
  <li><a href="#resilience-to-parameters" id="toc-resilience-to-parameters" class="nav-link" data-scroll-target="#resilience-to-parameters"><span class="header-section-number">4.4</span> Resilience to Parameters</a></li>
  </ul></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications"><span class="header-section-number">5</span> Applications</a>
  <ul class="collapse">
  <li><a href="#loss-landscapes" id="toc-loss-landscapes" class="nav-link" data-scroll-target="#loss-landscapes"><span class="header-section-number">5.1</span> Loss Landscapes</a></li>
  <li><a href="#curvature-as-a-tda-filtration" id="toc-curvature-as-a-tda-filtration" class="nav-link" data-scroll-target="#curvature-as-a-tda-filtration"><span class="header-section-number">5.2</span> Curvature as a TDA Filtration</a></li>
  </ul></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work"><span class="header-section-number">6</span> Related Work</a>
  <ul class="collapse">
  <li><a href="#foreman-ricci-curvature" id="toc-foreman-ricci-curvature" class="nav-link" data-scroll-target="#foreman-ricci-curvature"><span class="header-section-number">6.1</span> Foreman Ricci Curvature</a></li>
  <li><a href="#hickock-blumbergs-volume-comparison-curvature" id="toc-hickock-blumbergs-volume-comparison-curvature" class="nav-link" data-scroll-target="#hickock-blumbergs-volume-comparison-curvature"><span class="header-section-number">6.2</span> Hickock &amp; Blumberg’s Volume Comparison Curvature</a></li>
  <li><a href="#sritharan" id="toc-sritharan" class="nav-link" data-scroll-target="#sritharan"><span class="header-section-number">6.3</span> Sritharan</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">7</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF (ieee)</a></li></ul></div><div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="index-preview.html"><i class="bi bi-journal-code"></i>Article Notebook</a></li><li><a href="readme-paper-preview.html"><i class="bi bi-journal-code"></i>Publishing from Zetteldev with Quarto Manuscripts</a></li><li><a href="index-ricci-preview.html"><i class="bi bi-journal-code"></i>A Heat Diffusion-based Ricci Curvature and Applications to GNNs</a></li><li><a href="../nbs/3d-diffusion-trajectory-distance-normalization-preview.html"><i class="bi bi-journal-code"></i>Standard libraries</a></li><li><a href="../nbs/2a-Diffusion-Curvatures-of-Toy-Manifolds-preview.html"><i class="bi bi-journal-code"></i>Standard libraries</a></li><li><a href="../nbs/2-Toy-Manifolds-Benchmark-preview.html"><i class="bi bi-journal-code"></i>Standard libraries</a></li><li><a href="../nbs/5-sign-prediction-tests-preview.html"><i class="bi bi-journal-code"></i>Standard libraries</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://github.com/professorwug/diffusion-curvature/" target="_blank"><i class="bi bi-github"></i>GitHub Repo</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bridging Diffusion Geometry to Curvature</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Kincaid MacDonald <a href="mailto:kincaid@aya.yale.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0009-0006-4686-7488" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Yale
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Dhananjay Bhaskar <a href="https://orcid.org/0000-0002-7859-8394" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            MILA
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Yanlei Zhang <a href="https://orcid.org/0000-0002-7859-8394" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            MILA
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Ian Adelstein <a href="https://orcid.org/0000-0002-7859-8394" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Yale Department of Math
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Smita Krishnaswamy <a href="https://orcid.org/0000-0002-7859-8394" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Yale Department of Applied Math
          </p>
        <p class="affiliation">
            Yale School of Medicine
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 12, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    Modeling data geometry has proven a rich source of interpretable features: quantifying ‘the shape’ of molecules, cellular trajectories, and code enhances downstream classification performance. Yet, creating these features requires a theoretical ‘Rosetta Stone’ to bridge geometric theory into the noisy, discrete world of real data. Various bridging paradigms have been proposed, most prominently <em>diffusion geometry</em> – and have given rise to manifold distance estimators, and manifold embedding techniques.However, the estimation of curvature remains relatively unexplored. Curvature, being the most local geometric measure, is among the most challenging concepts to bridge into the discrete realm; for this reason, it is also a sterling test a given paradigm’s ability to translate geometric theory into the sampled data.Here, we introduce <em>Diffusion Curvature</em>, a new definition of scalar curvature on point clouds which inherits diffusion geometry’s robustness to noise and sampling. In our benchmarks, diffusion curvature proves itself the <em>only</em> method capable of robustly differentiating positive and negative curvature in high dimensions, while its absence of parameters requiring user tuning makes it significantly more user-friendly than other methods.We describe theoretical connections between diffusion curvature and the Ollivier-Ricci curvature, and introduce a general paradigm for comparing diffusions across graphs.We apply diffusion curvature to neural loss landscapes, single-cell RNA data [bit about what we find] motivating the practical utility of our method.
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>Manifold Learning, Geometric Deep Learning, Graph Curvature, Point Clouds, Diffusion Geometry, Topological Data Analysis, Neural Loss Landscapes, Single-Cell Data</p>
  </div>
</div>

</header>

<hr>
<!-- Latex formatting tweaks -->
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The <em>Manifold Assumption</em> is among machine learning’s most prolific muses. Given only the knowledge that the local Euclidean distances between raw datapoints are meaningful, the Assumption grants a fount of theoretical intuition and methodological inspiration. It lies behind low-dimensional embedding and visualization techniques like PHATE, manifold distance estimators like HeatGeo, and the entire field of Topological Data Analysis <span class="citation" data-cites="moon2019VisualizingStructureTransitions huguet2023HeatDiffusionPerspective">(<a href="#ref-moon2019VisualizingStructureTransitions" role="doc-biblioref">Moon et al. 2019</a>; <a href="#ref-huguet2023HeatDiffusionPerspective" role="doc-biblioref">Huguet et al. 2023</a>)</span>. It also invites one to use that theoretical inspiration to design new practical methods. Doing so, however, requires facing a sort of troll: one must bridge the idealized, analytic spaces of theory with the noisy, discrete world of real data. This is easier in some parts of theory than others. In particular, doing so is harder the more <em>locally</em> one tries to apply the Assumption. Global topological features are easier; manifold distances, harder; and manifold <em>curvatures</em>, perhaps the hardest of all.</p>
<p>But while distances or topological descriptors are widely used and have ready utility, one might be excused for asking “why care about curvatures?” A response, becoming more coherent with a raft of new graph and point-cloud curvature methods, is for its utility as a hyper-local descriptor of manifold geometry. There are many techniques, from Topological Data Analysis, to manifold distances estimators or graph or manifold embedding techniques that give global features. Curvature answers the opposing question: “<em>around this point, in this neighborhood, what’s happening to the local geometry</em>?” Such answers are directly interpretable in domains like single-cell biology, where curvature can indicate the extent of cell differentiation. They have also been shown to exceed and enhance the local discriminative powers of Graph Neural Networks (cite Bastian, COMPASS). (ADD other areas of application; reference Hickok &amp; Sritharan). loss landscapes, eg</p>
<p>But notions of graph and point-cloud curvature are also particularly poignant for their theoretical implications. On one level, they are attempting to square an impossible circle. While global, topological features have a theoretical tolerance for discrete data, Riemannian geometry is built upon measures of <em>infinitesimal</em> locality – and curvature is the most local of Riemannian measurements. This is precisely where discrete data is the least reliable. Globally, point clouds can be recognized as manifolds, but zoom in to the locale of individual points, and the inevitable presence of noise and varying densities imbues the discrete manifold with a Heisenberg-like uncertainty: the smaller you go, the less you know of the local structure.</p>
<p>Hence the need for that ‘bridge’ – a Rosetta Stone, if you will – between the continuous, analytically-specified spaces of theory and the messy realities of actual data. The several existing discrete curvature methods chiefly use one of two classes of bridge: optimal transport, as in the field’s bulwark, Ollivier-Ricci curvature, and tangent-plane approximation, as used by methods like Sritharan and Adal-PCA (REFINE). This choice may seem inconsequential – it’s only curvature, after all – but precisely because curvature is so local, the ability of such a bridge to estimate curvature qualifies it for a much broader role: as a means to bring <em>general</em> geometric ideas into the discrete realm.</p>
<p>How effective are these bridges? To explore this, we develop a comprehensive benchmarking suite, and present the first quantitative comparison of discrete curvature methods – probing their tolerance to noise, high dimensions, and parameter perturbations. The landscape is, unfortunately, rather dismal. Most existing methods reliably fail to distinguish between negative and positive curvatures in high dimensions, degrade under small quantities of noise, and are highly dependent on the user tuning parameters which the user – not having access to the ground-truth curvatures – has no ability to tune. Graph curvature methods like Ollivier-Ricci and Forman-Ricci are difficult to translate to higher-density point cloud data, and point cloud methods like Sritharan, Hickok, and Adal-PCA depend on higher-order geometric concepts (like volumes and distances) whose estimation depends on curvature. This is both discouraging to those practitioners and academics thinking of incorporate curvature estimation in their research, and suggestive of a larger cavity in geometric featurization. None of these existing bridges pass the curvature test.</p>
<p>Perhaps the most well-traveled bridge in other areas of geometric estimation is data diffusion, or as often styled, “Diffusion Geometry”. Beginning in 2006 with Coifman &amp; Lafon’s “Diffusion maps” <span class="citation" data-cites="coifman2006DiffusionMaps">(<a href="#ref-coifman2006DiffusionMaps" role="doc-biblioref">Coifman and Lafon 2006</a>)</span>, this bridge operates through repeated application of a <em>diffusion operator</em> which can be thought of either as simulating heat diffusion, or performing a weighted random walk across data points. When iterated, this diffusion process performs a sort of ‘integration’ of local structure into a set of semi-local features which enable one to escape the confounding factors noise and sparsity. This idea underlies distance estimators like Diffusion Distances and HeatGeo <span class="citation" data-cites="coifman2006DiffusionMaps">Huguet et al. (<a href="#ref-huguet2023HeatDiffusionPerspective" role="doc-biblioref">2023</a>)</span>, dimensionality-reduction methods like PHATE and GAGA <span class="citation" data-cites="moon2019VisualizingStructureTransitions">(<a href="#ref-moon2019VisualizingStructureTransitions" role="doc-biblioref">Moon et al. 2019</a>)</span>, and even a deep-learning based inference of the local Riemannian metric <span class="citation" data-cites="fasina2023NeuralFIMLearning">(<a href="#ref-fasina2023NeuralFIMLearning" role="doc-biblioref">Fasina et al. 2023</a>)</span>. More generally, the diffusion-based <em>graph scattering transform</em> has inspired new graph neural network architectures with state-of-the-art performance on baselines requiring fine-grained local discrimination. (CHECK. IS THIS TRUE?)</p>
<p>Can this bridge of diffusion geometry pass the ‘curvature test’? Moreover, can it produce a curvature method robust enough to noise, dimension, and parameterization to be practically useful on point cloud data? In <span class="citation" data-cites="bhaskar2022DiffusionCurvatureEstimating">Bhaskar et al. (<a href="#ref-bhaskar2022DiffusionCurvatureEstimating" role="doc-biblioref">2022</a>)</span>, we gave a preliminary positive answer with <em>diffusion laziness curvature</em>, an unsigned curvature derived from the diffusion operator. This strongly suggested that the powered diffusion operator contains curvature information, but was lacking both in theoretical pedigree and experimental validation on anything beyond 2-dimensional manifolds. We now present the maturation of this work.</p>
<p><em>Diffusion Curvature</em> is a signed curvature based on diffusion geometry. It is resilient to noise, requires no user-tuned parameters, and robustly distinguishes between signs in high dimensions and under noise. We prove theoretical connections between diffusion curvature and the field’s bulwark, <em>Ollivier-Ricci Curvature</em>, which position diffusion curvature as a point-wise adaptation of Ollivier-Ricci - and further connect diffusion geometry to the optimal transport literature. More broadly, the success of diffusion-based techniques in determining curvature strongly motivates diffusion geometry as the missing Rosetta Stone between geometric theory and data science practice.</p>
</section>
<section id="background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background</h1>
<section id="curvature-in-the-continuous-setting" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="curvature-in-the-continuous-setting"><span class="header-section-number">2.1</span> Curvature in the Continuous Setting</h2>
<p>There are many definitions of curvature on Riemannian manifolds. In this work, we focus on discrete analogs to the <em>Ricci</em> curvature <span class="math inline">\(\text{Ric}(x,y)\)</span>, which we’ll briefly motivate in the continuous setting.</p>
<p>Imagine two spheres of equal radius centered at <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in a Riemannian manifold <span class="math inline">\(\mathcal{M}\)</span>. Intuitively, the Ricci curvature between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> measures the difference between the distance of these midpoints, and the average distance between corresponding points in each sphere. In an area of positive Ricci curvature, points in spheres are, on average, closer than their midpoints; in negative Ricci curvature, the points in the sphere are further, on average, than the midpoints. This is formally described by first defining the <em>sectional curvature</em> as the contraction of length incurred by parallel transport through a 2-plane of the manifold, then defining the Ricci curvature as an average of the sectional curvatures between two points. For the present work, we needn’t reproduce the full definitions, but we will recall some properties of manifolds with Ricci curvature bounded from below by some <span class="math inline">\(k\)</span>.</p>
<p>First, recall the relation between curvature and volume. In spaces of high positive curvature, the volume of a ball is than a ball of the same radius in a flat space, and even smaller than a ball of the same radius in a hyperbolic space. This is formally expressed by the Bishop Gromov inequality <span class="citation" data-cites="2021BishopGromovInequality">(<a href="#ref-2021BishopGromovInequality" role="doc-biblioref"><span>“Bishop–<span>Gromov</span> Inequality”</span> 2021</a>)</span>.</p>
<div id="thm-bishop-gromov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Bishop-Gromov) </strong></span>Let <span class="math inline">\(\mathcal{M}\)</span> be a complete <span class="math inline">\(d\)</span>-dimensional Riemannian manifold with <span class="math inline">\(Ric(x,y) &gt; (d - 1)k\)</span> for all <span class="math inline">\(x,y \in \mathcal{M}\)</span> and <span class="math inline">\(k \in \mathbb{R}\)</span>. Let <span class="math inline">\(M_{K}^d\)</span> be the complete <span class="math inline">\(d\)</span>-dimensional simply connected space of constant sectional curvature <span class="math inline">\(k\)</span>. Denote by <span class="math inline">\(B(x,k)\)</span> the ball of radius <span class="math inline">\(k\)</span> centered at <span class="math inline">\(x\)</span>. Then for any <span class="math inline">\(x \in \mathcal{M}\)</span> and <span class="math inline">\(x_{k} \in M_{K}^d\)</span>, the function</p>
<p><span class="math display">\[
\phi(r) = \frac{\operatorname{Vol}B(x,r)}{\operatorname{Vol}B(x_{k},r)}
\]</span> is non-increasing on <span class="math inline">\((0,\infty)\)</span>.</p>
</div>
<p>This phenomenon of diminishing volume in positive curvature is related to the convergence of geodesic rays. In the plane (or saddle), two geodesic rays extending from the same point in different directions will never intersect. But on the sphere, they <em>will</em> meet again – at the opposite pole. This convergence constrains the maximum diameter a space of positive curvature may have, as expressed in the Bonnet-Myers Theorem <span class="citation" data-cites="ollivier2009RicciCurvatureMarkov">(<a href="#ref-ollivier2009RicciCurvatureMarkov" role="doc-biblioref">Ollivier 2009</a>)</span>:</p>
<div id="thm-bonnet-myers" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (Bonnet-Myers) </strong></span>Let <span class="math inline">\(X\)</span> be an <span class="math inline">\(d\)</span>-dimensional Riemannian manifold. Let <span class="math inline">\(\inf \operatorname{Ric}(X)\)</span> be the infimum of the Ricci curvature <span class="math inline">\(\operatorname{Ric}(v, v)\)</span> over all unit tangent vectors <span class="math inline">\(v\)</span>. Let <span class="math inline">\(S^d \subset \mathbb{R}^{d+1}\)</span> be the unit sphere of the same dimension as <span class="math inline">\(X\)</span>. Then, if <span class="math inline">\(\inf \operatorname{Ric}(X) \geqslant \inf \operatorname{Ric}\left(S^d\right)\)</span> then <span class="math inline">\(\operatorname{diam} X \leqslant \operatorname{diam} S^d\)</span>.</p>
</div>
<p>We’ll encounter versions of Bishop-Gromov and Bonnet-Myers in our discrete setting. But first, let’s describe the construction and properties of our specific discrete setting.</p>
</section>
<section id="the-discrete-setting" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="the-discrete-setting"><span class="header-section-number">2.2</span> The Discrete Setting</h2>
<p>Within the ambient setting of points <span class="math inline">\(x_{i} \in \mathbb{R}^D\)</span>, the Euclidean distances between the points in our point cloud are not very useful. To perform geometric analysis, we want the manifold’s <em>geodesic</em> distances between <span class="math inline">\(x_{i}, x_{j} \in \mathcal{M}\)</span>,. However, manifolds are locally euclidean, so within a sufficiently small neighborhood of <span class="math inline">\(x_{i} \in \mathcal{M}\)</span> , the euclidean distances are accurate. This is the basis of graph construction: retain only the trustworthy local distances, discard the rest, and then “integrate” over the local neighborhoods to recover features of the global geometry.</p>
<p>A graph <span class="math inline">\(G = (V, E)\)</span> is a collection of <span class="math inline">\(n\)</span> vertices <span class="math inline">\(v_{i} \in V\)</span> connected by (possibly weighted) edges <span class="math inline">\(e_{ij} \in E\)</span> . It is efficiently represented by a single <em>adjacency</em> (or <em>affinity</em>) matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, where <span class="math inline">\(A_{ij}\)</span> expresses the degree of connection between the vertices <span class="math inline">\(v_{i}\)</span> and <span class="math inline">\(v_{j}\)</span>. In a binary adjacency matrix, <span class="math inline">\(A_{ij}=1\)</span> iff there is an edge between <span class="math inline">\(v_{i}\)</span> and <span class="math inline">\(v_{j}\)</span>. In a weighted affinity matrix, <span class="math inline">\(0&lt;A_{ij}&lt;1\)</span> with a higher affinity indicating a closer connection between the nodes.</p>
<p>One can construct an affinity matrix from a point cloud with the following algorithm: 1. Compute the matrix <span class="math inline">\(D\)</span> of pairwise euclidean distances between points, so that <span class="math inline">\(D_{ij}=\|x_{i}-x_{j}\|_{2}\)</span>. 2. Apply a kernel <span class="math inline">\(\kappa\)</span> to the distances to construct the affinity matrix, where <span class="math inline">\(A_{ij} = \kappa(D_{ij})\)</span>. This is typically the gaussian kernel: <span class="math display">\[
\kappa(y) = \frac{1}{\sqrt{ 2\pi }\sigma}\exp\left( -\frac{y}{\sigma^2} \right)
\]</span> There are a variety of heuristics for selecting an appropriate kernel bandwidth <span class="math inline">\(\sigma\)</span>. In this paper, we use an adaptive kernel bandwidth, in which, when computing <span class="math inline">\(k(D_{ij})\)</span>, <span class="math inline">\(\sigma\)</span> is set to the mean distance from the points <span class="math inline">\(x_{i}\)</span> and <span class="math inline">\(x_{j}\)</span> to their <span class="math inline">\(k\)</span>-th nearest neighbor.</p>
<p>After building our graph affinity matrix <span class="math inline">\(A\)</span>, we created a new representation of the point cloud <span class="math inline">\(X\)</span> – turning it from an <span class="math inline">\(n \times D\)</span> matrix of unwieldy ambient coordinates into an <span class="math inline">\(n \times n\)</span> matrix of pairwise connections between points. The challenge is now to reassemble this information of local connectivity to recover the features of <span class="math inline">\(\mathcal{M}\)</span>. Graph diffusion does precisely this.</p>
</section>
<section id="graph-diffusion" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="graph-diffusion"><span class="header-section-number">2.3</span> Graph Diffusion</h2>
<p>By row-normalizing <span class="math inline">\(A\)</span>, one obtains the graph diffusion matrix <span class="math inline">\(P = D^{-1}A\)</span>, which is a commonly-used method of “integrating” the local connectivity of the graph <span class="math inline">\(A\)</span> into global geometric descriptors of <span class="math inline">\(\mathcal{M}\)</span>. Coifman and Lafon <span class="citation" data-cites="coifman2006DiffusionMaps">(<a href="#ref-coifman2006DiffusionMaps" role="doc-biblioref">Coifman and Lafon 2006</a>)</span> proved a correspondence between iterated graph diffusion <span class="math inline">\(P^t\)</span> and the Neumann heat kernel on <span class="math inline">\(\mathcal{M}\)</span>. Their technique, <em>Diffusion Maps</em>, uses the Euclidean distances between eigencoordinates of <span class="math inline">\(P\)</span> to approximate the geodesic distances on <span class="math inline">\(\mathcal{M}\)</span>. The visualization technique <span class="math inline">\(PHATE\)</span> <span class="citation" data-cites="moon2019VisualizingStructureTransitions">(<a href="#ref-moon2019VisualizingStructureTransitions" role="doc-biblioref">Moon et al. 2019</a>)</span> constructs a low-dimensional embedding of a point cloud <span class="math inline">\(X\)</span> such that a distance between the transition probabilities <span class="math inline">\(P\)</span> of <span class="math inline">\(X\)</span> is preserved in the embedding. (More on properties of phate, trajectory preservation.) <em>Diffusion Earth Mover’s Distance</em> <span class="citation" data-cites="tong2021DiffusionEarthMovera">(<a href="#ref-tong2021DiffusionEarthMovera" role="doc-biblioref">A. Y. Tong et al. 2021</a>)</span> efficiently approximates the transportation distance between distributions on a graph using multi-scale wavelet transform obtained by applying different scales of diffusion. <em>LEGSNet</em>‘s “learnable geometric scattering” computes tunable scales of diffusion with a graph neural network and achieves state of the art performance on biochemistry graph classification <span class="citation" data-cites="tong2021DatadrivenLearningGeometric">(<a href="#ref-tong2021DatadrivenLearningGeometric" role="doc-biblioref">A. Tong et al. 2021</a>)</span>. These are but a few of the many manifold learning techniques based in diffusion.</p>
<p>Constructing the diffusion matrix from the affinity matrix <span class="math inline">\(A\)</span> is straightforward: you simply row-normalize <span class="math inline">\(A\)</span>, with an optional step to normalizing by density.</p>
<p>Here is the algorithm presented in Coifman and Lafon <span class="citation" data-cites="coifman2006DiffusionMaps">(<a href="#ref-coifman2006DiffusionMaps" role="doc-biblioref">Coifman and Lafon 2006</a>)</span>:</p>
<ol type="1">
<li>(Optional) Compute an <em>anisotropic density normalization</em> on <span class="math inline">\(A\)</span>, obtaining the anisotropic adjacency matrix <span class="math inline">\(A_{\star}\)</span>.</li>
<li>Construct the degree matrix <span class="math inline">\(D\)</span>, whose diagonal entries are the rowsums of <span class="math inline">\(A\)</span>, i.e.&nbsp;<span class="math inline">\(D_{ii} = \sum_{j}A_{ij}\)</span>.. The other entries are zeros.</li>
<li>Define <span class="math inline">\(P = D^{-1} A\)</span>, the graph diffusion matrix.</li>
</ol>
<ul class="task-list">
<li><label><input type="checkbox">Clean this up: get anisotropic equation, and clarify the role of the self affinity. When is it removed? When is laziness added?</label></li>
</ul>
<p><span class="math inline">\(P\)</span> has several nice properties. The rows <span class="math inline">\(P[i]\)</span> give the transition probabilities of a single step random walk starting at point <span class="math inline">\(x_{i}\)</span>; each row <span class="math inline">\(P[i]\)</span> can be viewed as a probability distribution centered at <span class="math inline">\(x_{i}\)</span>. This is preserved under powers of the matrix. The rows of <span class="math inline">\(P^t\)</span> still sum to 1, and <span class="math inline">\(P^t[i]\)</span> now gives the probability distribution of a <span class="math inline">\(t\)</span>-step random walk starting at <span class="math inline">\(x_{i}\)</span>.</p>
<p>Although <span class="math inline">\(P\)</span> is not symmetric, it is conjugate to a symmetric matrix, via <span class="math inline">\(D^{0.5}PD^{-0.5} = D^{-0.5}AD^{-0.5}\)</span>, granting it a full basis of real-valued eigenvectors and eigenvalues. These eigenvectors are shared with the normalized graph Laplacian <span class="math inline">\(L = I - D^{-0.5}AD^{-0.5}\)</span>. The eigenvalues of <span class="math inline">\(P\)</span> have magnitude less than or equal to 1. Powering the matrix <span class="math inline">\(P^t\)</span> thus corresponds to powering the eigenvalues <span class="math inline">\(\lambda_{i}^t\)</span> of <span class="math inline">\(P\)</span>, via diagonalization <span class="math display">\[
P^t = \Psi \Lambda^t \Psi^T
\]</span> This is similar to applying a low-pass filter to the graph. As <span class="math inline">\(t\)</span> increases, the smallest eigenvalues decay fastest under repeated powering, and their corresponding eigenvector vanishes from the eigenbasis – leaving only the largest <span class="math inline">\(\lambda_{i}\)</span>, whose eigenvectors trace global geometric features.</p>
<p>This is a remarkable feature of the diffusion matrix: the ability to “denoise” itself by iterating the random walk over larger time scales. Intuitively, the paths through the data most robustly trafficked by random walkers are those supported by multiple high-probability connections from independent starting points.</p>
</section>
<section id="ollivier-ricci-curvature" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="ollivier-ricci-curvature"><span class="header-section-number">2.4</span> Ollivier-Ricci Curvature</h2>
<p>Developed by Yann Ollivier in 2007, <em>Coarse Ricci Curvature</em> (or sometimes, “Ollivier Ricci Curvature”) is a direct translation of Ricci curvature to discrete metric spaces like graphs <span class="citation" data-cites="ollivier2009RicciCurvatureMarkov">(<a href="#ref-ollivier2009RicciCurvatureMarkov" role="doc-biblioref">Ollivier 2009</a>)</span>. Several classical properties of Ricci curvature can be extended to the graph setting using Coarse Ricci Curvature. Ollivier has, for instance, proven versions of concentration inequalities, Bonnet Myers (more). Coarse Ricci Curvature has, in this way, become something of a bridge between continuous and coarse geometry. The basis of this bridge is optimal transport, and specifically, the 1-Wasserstein distance.</p>
<p>In the Riemannian setting, Ricci curvature captures the phenomenon that, in positive curvature, “small spheres are closer (in transportation distance) than their centers are” <span class="citation" data-cites="ollivier2009RicciCurvatureMarkov">(<a href="#ref-ollivier2009RicciCurvatureMarkov" role="doc-biblioref">Ollivier 2009</a>)</span>. On the sphere, for instance, imagine two circles centered on the north and south poles: every point in each circle is closer to the corresponding point in the opposite circle than are the circles’ centers. This effect diminishes as one moves the circles closer together, but never reaches equality. In negatively curved spaces, the discrepancy reverses, while in a flat space, the average distance between the points of the circles is the distance between the centers.</p>
<p>Coarse Ricci Curvature captures a similar phenomenon on graphs. Instead of spheres, it uses locally-centered probability distributions defined by random walks. And to measure the distance between these walks, it uses the 1-Wasserstein (or Earth Mover’s) distance. We’ll briefly define each.</p>
<p>The 1-Wasserstein distance is a measure of the distance between probability distributions. Given distributions <span class="math inline">\(\mu_{x}\)</span> and <span class="math inline">\(\mu_{y}\)</span> over some shared space <span class="math inline">\(X\)</span>, the Wasserstein distance quantifies the smallest amount of “work” needed to transform one distribution into another, by transporting probability “mass” between pairs of points over the ground metric <span class="math inline">\(d(x,y)\)</span>:</p>
<div id="def-1-wasserstein" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (1-Wasserstein Distance) </strong></span>The 1-Wasserstein distance between distributions <span class="math inline">\(\mu_{x}\)</span> and <span class="math inline">\(\mu_{y}\)</span> is <span class="math display">\[ W_{1}(\mu_{x},\mu_{y}) := \inf_{\xi \in \Pi(\mu_{x},\mu_{u})} \int \int d(x,y) \, d\xi(x,y) \]</span> where the “transportation plan” <span class="math inline">\(\xi\)</span> is drawn from the space <span class="math inline">\(\Pi(\mu_{x},\mu_{y})\)</span> of joint probability distributions over <span class="math inline">\(X \times X\)</span> which project onto <span class="math inline">\(\mu_{x}\)</span> and <span class="math inline">\(\mu_{y}\)</span>. In the discrete setting, this translates naturally into an infimum over a summation. <span class="math display">\[W_{1}(\mu_{x},\mu_{y}) := \inf_{\xi \in \Pi(\mu_{x},\mu_{y})} \sum_{x \in X} \sum_{y \in X} d(x,y) \xi(x,y)\]</span></p>
</div>
<p>What is the analog on a graph of a “small sphere” around a point? Ollivier replaces spheres with a family of measures <span class="math inline">\(m_{x}(\cdot)\)</span> defined for each point <span class="math inline">\(x\)</span>, where</p>
<ol type="1">
<li>Each <span class="math inline">\(\mu_{x}(\cdot)\)</span> depends measurably on <span class="math inline">\(x\)</span>, i.e.&nbsp;the map <span class="math inline">\(x \to \mu_{x}\)</span> is measurable.</li>
<li>Each <span class="math inline">\(\mu_{x}(\cdot)\)</span> has finite first moment, or <em>Jump</em>, i.e.&nbsp;for some <span class="math inline">\(o \in X\)</span> <span class="math inline">\(\int d(o,y) \mu_{x}(y) \, dx &lt; \infty\)</span>.</li>
</ol>
<p>In graphs, Ollivier defines these <span class="math inline">\(\mu_x\)</span> as the probability distributions created by a single-step random walk from the point <span class="math inline">\(x\)</span>.</p>
<p>With a transition probability <span class="math inline">\(\alpha\)</span>, and equal probability of moving to each of <span class="math inline">\(x\)</span>’s neighbors on the graph, <span class="math inline">\(\mu_{x}(x) = (1-\alpha)\)</span> and <span class="math inline">\(m_{x}(y) = \alpha\)</span> if <span class="math inline">\(y \in N(x)\)</span> or <span class="math inline">\(0\)</span> otherwise.</p>
<p>This is analogous to defining <span class="math inline">\(m_{x} = P e_{x}\)</span>, if <span class="math inline">\(P\)</span> is the diffusion matrix created from a binary adjacency matrix. Note, however, that there is nothing limiting us to binary adjacency matrices, or even single steps of diffusion; the two conditions above are equally satisfied by weighted adjacency matrices and <span class="math inline">\(t\)</span>-step diffusions, and in sparse or noisy graphs, this may be desirable.</p>
<div id="def-coarse-ricci-curvature" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Coarse Ricci Curvature) </strong></span>The <em>Coarse Ricci Curvature</em> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is <span class="math display">\[\kappa(x, y):=1-\frac{W_1\left(m_x, m_y\right)}{d(x, y)}\]</span></p>
</div>
<p>There are a number of provisos attached to this definition, which tries to approximate a continuous phenomenon within discrete constraints. These constraints, and the relationship between Ricci and Ollivier’s coarse Ricci curvature are illustrated Ollivier’s Example 2.6 <span class="citation" data-cites="ollivier2009RicciCurvatureMarkov">(<a href="#ref-ollivier2009RicciCurvatureMarkov" role="doc-biblioref">Ollivier 2009</a>)</span>:</p>
<div id="exm-ollivier-example-2.6" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 </strong></span>Let <span class="math inline">\((X,d)\)</span> be a smooth Riemannian manifold of dimension <span class="math inline">\(d\)</span> and let <span class="math inline">\(\text{vol}\)</span> be the Riemannian volume measure. Let <span class="math inline">\(\epsilon&gt;0\)</span> small enough and consider the ball of radius <span class="math inline">\(\epsilon\)</span> around each point <span class="math inline">\(x\)</span>. Let <span class="math inline">\(x,y \in X\)</span> be two sufficiently close points. Let <span class="math inline">\(v\)</span> be the unit tangent vector at <span class="math inline">\(x\)</span> directed towards <span class="math inline">\(y\)</span>. The coarse Ricci curvature along <span class="math inline">\(v\)</span> is then <span class="math display">\[\kappa(x,y) = \frac{\epsilon^2 \text{Ric}(v,v)}{2(d+2)}+o(\epsilon^3 + \epsilon^2d(x,y))\]</span></p>
</div>
<p>Hence the coarse Ricci curvature applied to a manifold recovers the Ricci curvature, up to a scaling factor contingent on dimension, and plus an error term that grows with the radius of ball and distance between points.</p>
<p>Ollivier’s choice not to scale <span class="math inline">\(\kappa(x,y)\)</span> by dimension is interesting, and likely motivated by his application of coarse Ricci curvature to graph-like spaces for which dimension isn’t clearly defined, like social networks. Within our domain of point-cloud data, incorporating dimension may be desirable; without it, spaces of high dimension can be conflated with spaces of lower dimension but higher negative curvature.</p>
<p>A result on coarse Ricci curvature which will prove useful concerns the <em>contraction (or expansion) of measure</em> that occurs under diffusion in spaces of positive (or negative) curvature. This is <span class="citation" data-cites="ollivier2009RicciCurvatureMarkov">Ollivier (<a href="#ref-ollivier2009RicciCurvatureMarkov" role="doc-biblioref">2009</a>)</span>’s Proposition 20:</p>
<div id="prp-ollivier-contraction-of-measure" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (<span class="math inline">\(W_1\)</span> Contraction of Measure) </strong></span>Let <span class="math inline">\((X,d,m)\)</span> be a metric space with a random walk. Let <span class="math inline">\(\kappa \in \mathbb{R}\)</span>. Then we have <span class="math inline">\(\kappa(x,y) \geq \kappa\)</span> for all <span class="math inline">\(x,y \in X\)</span> iff for any two probability distributions <span class="math inline">\(\mu, \mu' \in \mathcal{P}(X)\)</span> one has</p>
<p><span class="math display">\[
W_{1}(\mu \star m, \mu' \star m) \leq (1-k)W_{1}(\mu, \mu')
\]</span> Where <span class="math display">\[
\mu \star m := \int_{{x \in X}} d\mu(x)m_{x} \, dx
\]</span></p>
</div>
</section>
</section>
<section id="methods" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Methods</h1>
<p>We consider a measure space <span class="math inline">\((X,p)\)</span> equipped with a random walk, e.g.&nbsp;a point cloud and a diffusion operator derived as above. In this section, we define a new notion of curvature deriving from this random walk.</p>
<p>The core intuition of Diffusion Curvature is that the <em>laziness</em> of random walks on a graph is a proxy for the curvature of the underlying manifold. Picture a “random walker” drunkenly traversing a sphere. If he manages, over the course of several steps, to wander to the opposite pole, he has many ways of getting back to where he started. By contrast, if he begins on the top of a (negatively-curved) saddle and wanders down one side, any path aside from exactly retracing his steps incurs a steep penalty in extra distance. On the sphere, the random walker is more likely to find his way home: his walks are “lazier”.</p>
<p>Our previous paper <span class="citation" data-cites="bhaskar2022DiffusionCurvatureEstimating">(<a href="#ref-bhaskar2022DiffusionCurvatureEstimating" role="doc-biblioref">Bhaskar et al. 2022</a>)</span> measured this laziness directly, as the return probability within a k-neighborhood of the starting point. This required tuning the parameter <span class="math inline">\(k\)</span>, and neglected the information provided by probabilities outside of this neighborhood.</p>
<p>We refine our previous definition of diffusion laziness by replacing the neighborhood-sum with a distributional distance measure <span class="math inline">\(D\)</span> between a dirac <span class="math inline">\(\delta_{x}\)</span> and its <span class="math inline">\(t\)</span>-step diffusion <span class="math inline">\(p_{x}^X(t)\)</span>. This provides a more sensitive and parameter-free measure of how ‘spread out’ the diffusion is.</p>
<div id="def-energy-of-diffusion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Diffusion Energy) </strong></span>Given some distributional distance <span class="math inline">\(D\)</span>, the <span class="math inline">\(D\)</span>-energy snapshot of a diffusion <span class="math inline">\(p_{x}^X(t)\)</span> at time <span class="math inline">\(t\)</span> is <span class="math display">\[
l_{X}(x,t) := D\left(\delta_x, p_x^X(t)\right)
\]</span></p>
</div>
<p>By <span class="math inline">\(p_{x}^X(t)\)</span>, we indicate the <span class="math inline">\(t\)</span>-step diffusion of the dirac <span class="math inline">\(\delta_{x}\)</span> centered at <span class="math inline">\(x \in X\)</span>, using the diffusion operator for the measure space <span class="math inline">\(X\)</span>. We use the term ‘Diffusion Energy’ in reference to our previous work (CITE), which measured curvature via ‘Diffusion Laziness’. That laziness measurement increased with positive curvature; diffusion energy does the opposite, increasing with more negative curvature.</p>
<p>The distributional distance <span class="math inline">\(D\)</span> may be the Wasserstein-1 distance, as with Ollivier-Ricci curvature; it may be the Jensen-Shannon Distance, evoking the classical use of entropy to measure the diffusion of heat; or it may be some other domain-specific form chosen for a particular metric measure space.</p>
<p>This measure of diffusion energy provides a snapshot of curvature at single diffusion time. However, this curvature is unsigned, and sensitive to the choice of <span class="math inline">\(t\)</span>. Moreover, as we’ll discuss, differences in graph construction cause the times associated with the diffusion operators from different spaces to change at different speeds. This makes it hard to compare the geometric information gleaned from the diffusion operator across manifolds in terms of time.</p>
<p>To remedy this, we perform a change of variable, from the diffusion time <span class="math inline">\(t\)</span> to a <em>Diffusion Trajectory Distance</em>. As <span class="math inline">\(t\)</span> ranges from 0 to <span class="math inline">\(\infty\)</span>, the diffusions <span class="math inline">\(p_{x}^X(t)\)</span> can be viewed as a trajectory on the <span class="math inline">\(n\)</span>-dimensional probability simplex, where <span class="math inline">\(n\)</span> is the number of points in <span class="math inline">\(X\)</span>. Given some Riemannian metric <span class="math inline">\({} g_{p} {}\)</span> on the probability simplex (e.g.&nbsp;the Fisher Information Metric), we define the Diffusion Trajectory Distance of scale <span class="math inline">\(t\)</span> as the arc length of the trajectory to <span class="math inline">\(p_{x}^X(t)\)</span>.</p>
<div id="def-diffusion-trajctory-distance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Diffusion Trajectory Distance) </strong></span><span class="math display">\[
d_{DTD}(x, t)= \int_{\tau=0}^{t} g_p\left(\frac{d p_x^X(\tau)}{d \tau}, \frac{ d p_x^X(\tau) }{d \tau} \right)
\]</span></p>
</div>
<p>This function produces the diffusion trajectory distance corresponding to each <span class="math inline">\(t\)</span>. We also consider the inverse, which translates from diffusion trajectory distances to <span class="math inline">\(t\)</span>.</p>
<p><span class="math display">\[
t_{DTD}(x,d) = d_{DTD}^{-1}(x,t)
\]</span></p>
<p>While the speed of diffusion (and hence, scale of <span class="math inline">\(t\)</span>) varies between datasets – e.g.&nbsp;the same value of <span class="math inline">\(t\)</span> may correspond to a small diffusion in one setting, and one nearing steady-state in another – the diffusion trajectory distance is directly comparable between datasets. The function <span class="math inline">\(t_{DTD}\)</span> thus allows to to compare diffusion energies between across measure spaces, giving rise to a signed curvature.</p>
<div id="def-diffusion-curvature" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Diffusion Curvature) </strong></span>Given a finite collection of points <span class="math inline">\(X \subseteq \mathcal{M}\)</span>, where <span class="math inline">\(\mathcal{M}\)</span> is a Riemannian manifold of intrinsic dimension <span class="math inline">\(n\)</span>, the <em>Diffusion Curvature</em> of <span class="math inline">\(x \in X\)</span> is</p>
<p><span class="math display">\[
k_(x, d) = l_{E} (x,t_{DTD}(x,d)) - l_{X}(x, t_{DTD}(x,d))
\]</span> where <span class="math inline">\({} E \subseteq \mathbb{R}^n {}\)</span> is a collection of uniformly sampled points from <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</div>
<p>In practice, we don’t have an infinitesimal parameterization of <span class="math inline">\(p_{x}^X\)</span>, so instead of computing the diffusion trajectory distances as a path integral, we approximate them as a piecewise sum of distributional distances: we compute <span class="math inline">\(T\)</span> scales of diffusion (e.g.&nbsp;50), sum the distributional distances between each scale to estimate the path integral, and from this derive <span class="math inline">\(t_{DTD}\)</span> via a piecewise linear approximation of <span class="math inline">\(d_{DTD}\)</span>. This algorithm is summarized by Box WAWA.</p>
<p>The usefulness of this definition rests upon two claims:</p>
<ol type="1">
<li>That diffusion energy is indeed a measure of (unsigned) curvature</li>
<li>That our integration along diffusion trajectories enables this energy to be compared across point clouds with different diffusion processes.</li>
</ol>
<p>The next two sections will analyze this first claim. We first analyze diffusion energy within Ollivier’s framework of metric measure theory, and bound the diffusion energy from above by the Ollivier-Ricci curvature. We then provide a motivating example recovering a Bishop-Gromov type volume comparison from the diffusion curvature with an entropic distance.</p>
<p>Finally, we support the second claim with illustrations of point clouds which diffusion trajectories can uniquely compare. We also describe how common techniques of constructing graphs from point clouds can obscure geometric information, and propose a ‘curvature-agnostic kernel’ to facilitate more geometrically-faithful graph construction.</p>
<section id="diffusion-energy-is-bounded-by-ollivier-ricci-curvature" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="diffusion-energy-is-bounded-by-ollivier-ricci-curvature"><span class="header-section-number">3.1</span> Diffusion Energy is bounded by Ollivier-Ricci curvature</h2>
<p>Ollivier Ricci curvature measures the extent to which ‘spheres are closer than their centers’, using graph diffusions to create spheres, and optimal transport to measure distances. Diffusion energy uses both ingredients. But whereas Ollivier Ricci curvature takes the Wasserstein distance between spheres of the same size, Diffusion energy takes the distance between successive sizes of the same sphere.</p>
<p>This simplification makes Diffusion energy more localized and much computationally lighter. Here we demonstrate that it also retains the theoretical guarantees of Ollivier Ricci curvature, and might be though of as a natural node-wise adaptation of Ollivier Ricci curvature.</p>
<div id="prp-coarse-ricci-curvature-bounds-diffusion-energy" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2 (Coarse Ricci Curvature Bounds Diffusion Energy) </strong></span>Let <span class="math inline">\((X,d,m)\)</span> be a metric space equipped with a random walk, with coarse Ricci curvature bounded from below by some <span class="math inline">\(k\)</span> such that <span class="math inline">\(\kappa(x,y) \geq k\)</span> for all <span class="math inline">\(x,y \in X\)</span>. The diffusion energy of a <span class="math inline">\(t\)</span> step diffusion in <span class="math inline">\(X\)</span> is bounded above by <span class="math display">\[
W_{1}(\delta_{x}, m_{x}^t) \leq W_1\left(\delta_x,m_x\right)\frac{(1-(1-k)^t)}{k}
\]</span></p>
<p>In particular, if <span class="math inline">\(k&gt;0\)</span> then <span class="math inline">\(W_{1}(\delta_{x},m_{x}^t) \leq \frac{W_1\left(\delta_x,m_x\right)}{k}\)</span>, and if <span class="math inline">\(k=0\)</span>, then <span class="math inline">\(W_{1}(\delta_{x},m_{x}^t) \leq tW_1\left(\delta_x,m_x\right)\)</span>.</p>
</div>
<p>To prove this, first we bound <span class="math inline">\(W_{1}(m_{x}^t,m_{x}^{t+1})\)</span> using <a href="#prp-ollivier-contraction-of-measure" class="quarto-xref">Proposition&nbsp;1</a>. The proposition states that a lower bound on curvature, such as we have, implies that <span class="math display">\[
W_{1}(\mu \star m, \mu' \star m) \leq (1-k)W_{1}(\mu, \mu')
\]</span> where here <span class="math inline">\(\mu,\mu'\)</span> are two probability distributions and <span class="math inline">\(m\)</span> is a random walk. This provides an easy lemma:</p>
<div id="lem-inductive-contraction-of-measure" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 (Inductive Contraction of Measure) </strong></span>Let <span class="math inline">\((X,d,m)\)</span> be a metric space with a random walk. Suppose there is some <span class="math inline">\(k \in \mathbb{R}\)</span> such that the coarse Ricci curvature <span class="math inline">\(\kappa(x,y) \geq k\)</span> for all <span class="math inline">\(x,y \in X\)</span>. Then: <span class="math display">\[
W_1\left(m_x^t, m_x^{t+1}\right) \leq(1-k)^t W_1\left(\delta_x,m_x\right)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We proceed by induction. For <span class="math inline">\(t=0\)</span>, the above is true, as <span class="math inline">\(W_{1}(m_{x}^0, m_{x}^{1}) =W_{1}(\delta_x, m_{x}^{1}) = W_1\left(\delta_x,m_x\right)\)</span>. Suppose it holds for <span class="math inline">\(t-1\)</span>, e.g. <span class="math display">\[
W_1\left(m_x^{t-1}, m_x^{t}\right) \leq(1-k)^{t-1} W_1\left(\delta_x,m_x\right)
\]</span></p>
<p>Consider <span class="math inline">\(W_1\left(m_x^{t-1}\star m, m_x^{t}\star m\right)\)</span>, the application of another step of diffusion. By Ollivier’s Proposition 20, this distance is bounded above by <span class="math display">\[
W_1\left(\mu_1 \star m, \mu_2 \star m\right) \leq(1-k) W_1\left(\mu_1, \mu_2\right)
\]</span> So <span class="math display">\[
W_1\left(m_x^{t-1}\star m, m_x^{t}\star m\right) \leq  (1-k)W_1\left(m_x^{t-1}, m_x^{t}\right)
\]</span></p>
<p>which, since the statement holds for <span class="math inline">\(t-1\)</span>, yields <span class="math display">\[
W_1\left(m_x^t, m_x^{t+1}\right) \leq(1-k)^t W_1\left(\delta_x,m_x\right)
\]</span></p>
</div>
<p>We can now use this lemma to decompose our <span class="math inline">\(t\)</span>-step diffusion into a sum of single-step diffusions, allowing an easy proof of the proposition.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the triangle inequality, <span class="math display">\[
W_{1}(\delta_{x},m_{x}^t) \leq W_1\left(\delta_x,m_x\right) + W_{1}(m_{x},m_{x}^2) + \dots + W_{1}(m_{x}^{t-1}, m_{x}^t)
\]</span></p>
<p>By <a href="#lem-inductive-contraction-of-measure" class="quarto-xref">Lemma&nbsp;1</a>, <span class="math display">\[
\leq W_1\left(\delta_x,m_x\right)\left(1+(1-k)+(1-k)^2+\ldots+(1-k)^{t-1}\right)
\]</span></p>
<p>This truncated series is equal to <span class="math inline">\(\frac{1-(1-k)^t}{1-(1-k)} = \frac{(1-(1-k)^t)}{k}\)</span>. If <span class="math inline">\(k&gt;0\)</span>, then as <span class="math inline">\(t \to \infty\)</span>, the infinite sum <span class="math inline">\(\sum_{i=0}^t (1-k)^i\)</span> converges to the geometric series <span class="math inline">\(\frac{1}{1-(1-k)} = \frac{1}{k}\)</span>. Because the sum is monotonically increasing with <span class="math inline">\(t\)</span>, the partial sum is upper bounded by the infinite sum. It follows that <span class="math display">\[
W_{1}(\delta_{x},m_{x}^t) \leq \frac{W_1\left(\delta_x,m_x\right)}{k}
\]</span></p>
<p>If <span class="math inline">\(k=0\)</span>, then obviously <span class="math inline">\(\sum_{i}^t (1-k)^i = t\)</span>.</p>
</div>
<p>The above shows that the diffusion energy <span class="math inline">\(l_{W_1}\)</span> has an upper-bound given by the Ollivier Ricci curvature. We expect that the higher a manifold’s curvature, the lower <span class="math inline">\(l_{W_{1}}\)</span> should be. This is formalized by Proposition 1: the higher the Ollivier-Ricci curvature of a manifold, the smaller its diffusion energy. This result motivates the use of diffusion curvature as a point-wise adaptation of Ollivier-Ricci curvature.</p>
</section>
<section id="a-motivating-example-entropic-diffusion-curvature-recovers-a-bishop-gromov-volume-comparison" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="a-motivating-example-entropic-diffusion-curvature-recovers-a-bishop-gromov-volume-comparison"><span class="header-section-number">3.2</span> A Motivating Example: Entropic Diffusion Curvature Recovers a Bishop-Gromov Volume Comparison</h2>
<p>Among the most obvious manifestations of curvature is differences in volume: the higher the curvature, the smaller the volume of geodesic balls of the same radius. In the continuous setting, this is expressed by <a href="#thm-bishop-gromov" class="quarto-xref">Theorem&nbsp;1</a>.</p>
<p>One route to discrete curvature is thus estimating volumes; this is the route taken by <span class="citation" data-cites="hickok2023IntrinsicApproachScalarCurvature">Hickok and Blumberg (<a href="#ref-hickok2023IntrinsicApproachScalarCurvature" role="doc-biblioref">2023</a>)</span>. However, standard methods of estimating volumes in discrete, sampled spaces like point clouds or graphs rely on more complex geometric quantities, like density estimation or approximated geodesic distances, which are not only susceptible to noise, but often themselves dependent on the curvature!<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> One wonders: can we use the tools of diffusion geometry to robustly estimate volume?</p>
<p>Indeed, this is one way to interpret diffusion energy: as an inverse volume measurement. <span class="citation" data-cites="huguet2023HeatDiffusionPerspective">Huguet et al. (<a href="#ref-huguet2023HeatDiffusionPerspective" role="doc-biblioref">2023</a>)</span> motivates this with a result from Saloff Costes et al. <span class="citation" data-cites="saloff-coste2010HeatKernelIts">(<a href="#ref-saloff-coste2010HeatKernelIts" role="doc-biblioref">Saloff-Coste 2010</a>)</span>. For manifold satisfying the Parabolic Harnock Inequality (e.g.&nbsp;all manifolds of positive curvature), heat diffusion <span class="math inline">\(m_{x}^t\)</span> on the manifold is bounded by</p>
<p><span class="math display">\[
\frac{c_1}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_2 t}\right) \leq m_{x}^t(y) \leq \frac{c_3}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_4 t}\right)
\]</span></p>
<p>In other words, heat diffusion on non-Euclidean manifolds behaves approximately like Euclidean heat diffusion scaled by the local volume.</p>
<p>Powering the diffusion matrix approximates heat diffusion [cite Coifman]. We can thus relate our measure of the ‘spread’ or ‘energy’ of this diffusion to the manifold’s volume.</p>
<p>The Shannon entropy of the diffusion <span class="math inline">\(m_{x}\)</span> over the bounded neighborhood of <span class="math inline">\(X\)</span> with non-zero diffusion mass, can be written <span class="math display">\[
H\left(m_x^t\right)=-\int_{y_\epsilon x} m_x^t(y) \ln \left(m_x^t(y)\right)
\]</span> Here we use Shannon’s differential entropy.Though this formulation encounters pathologies on unbounded domains, it approximates the entropy a signal on a sampled region as the number of samples goes to infinity.</p>
<p>We’ll simplify the notation by combining the integral and distribution into an expected value over <span class="math inline">\(m_{x}^t\)</span> <span class="math display">\[
=-\mathbb{E}_{m_x} \ln \left(m_x(y)\right) d y
\]</span> Recalling the above result, there are constants <span class="math inline">\(c_{1}\dots c_{4}\)</span> such that</p>
<p><span class="math display">\[
\frac{c_1}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_2 t}\right) \leq m_{x}^t(y) \leq \frac{c_3}{V(x, \sqrt{t})} \exp \left(-\frac{d(x, y)^2}{c_4 t}\right)
\]</span> hence we can approximate <span class="math inline">\(H(m_{x}^t)\)</span> by</p>
<p><span class="math display">\[
=-\mathbb{E}_{m_{x}^t} \ln \left(\frac{1}{V(y, \sqrt{t})} \exp \left(\frac{-d(x, y)^2}{4 t}\right)\right)
\]</span> <span class="math display">\[
=-\mathbb{E}_{m_x^t} \ln \left(\frac{1}{V(y, \sqrt{6})}\right)-\frac{d(x, y)^2}{4 t}
\]</span> <span class="math display">\[
=\mathbb{E}_{m_x^t} \ln (V(y, \sqrt{t}))+\mathbb{E}_{m_{x}^t} \frac{d(x, y)^2}{4 t}
\]</span></p>
<p>By assumption, the volumes <span class="math inline">\(V(y,\sqrt{ t })\)</span> equal some constant <span class="math inline">\(V_{m}(\sqrt{ t })\)</span> over the support of <span class="math inline">\(m_{x}^t\)</span>. Also note that the right hand term is precisely <span class="math inline">\(\frac{1}{4t}W_{2}(\delta_{x}, m_{x}^t)\)</span>, so <span class="math display">\[
H(m_{x}^t) \simeq \ln(V_{m}(\sqrt{ t })) + \frac{1}{4t}W_{2}(d_{x},m_{x}^t)
\]</span> The Diffusion Curvature is then <span class="math display">\[
\frac{1}{t}(H(m_{E}^t) - H(m_{x}^t)) \simeq \frac{1}{t}\ln(V_{E}(\sqrt{ t })) - \frac{1}{t}\ln(V_{m}(\sqrt{ t })) + \frac{1}{4t^2}( W_{2}(d_{x},m_{E}^t) - W_{2}(d_{x},m_{x}^t))
\]</span></p>
<p>This presents us with two comparisons: one between the average volume within <span class="math inline">\(N_{t}(x)\)</span> both in Euclidean space and on the manifold, and one between the Wasserstein spread of diffusion in Euclidean space and on the manifold. Note that both exhibit the same inverse relationship to curvature. If the curvature of <span class="math inline">\(m\)</span> is greater than in euclidean space (e.g.&nbsp;positive), the volume of a ball is smaller – just as the spread of diffusion is diminished. In negative curvature, we have the opposite.</p>
</section>
<section id="diffusion-trajectory-aligment-comparing-diffusion-across-point-clouds" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="diffusion-trajectory-aligment-comparing-diffusion-across-point-clouds"><span class="header-section-number">3.3</span> Diffusion Trajectory Aligment: Comparing Diffusion Across Point Clouds</h2>
<section id="problem-setting-motivation" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="problem-setting-motivation"><span class="header-section-number">3.3.1</span> Problem Setting &amp; Motivation</h3>
<p>For a single graph, the energy of diffusion is linked to graph volume and theoretically bounded by curvature. Within a given graph, the lower the curvature, the higher the diffusion energy. As shown above, we can draw analogy to Bishop-Gromov to convert this unsigned magnitude-of-curvature estimation to a signed curvature via a comparison between spaces.</p>
<p>A comparison like this is at the heart of many discrete curvatures. Ollivier-Ricci compares the transport distance between two distributions with the transport distance on a flat space. Hickok and Blumberg compare the approximate the volume of balls on a given point cloud with the Euclidean volume of balls with the same radius. In these definitions, the use of more complex geometric constructs (distances, volumes, densities) grants the Euclidean comparison an analytic form. Our definition makes a different trade-off: it doesn’t depend on the error-prone approximations of those higher-order phenomena, but it doesn’t benefit from their theoretical simplicity.</p>
<p>To put it bluntly, there’s no analytical form of, e.g.&nbsp;the diffusion energy in <span class="math inline">\(\mathbb{R}^8\)</span> at <span class="math inline">\(t=25\)</span> because there’s no single translation between <span class="math inline">\(\mathbb{R}^8\)</span> and one of the “discrete measure space with a random walk” our definition operates on. There are many different ways of uniformly sampling <span class="math inline">\(N\)</span> points from <span class="math inline">\(\mathbb{R}^d\)</span> – and there are many ways of turning those points into a graph. But different samplings, and different modes of graph construction result in different <em>speeds of diffusion</em> on the graph, rendering direct comparison of diffusion energies difficult.</p>
<p>For example, imagine constructing two <span class="math inline">\(k\)</span>-nearest neighbor graphs with a different value of <span class="math inline">\(k\)</span> for each pointcloud. In the graph constructed with a larger <span class="math inline">\(k\)</span>, diffusion will happen more quickly because everything is more tightly connected. The diffusion energy of a point on this graph, at any given <span class="math inline">\(t\)</span>, will appear much higher than its sister graph – even if both were constructed from the same space! The vagaries of graph construction have obscured the underlying geometry of the manifold.</p>
<p>An immediate workaround is to try to bypass this obstruction by constructing graphs in <em>exactly</em> the same manner from both our target points and comparison points. One might use a <span class="math inline">\(k\)</span>nn graph with identical <span class="math inline">\(k\)</span>’s, or an adaptive kernel which sets the bandwidth locally based on the distance to each nearest neighbor. We experimented with this approach extensively, and came to the unexpected conclusion that the interplay between curvature and distance makes it extremely hard to create a kernel with the same relative bandwidth in spaces of differing curvatures. Specifically, the local Euclidean distance used by standard graph construction techniques like the k-neighbor adaptive kernel becomes less reliable the greater the magnitude of curvature. Thus, not only are comparisons between spaces compromised, but even the geometric information within a space is obscured by a gratuitously fluctuating kernel bandwidth. We explore this in more detail in the next section, and propose a simple change which lessens these symptoms.</p>
<p>However, the underlying ‘disease’ remains: although we assume our point clouds are sampled manifolds, there’s no one-to-one translation between these manifolds and the measure spaces with random walks diffusion geometry operates on. Different samplings and different means of graph construction all create different random walks. Trying to homogenize these random walks via careful graph construction relies on geometric information, like distances, that we don’t (yet) have – and tries to wage war against the notoriously fickle process of sampling. Far better than trying to homogenize these random walks on the level of <em>graph construction</em> would be a method that can reliably compare diffusions <em>across</em> graphs.</p>
<p>This problem, of reliably comparing diffusions across graphs, has wide applicability. … research needed here … Other techniques partially address this by considering <em>multiscale diffusions</em>. The <em>multiscale diffusion distance</em> takes a weighted average across powers of diffusion. Diffusion Earth Mover’s Distance extends this to an optimal transport measure based on the differences distributions diffused at multiple scales. The <em>Graph scattering transform</em> produces a rich set of features for comparing graphs derived from differences between powers of diffusion. And many graph neural networks are built upon flavors of message passing which resemble graph diffusion, while others have been modeled on graph scattering. All of these methods come with a caveat: if compared across graphs, discrepancies in graph construction will manifest as spurious differences in graph geometry.</p>
</section>
<section id="diffusion-trajectory-alignment" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="diffusion-trajectory-alignment"><span class="header-section-number">3.3.2</span> Diffusion Trajectory Alignment</h3>
<p>We consider a new approach to this problem, which replaces the diffusion time <span class="math inline">\(t\)</span> – so heavily dependent on the vagaries of graph construction – with a distance along the <em>diffusion trajectory</em>. To obtain this, we view scales of diffusion as points along a trajectory on the <span class="math inline">\(N\)</span>-dimensional probability simplex, where <span class="math inline">\(N\)</span> is the number of nodes on the graph.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> We define the Diffusion Trajectory Distance as [[[Invalid date]]</p>
<p><span class="math display">\[
d_{DTD}(x, t)= \int_{\tau=0}^{t} g_p\left(\frac{d p_x^X(\tau)}{d \tau}, \frac{ d p_x^X(\tau) }{d \tau} \right)
\]</span></p>
<p>This trajectory has the advantage of erasing any differences caused by the speed of diffusion. Instead, we can use the distance along that trajectory <em>Diffusion Trajectory Distance</em>) to align the scales of diffusion between two graphs. As illustrated in <a href="#fig-curvature-curves" class="quarto-xref">Figure&nbsp;1</a>, by comparing the diffusion energies of several graphs across their diffusion trajectory distances. Making the same comparison across times is uninformative - and misleadingly suggests that the sphere is the most negatively curved. <em>Diffusion Trajectory Alignment</em>, however, reveals the correct relation between the surfaces.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-curvature-curves" class="cell">
<div class="cell-output cell-output-display">
<div id="fig-curvature-curves" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-curvature-curves-output-2.png" class="img-fluid figure-img" alt="Line plots of diffusion energies."></p>
<figcaption class="figure-caption">Figure&nbsp;1: Diffusion energies across multiple scales on a 4-dimensional saddle, plane, and sphere (all 3-manifolds). When comparing directly between times, there is no clear separation of curvatures. Aligning diffusions by diffusion trajectory distance reveals curvature.</figcaption>
</figure>
</div>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-1" href="../nbs/3d-diffusion-trajectory-distance-normalization-preview.html#cell-fig-curvature-curves">Source: Standard libraries</a></div>
<p>Applying diffusion trajectory alignment to our diffusion energies also has the happy effect of focusing our measurement on the most relevant scales of diffusion. When using diffusion times, one must be careful to select a time around the ‘elbow’ of the diffusion energies (<a href="#def-energy-of-diffusion" class="quarto-xref">Definition&nbsp;3</a>). Smaller <span class="math inline">\(T\)</span>’s yield more localized measurements of curvature. But make <span class="math inline">\(t\)</span> too small, and the diffusion might have barely differentiated itself from a dirac, drowning the curvature in sampling-induced noise. Yet make <span class="math inline">\(t\)</span> too large, and the diffusion will progress to the uninformative steady-state distribution. With the diffusion trajectory distance, all of the undesirably large <span class="math inline">\(t\)</span>’s are compressed into a narrow range of distances – the diffusion trajectory limits to the steady state distribution, and the diffusion trajectory distances that correspond to it are all within some <span class="math inline">\(\epsilon\)</span> of the maximum diffusion trajectory distance. Finding a time around the elbow of the diffusion energies then corresponds to choosing a diffusion distance within <span class="math inline">\(80-95\)</span>% of the maximum; and via the inverse diffusion trajectory distance, <span class="math inline">\(t_{DTD}\)</span>, this can be translated to the appropriate scale of <span class="math inline">\(t\)</span> for each dataset.</p>
<p>In practice, we don’t have access to a metric <span class="math inline">\(g_{p}\)</span> on the probability simplex; this space is too high-dimensional, and we lack a continuous parameterization of graph diffusion. Instead, we approximate this integral by measuring the distances between subsequent diffusions (performed for every integer <span class="math inline">\(1 \leq t \leq T(d)\)</span>). This approximates the diffusion trajectory distances for each scale of diffusion, giving us discrete examples of <span class="math inline">\(d_{DTD}\)</span>. We then construct a piecewise linear approximation of <span class="math inline">\(d_{DTD}\)</span> to estimate the inverse, <span class="math inline">\(t_{DTD}\)</span>.</p>
</section>
<section id="the-curvature-agnostic-kernel-automatic-bandwidth-tuning" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="the-curvature-agnostic-kernel-automatic-bandwidth-tuning"><span class="header-section-number">3.3.3</span> The Curvature-Agnostic Kernel &amp; Automatic Bandwidth Tuning</h3>
<p>Diffusion trajectory alignment significantly reduces the interference caused by graph construction, but does not completely eliminate it. Due to our lack of access to a diffusion coordinate space metric, and the approximations used for the diffusion trajectory distance, we find the robustness of our algorithm is significantly improved by a few adjustments to standard graph construction techniques to make them more curvature agnostic.</p>
<p>To turn a point cloud into a graph, one places a kernel (e.g.&nbsp;a gaussian) at each point, and assigns edges to surrounding points with weights given by the kernel. The main parameter here is the bandwidth of each kernel: make it too high, and everything is connected; too low, and points are isolated. Moreover, the distances between a point and its neighbors may vary. Where the points are sparse, one wants a higher bandwidth. Where dense, a lower bandwidth.</p>
<p>A standard technique for accommodating the variable densities of point cloud data is a <span class="math inline">\(k\)</span>-neighbor <em>adaptive kernel</em>. Here, the bandwidth at each point is set to the squared distance to the <span class="math inline">\(k\)</span>-th nearest neighbor, averaged in some way between the two inputs. Here’s one standard adaptive kernel:</p>
<p><span class="math display">\[
\begin{aligned}
k(x, y) &amp; =\frac{e^{-\frac{d(x, y)^2}{d(x, N_{k}(x))^2}}}{d(y, N_{k}(y))}
+\frac{e^{-\frac{d(x, y)^2}{d(y, N_{k}(x))^2}}}{d(x, N_{k}(y)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(N_{k}(x)\)</span> gives the <span class="math inline">\(k\)</span>th nearest neighbor to <span class="math inline">\(x\)</span>.</p>
<p>Note that that parameter <span class="math inline">\(k\)</span> plays roles in two kinds of adaptation. By setting the bandwidth to some Euclidean distance <span class="math inline">\(d(x,N_{k}(x))\)</span> between data points, it adapts the kernel to the <em>right scale</em> for the point cloud, concentrating its probability within a local neighborhood. It also controls the <em>size</em> of that local neighborhood; typically set between 5 and 15.</p>
<p>Unfortunately, this adaptive process hinges on the correspondence between the Euclidean distances <span class="math inline">\(d(x,N_{k}(x))\)</span>, and the manifold distances – a correspondence that degrades <em>faster</em> the higher the magnitude of curvature! Consider a highly-curved paraboloid. The Euclidean distance from its center point to its nearest few neighbors is roughly the geodesic distance. But as the neighborhood widens, and the neighbors move higher up the paraboloid, the Euclidean distance now underestimates the true geodesic distance: it cuts across the manifold. This happens in spaces of high positive or high negative curvature.</p>
<p>As a result, the adaptive kernel constructed in highly curved spaces has a smaller bandwidth than in flat spaces, even using the same kernel parameters. If one directly compares the laziness of diffusion on two such graphs, it will appear lazier than on the flat manifold.</p>
<p>To remedy this, we make a simple adjustment to the standard adaptive kernel. The key is separating the parameter which controls neighborhood size from neighborhood scale. In the adaptive kernel, both are controlled by the parameter <span class="math inline">\(k\)</span>; as discussed above, in spaces of high curvature, the same <span class="math inline">\(k\)</span> creates different scales. Instead, we always set <span class="math inline">\(k = 1\)</span> and use another parameter – <span class="math inline">\(\nu\)</span>, the neighbor-size – to control the variance of the kernel.</p>
<p><span class="math display">\[
\begin{aligned}
k(x, y) &amp; =\frac{e^{-\frac{d(x, y)^2}{d(x, N_{k}(x))^2\nu^2}}}{d(y, N_{k}(y))}
+\frac{e^{-\frac{d(x, y)^2}{d(y, N_{k}(x))^2\nu^2}}}{d(x, N_{k}(y)}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\nu\)</span> behaves like the kernel bandwidth in the regular fixed Gaussian kernel. We call this the <em>Curvature-Agnostic Kernel</em>.</p>
<ul class="task-list">
<li><label><input type="checkbox">Figure comparing the adaptive to curvature agnostic kernels with diffusion trajectory normalization. Use a stacked image.</label></li>
</ul>
<p>Figure WAWA illustrates the increased robustness diffusion trajectory alignment gains from using this curvature-agnostic kernel. The relative ordering of graphs is still achieved by the traditional adaptive kernel, but is significantly denoised by these tweaks.</p>
<p>The main parameter of the curvature-agnostic kernel is the “neighbor scale” <span class="math inline">\(\nu\)</span>, which sets the kernel bandwidth to the specified multiple of the average distance to the <span class="math inline">\(k\)</span>th-nearest neighbor. As a single parameter, this can be tuned easily. We use as a heuristic the desired median number of points with non-negligible single-step diffusion probability, and perform a binary search over values of <span class="math inline">\(\nu\)</span> until this number is within a desired tolerance.</p>
<p>We suspect the curvature-agnostic kernel may be generally useful in manifold learning. In any downstream analysis that leverages the graph geometry – be it distance estimation, [CITE HUGUET] or metric learning [CITE US] – this analysis is made easier by a kernel whose bandwidth does not gratuitously fluctuate in regions of high curvature.</p>
<ul class="task-list">
<li><label><input type="checkbox">Can we provide experimental results here? Does PHATE or HeatGeo perform better with this kernel?</label></li>
</ul>
</section>
</section>
</section>
<section id="the-curvature-colosseum-a-discrete-curvature-benchmarking-suite" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Curvature Colosseum: A Discrete Curvature Benchmarking Suite</h1>
<p>The discrete curvature literature lacks a standard benchmark. Previous work has used a combination of evaluations on 2-dimensional toy manifolds (where correctness is visually obvious) and generally one or two bespoke higher-dimensional test cases, on which the target method is extensively tuned – and even then, often has ambiguous results. Comparisons between methods have been scarce.</p>
<p>Thus, Ollivier-Ricci curvature was introduced mostly on toy-examples. It has since been compared to Forman-Ricci (BASTIAN) ??? Hickok &amp; Blumberg primarily tested their method on the 2-dimensional torus and sphere. They compared to WAWA. However, their point clouds weren’t uniformly sampled with respect to the manifold geometry - causing some density artifacts. CHECK! Sritharan et al.&nbsp;performed the most extensive testing we have seen, with studies of correlation against ground truth and robustness to parameters. Zhang et al.&nbsp;compared accuracy at a variety of noise scales to Hickok &amp; Blumberg and to our previous unsigned diffusion laziness curvature, but only on 2-dimensional manifolds.</p>
<p>To overcome these shortcomings, we propose a discrete-curvature benchmarking suite we affectionately dub the ‘Curvature Colosseum.’ It permits standardized comparison between methods on three axes:</p>
<ol type="1">
<li><em>Resilience to Noise.</em> Pearson correlations with ground truth curvature on 2-Dimensional manifolds across noise scales.</li>
<li><em>Accuracy on High Dimensions.</em> Pearson correlations with ground truth curvature on 3-8 dimensional surfaces.</li>
<li><em>Sign Prediction</em>. Ability to distinguish spaces of positive, negative, and zero curvature in dimensions 3-8.</li>
</ol>
<p>Each of the manifolds used above is rejection sampled (CITE SOMEONE) to ensure uniform density with respect to manifold volume. When benchmarking curvature methods from the literature, we use their suggested parameters when available.</p>
<p>We provide full implementation details in the appendix. Our code is available at GITHUB LINK, along with a public dashboard of the results at ANOTHER LINK.</p>
<p>The remaining parts of this section detail the results of Diffusion Curvature and its competition on each axis of the Colosseum.</p>
<section id="performance-in-low-dimensions-under-noise" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="performance-in-low-dimensions-under-noise"><span class="header-section-number">4.1</span> Performance in Low Dimensions under Noise</h2>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-2-manifolds" class="cell">
<div class="cell-output cell-output-display">
<div id="fig-2-manifolds" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-2-manifolds-output-1.png" class="img-fluid figure-img" alt="The Torus, Saddle, Ellipsoid are shown in the first row, colored by the diffusion curvature. The second row shows the scatter plot of the Gaussian curvature vs the diffusion curvature, colored by the diffusion curvature. The origin lines are highlighted in the scatter plots."></p>
<figcaption class="figure-caption">Figure&nbsp;2: Diffusion Curvature vs Gaussian Curvature on 2-Manifolds.</figcaption>
</figure>
</div>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-2" href="../nbs/2a-Diffusion-Curvatures-of-Toy-Manifolds-preview.html#cell-fig-2-manifolds">Source: Standard libraries</a></div>
<p>First, a sanity check. Does diffusion curvature recover the Gaussian curvature of basic 2-manifolds? <a href="#fig-2-manifolds" class="quarto-xref">Figure&nbsp;2</a> shows the diffusion curvature of a Torus, Hyperboloid, and Ellipsoid, along with a scatter plot correlating diffusion curvature and Gaussian curvature. <a href="#fig-2-manifolds-visual-comparison" class="quarto-xref">Figure&nbsp;3</a> places this in the context of existing methods.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-2-manifolds-visual-comparison" class="cell">
<div class="cell-output cell-output-display">
<div id="fig-2-manifolds-visual-comparison" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-2-manifolds-visual-comparison-output-1.png" class="img-fluid figure-img" alt="The Torus, Saddle, Ellipsoid are shown in the first row, colored by the diffusion curvature. The second row shows the scatter plot of the Gaussian curvature vs the diffusion curvature, colored by the diffusion curvature. The origin lines are highlighted in the scatter plots."></p>
<figcaption class="figure-caption">Figure&nbsp;3: Diffusion Curvature vs Gaussian Curvature on 2-Manifolds.</figcaption>
</figure>
</div>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-3" href="../nbs/2-Toy-Manifolds-Benchmark-preview.html#cell-fig-2-manifolds-visual-comparison">Source: Standard libraries</a></div>
<p>All of these methods in <a href="#fig-2-manifolds-visual-comparison" class="quarto-xref">Figure&nbsp;3</a> approximately give the correct relative coloring of these 2-manifolds, except perhaps Forman-Rici. As expected, the graph-based methods, Ollivier-Ricci and Forman-Ricci, perform the worst; they weren’t designed to deal with the noise of pointcloud data. Hickok &amp; Blumberg’s definition performs the best in terms of correspondence with the ground truth Gaussian curvature, but diffusion curvature follows it closely. Observing the bottom panel of <a href="#fig-2-manifolds-visual-comparison" class="quarto-xref">Figure&nbsp;3</a>, we can see diffusion curvature corresponding closely with the red line of ground truth curvature.</p>
<p>In Table <a href="#fig-2-manifolds-noise-table" class="quarto-xref">Figure&nbsp;4</a>, we see the results of adding Gaussian noise to each manifold. Here, the noise resilience of diffusion geometry grants diffusion curvature a huge advantage. It has the highest pearson correlation with the ground truth under all but the smallest scales of noise.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-2-manifolds-noise-table" class="cell">
<div id="fig-2-manifolds-noise-table" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div id="wbgooxrtft" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>
#wbgooxrtft table {
          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
          -webkit-font-smoothing: antialiased;
          -moz-osx-font-smoothing: grayscale;
        }

#wbgooxrtft thead, tbody, tfoot, tr, td, th { border-style: none; }
 tr { background-color: transparent; }
#wbgooxrtft p { margin: 0; padding: 0; }
 #wbgooxrtft .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }
 #wbgooxrtft .gt_caption { padding-top: 4px; padding-bottom: 4px; }
 #wbgooxrtft .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }
 #wbgooxrtft .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }
 #wbgooxrtft .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }
 #wbgooxrtft .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }
 #wbgooxrtft .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }
 #wbgooxrtft .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }
 #wbgooxrtft .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }
 #wbgooxrtft .gt_column_spanner_outer:first-child { padding-left: 0; }
 #wbgooxrtft .gt_column_spanner_outer:last-child { padding-right: 0; }
 #wbgooxrtft .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }
 #wbgooxrtft .gt_spanner_row { border-bottom-style: hidden; }
 #wbgooxrtft .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }
 #wbgooxrtft .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }
 #wbgooxrtft .gt_from_md> :first-child { margin-top: 0; }
 #wbgooxrtft .gt_from_md> :last-child { margin-bottom: 0; }
 #wbgooxrtft .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }
 #wbgooxrtft .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }
 #wbgooxrtft .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }
 #wbgooxrtft .gt_row_group_first td { border-top-width: 2px; }
 #wbgooxrtft .gt_row_group_first th { border-top-width: 2px; }
 #wbgooxrtft .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }
 #wbgooxrtft .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }
 #wbgooxrtft .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }
 #wbgooxrtft .gt_left { text-align: left; }
 #wbgooxrtft .gt_center { text-align: center; }
 #wbgooxrtft .gt_right { text-align: right; font-variant-numeric: tabular-nums; }
 #wbgooxrtft .gt_font_normal { font-weight: normal; }
 #wbgooxrtft .gt_font_bold { font-weight: bold; }
 #wbgooxrtft .gt_font_italic { font-style: italic; }
 #wbgooxrtft .gt_super { font-size: 65%; }
 #wbgooxrtft .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }
 #wbgooxrtft .gt_asterisk { font-size: 100%; vertical-align: 0; }
 
</style>

<table class="gt_table table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead class="gt_header">
<tr class="header">
<th colspan="19" class="gt_heading gt_title gt_font_normal" data-quarto-table-cell-role="th">Curvature on 2-Manifolds with Noise</th>
</tr>
<tr class="odd">
<th colspan="19" class="gt_heading gt_subtitle gt_font_normal gt_bottom_border" data-quarto-table-cell-role="th">Pearson correlations and sign accuracies</th>
</tr>
</thead>
<tbody>
<tr class="odd gt_col_headings gt_spanner_row">
<td rowspan="2" id="Model &amp;amp; Metric" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Model &amp; Metric</td>
<td colspan="6" id="Torus" class="gt_center gt_columns_top_border gt_column_spanner_outer" data-quarto-table-cell-role="th" scope="colgroup"><span class="gt_column_spanner">Torus</span></td>
<td colspan="6" id="Hyperboloid" class="gt_center gt_columns_top_border gt_column_spanner_outer" data-quarto-table-cell-role="th" scope="colgroup"><span class="gt_column_spanner">Hyperboloid</span></td>
<td colspan="6" id="Ellipsoid" class="gt_center gt_columns_top_border gt_column_spanner_outer" data-quarto-table-cell-role="th" scope="colgroup"><span class="gt_column_spanner">Ellipsoid</span></td>
</tr>
<tr class="even gt_col_headings">
<td id="Torus" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Torus</td>
<td id="Torus noise = 0.05" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Torus noise = 0.05</td>
<td id="Torus noise = 0.1" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Torus noise = 0.1</td>
<td id="Torus noise = 0.15" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Torus noise = 0.15</td>
<td id="Torus noise = 0.2" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Torus noise = 0.2</td>
<td id="Torus noise = 0.25" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Torus noise = 0.25</td>
<td id="Hyperboloid" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Hyperboloid</td>
<td id="Hyperboloid noise = 0.05" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Hyperboloid noise = 0.05</td>
<td id="Hyperboloid noise = 0.1" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Hyperboloid noise = 0.1</td>
<td id="Hyperboloid noise = 0.15" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Hyperboloid noise = 0.15</td>
<td id="Hyperboloid noise = 0.2" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Hyperboloid noise = 0.2</td>
<td id="Hyperboloid noise = 0.25" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Hyperboloid noise = 0.25</td>
<td id="Ellipsoid" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Ellipsoid</td>
<td id="Ellipsoid noise = 0.05" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Ellipsoid noise = 0.05</td>
<td id="Ellipsoid noise = 0.1" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Ellipsoid noise = 0.1</td>
<td id="Ellipsoid noise = 0.15" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Ellipsoid noise = 0.15</td>
<td id="Ellipsoid noise = 0.2" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Ellipsoid noise = 0.2</td>
<td id="Ellipsoid noise = 0.25" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">Ellipsoid noise = 0.25</td>
</tr>
</tbody>
<tbody class="gt_table_body">
<tr class="odd gt_group_heading_row">
<td colspan="19" class="gt_group_heading" data-quarto-table-cell-role="th">Pearson Correlation</td>
</tr>
<tr class="even">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Diffusion Curvature</td>
<td class="gt_row gt_right">0.979</td>
<td class="gt_row gt_right">0.978</td>
<td class="gt_row gt_right" style="font-weight: bold">0.978</td>
<td class="gt_row gt_right" style="font-weight: bold">0.972</td>
<td class="gt_row gt_right" style="font-weight: bold">0.959</td>
<td class="gt_row gt_right" style="font-weight: bold">0.937</td>
<td class="gt_row gt_right">0.471</td>
<td class="gt_row gt_right">0.501</td>
<td class="gt_row gt_right">0.475</td>
<td class="gt_row gt_right">0.422</td>
<td class="gt_row gt_right">0.350</td>
<td class="gt_row gt_right">0.297</td>
<td class="gt_row gt_right">0.606</td>
<td class="gt_row gt_right">0.626</td>
<td class="gt_row gt_right">0.638</td>
<td class="gt_row gt_right" style="font-weight: bold">0.640</td>
<td class="gt_row gt_right" style="font-weight: bold">0.634</td>
<td class="gt_row gt_right" style="font-weight: bold">0.621</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Forman-Ricci</td>
<td class="gt_row gt_right">0.717</td>
<td class="gt_row gt_right">0.734</td>
<td class="gt_row gt_right">0.764</td>
<td class="gt_row gt_right">0.768</td>
<td class="gt_row gt_right">0.765</td>
<td class="gt_row gt_right">0.771</td>
<td class="gt_row gt_right">−0.106</td>
<td class="gt_row gt_right">−0.090</td>
<td class="gt_row gt_right">−0.080</td>
<td class="gt_row gt_right">−0.094</td>
<td class="gt_row gt_right">−0.150</td>
<td class="gt_row gt_right">−0.198</td>
<td class="gt_row gt_right">0.482</td>
<td class="gt_row gt_right">0.511</td>
<td class="gt_row gt_right">0.522</td>
<td class="gt_row gt_right">0.509</td>
<td class="gt_row gt_right">0.507</td>
<td class="gt_row gt_right">0.502</td>
</tr>
<tr class="even">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Hickok &amp; Blumberg</td>
<td class="gt_row gt_right" style="font-weight: bold">0.987</td>
<td class="gt_row gt_right" style="font-weight: bold">0.978</td>
<td class="gt_row gt_right">0.950</td>
<td class="gt_row gt_right">0.910</td>
<td class="gt_row gt_right">0.861</td>
<td class="gt_row gt_right">0.811</td>
<td class="gt_row gt_right" style="font-weight: bold">0.651</td>
<td class="gt_row gt_right" style="font-weight: bold">0.650</td>
<td class="gt_row gt_right" style="font-weight: bold">0.644</td>
<td class="gt_row gt_right" style="font-weight: bold">0.634</td>
<td class="gt_row gt_right" style="font-weight: bold">0.622</td>
<td class="gt_row gt_right" style="font-weight: bold">0.614</td>
<td class="gt_row gt_right" style="font-weight: bold">0.734</td>
<td class="gt_row gt_right" style="font-weight: bold">0.745</td>
<td class="gt_row gt_right" style="font-weight: bold">0.704</td>
<td class="gt_row gt_right">0.623</td>
<td class="gt_row gt_right">0.544</td>
<td class="gt_row gt_right">0.479</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Ollivier-Ricci</td>
<td class="gt_row gt_right">0.614</td>
<td class="gt_row gt_right">0.625</td>
<td class="gt_row gt_right">0.664</td>
<td class="gt_row gt_right">0.685</td>
<td class="gt_row gt_right">0.699</td>
<td class="gt_row gt_right">0.704</td>
<td class="gt_row gt_right">0.389</td>
<td class="gt_row gt_right">0.375</td>
<td class="gt_row gt_right">0.367</td>
<td class="gt_row gt_right">0.347</td>
<td class="gt_row gt_right">0.328</td>
<td class="gt_row gt_right">0.324</td>
<td class="gt_row gt_right">0.359</td>
<td class="gt_row gt_right">0.369</td>
<td class="gt_row gt_right">0.380</td>
<td class="gt_row gt_right">0.394</td>
<td class="gt_row gt_right">0.410</td>
<td class="gt_row gt_right">0.407</td>
</tr>
<tr class="even gt_group_heading_row">
<td colspan="19" class="gt_group_heading" data-quarto-table-cell-role="th">Sign Accuracy</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Diffusion Curvature</td>
<td class="gt_row gt_right" style="font-weight: bold">0.903</td>
<td class="gt_row gt_right" style="font-weight: bold">0.849</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right" style="font-weight: bold">0.916</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.190</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.266</td>
<td class="gt_row gt_right">0.235</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right">0.509</td>
<td class="gt_row gt_right">0.529</td>
<td class="gt_row gt_right">0.251</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
</tr>
<tr class="even">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Forman-Ricci</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right">0.341</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Hickok &amp; Blumberg</td>
<td class="gt_row gt_right">0.696</td>
<td class="gt_row gt_right">0.810</td>
<td class="gt_row gt_right" style="font-weight: bold">0.878</td>
<td class="gt_row gt_right">0.406</td>
<td class="gt_row gt_right">0.365</td>
<td class="gt_row gt_right">0.365</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.215</td>
<td class="gt_row gt_right">0.415</td>
<td class="gt_row gt_right">0.474</td>
<td class="gt_row gt_right">0.501</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right">0.098</td>
<td class="gt_row gt_right">0.002</td>
<td class="gt_row gt_right">0.002</td>
<td class="gt_row gt_right">0.002</td>
</tr>
<tr class="even">
<td class="gt_row gt_left gt_stub" data-quarto-table-cell-role="th">Ollivier-Ricci</td>
<td class="gt_row gt_right">0.659</td>
<td class="gt_row gt_right">0.659</td>
<td class="gt_row gt_right">0.659</td>
<td class="gt_row gt_right" style="font-weight: bold">0.659</td>
<td class="gt_row gt_right">0.659</td>
<td class="gt_row gt_right" style="font-weight: bold">0.659</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right">0.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
<td class="gt_row gt_right" style="font-weight: bold">1.000</td>
</tr>
</tbody>
</table>


</div>
        
<figcaption class="figure-caption">Figure&nbsp;4: Performance of methods under noise on 2-Manifolds with Noise.</figcaption>
</figure>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-4" href="../nbs/2-Toy-Manifolds-Benchmark-preview.html#cell-fig-2-manifolds-noise-table">Source: Standard libraries</a></div>
</section>
<section id="performance-in-high-dimensions-under-noise" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="performance-in-high-dimensions-under-noise"><span class="header-section-number">4.2</span> Performance in High Dimensions under Noise</h2>
<p>TBD!</p>
</section>
<section id="sign-differentiation" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sign-differentiation"><span class="header-section-number">4.3</span> Sign Differentiation</h2>
<p>To test the ability to differentiate negative from positive curvature in high dimensions, we constructed a dataset containing saddle points, planes, and spheres in dimensions 2-6. For planes, we used uniform samples from the <span class="math inline">\(d\)</span>-dimensional unit cube; for spheres, we sampled points from a Gaussian in <span class="math inline">\(\mathbb{R}^d\)</span> and normalized them to have unit length; for saddles, we constructed a quadric surface with the sign of each quadratic alternating between dimensions, and performed rejection sampling on the resulting parameterization. We sampled 2000 points from each surface, and performed this sampling 20 times per surface, creating a dataset of 60 point clouds of negative, zero, and positive curvature.</p>
<p>To make the comparison as fair as possible, we use our curvature-agnostic kernel to construct the graphs used for Forman-Ricci and Ollivier-Ricci curvature instead of the standard adaptive or k-nearest neighbor kernels generally used (which give worse performance). Hickok &amp; Bloomberg’s definition operates directly on pointclouds, so we leave it as is.</p>
<p>As seen in <a href="#fig-sadspheres" class="quarto-xref">Figure&nbsp;5</a>, only diffusion curvature differentiates the sign across all dimensions. Hickok &amp; Blumberg’s curvature initially correctly classifies saddles as negatively curved and spheres as positive in dimension 2, however in dimensions 3-5, it classifies everything as negatively curved, with planes given a higher negative curvature than saddles. By dimension 6, it assigns the most negatively curved surface the least degree of negative curvature. For Ollivier-Ricci, the variance within samplings is extremely high (an artifact of applying a graph-based definition to noisy pointclouds), but it both describes all curvatures as positive and gives the negatively-curved dataset the highest positive curvature. Forman-Ricci does the best among these, comparatively; it at least gives saddles, spheres, and planes the correct ordering of curvature. However the magnitude of its curvatures is entirely negative; there’s no sign differentiation.</p>
<p>Diffusion curvature both preserves the correct relative ordering and distinguishses between the signs of saddles and spheres with 95% accuracy.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-sadspheres" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-sadspheres" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-sadspheres-output-1.png" class="img-fluid figure-img" alt="..."></p>
<figcaption class="figure-caption">Figure&nbsp;5: Predicted curvatures of Saddles and Spheres in dimensions 2-6. Diffusion Curvature robustly distinguishes between the signs of the data, even in high dimensions, and with relative sparsity.</figcaption>
</figure>
</div>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-5" href="../nbs/5-sign-prediction-tests-preview.html#cell-fig-sadspheres">Source: Standard libraries</a></div>
</section>
<section id="resilience-to-parameters" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="resilience-to-parameters"><span class="header-section-number">4.4</span> Resilience to Parameters</h2>
</section>
</section>
<section id="applications" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Applications</h1>
<section id="loss-landscapes" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="loss-landscapes"><span class="header-section-number">5.1</span> Loss Landscapes</h2>
</section>
<section id="curvature-as-a-tda-filtration" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="curvature-as-a-tda-filtration"><span class="header-section-number">5.2</span> Curvature as a TDA Filtration</h2>
</section>
</section>
<section id="related-work" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Related Work</h1>
<section id="foreman-ricci-curvature" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="foreman-ricci-curvature"><span class="header-section-number">6.1</span> Foreman Ricci Curvature</h2>
</section>
<section id="hickock-blumbergs-volume-comparison-curvature" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="hickock-blumbergs-volume-comparison-curvature"><span class="header-section-number">6.2</span> Hickock &amp; Blumberg’s Volume Comparison Curvature</h2>
</section>
<section id="sritharan" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sritharan"><span class="header-section-number">6.3</span> Sritharan</h2>
</section>
</section>
<section id="conclusion" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Conclusion</h1>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bhaskar2022DiffusionCurvatureEstimating" class="csl-entry" role="listitem">
Bhaskar, Dhananjay, Kincaid MacDonald, Oluwadamilola Fasina, Dawson Thomas, Bastian Rieck, Ian Adelstein, and Smita Krishnaswamy. 2022. <span>“Diffusion Curvature for Estimating Local Curvature in High Dimensional Data.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 21738–49. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/88438dc62fc5c8777e2b5f1b4f6d37a2-Abstract-Conference.html">https://proceedings.neurips.cc/paper_files/paper/2022/hash/88438dc62fc5c8777e2b5f1b4f6d37a2-Abstract-Conference.html</a>.
</div>
<div id="ref-2021BishopGromovInequality" class="csl-entry" role="listitem">
<span>“Bishop–<span>Gromov</span> Inequality.”</span> 2021. In <em>Wikipedia</em>. <a href="https://en.wikipedia.org/w/index.php?title=Bishop%E2%80%93Gromov_inequality&amp;oldid=1059331416">https://en.wikipedia.org/w/index.php?title=Bishop%E2%80%93Gromov_inequality&amp;oldid=1059331416</a>.
</div>
<div id="ref-coifman2006DiffusionMaps" class="csl-entry" role="listitem">
Coifman, Ronald R., and Stéphane Lafon. 2006. <span>“Diffusion Maps.”</span> <em>Applied and Computational Harmonic Analysis</em>, Special <span>Issue</span>: <span>Diffusion Maps</span> and <span>Wavelets</span>, 21 (1): 5–30. <a href="https://doi.org/10.1016/j.acha.2006.04.006">https://doi.org/10.1016/j.acha.2006.04.006</a>.
</div>
<div id="ref-fasina2023NeuralFIMLearning" class="csl-entry" role="listitem">
Fasina, Oluwadamilola, Guillaume Huguet, Alexander Tong, Yanlei Zhang, Guy Wolf, Maximilian Nickel, Ian Adelstein, and Smita Krishnaswamy. 2023. <span>“Neural <span>FIM</span> for Learning <span>Fisher Information Metrics</span> from Point Cloud Data.”</span> June 11, 2023. <a href="https://doi.org/10.48550/arXiv.2306.06062">https://doi.org/10.48550/arXiv.2306.06062</a>.
</div>
<div id="ref-hickok2023IntrinsicApproachScalarCurvature" class="csl-entry" role="listitem">
Hickok, Abigail, and Andrew J. Blumberg. 2023. <span>“An <span>Intrinsic Approach</span> to <span>Scalar-Curvature Estimation</span> for <span>Point Clouds</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2308.02615">https://doi.org/10.48550/arXiv.2308.02615</a>.
</div>
<div id="ref-huguet2023HeatDiffusionPerspective" class="csl-entry" role="listitem">
Huguet, Guillaume, Alexander Tong, Edward De Brouwer, Yanlei Zhang, Guy Wolf, Ian Adelstein, and Smita Krishnaswamy. 2023. <span>“A <span>Heat Diffusion Perspective</span> on <span>Geodesic Preserving Dimensionality Reduction</span>.”</span> May 30, 2023. <a href="https://doi.org/10.48550/arXiv.2305.19043">https://doi.org/10.48550/arXiv.2305.19043</a>.
</div>
<div id="ref-moon2019VisualizingStructureTransitions" class="csl-entry" role="listitem">
Moon, Kevin R., David van Dijk, Zheng Wang, Scott Gigante, Daniel B. Burkhardt, William S. Chen, Kristina Yim, et al. 2019. <span>“Visualizing Structure and Transitions in High-Dimensional Biological Data.”</span> <em>Nature Biotechnology</em> 37 (12, 12): 1482–92. <a href="https://doi.org/10.1038/s41587-019-0336-3">https://doi.org/10.1038/s41587-019-0336-3</a>.
</div>
<div id="ref-ollivier2009RicciCurvatureMarkov" class="csl-entry" role="listitem">
Ollivier, Yann. 2009. <span>“Ricci Curvature of <span>Markov</span> Chains on Metric Spaces.”</span> <em>Journal of Functional Analysis</em> 256 (3): 810–64. <a href="https://doi.org/10.1016/j.jfa.2008.11.001">https://doi.org/10.1016/j.jfa.2008.11.001</a>.
</div>
<div id="ref-saloff-coste2010HeatKernelIts" class="csl-entry" role="listitem">
Saloff-Coste, Laurent. 2010. <span>“The Heat Kernel and Its Estimates.”</span> In <em>Advanced <span>Studies</span> in <span>Pure Mathematics</span></em>, 405–36. Kyoto University, Japan. <a href="https://doi.org/10.2969/aspm/05710405">https://doi.org/10.2969/aspm/05710405</a>.
</div>
<div id="ref-tong2021DiffusionEarthMovera" class="csl-entry" role="listitem">
Tong, Alexander Y., Guillaume Huguet, Amine Natik, Kincaid Macdonald, Manik Kuchroo, Ronald Coifman, Guy Wolf, and Smita Krishnaswamy. 2021. <span>“Diffusion <span>Earth Mover</span>’s <span>Distance</span> and <span>Distribution Embeddings</span>.”</span> In <em>Proceedings of the 38th <span>International Conference</span> on <span>Machine Learning</span></em>, 10336–46. PMLR. <a href="https://proceedings.mlr.press/v139/tong21a.html">https://proceedings.mlr.press/v139/tong21a.html</a>.
</div>
<div id="ref-tong2021DatadrivenLearningGeometric" class="csl-entry" role="listitem">
Tong, Alexander, Frederick Wenkel, Kincaid Macdonald, Smita Krishnaswamy, and Guy Wolf. 2021. <span>“Data-Driven Learning of Geometric Scattering Modules for Gnns.”</span> In <em>2021 <span>IEEE</span> 31st <span>International Workshop</span> on <span>Machine Learning</span> for <span>Signal Processing</span> (<span>MLSP</span>)</em>, 1–6. IEEE. <a href="https://ieeexplore.ieee.org/abstract/document/9596169/">https://ieeexplore.ieee.org/abstract/document/9596169/</a>.
</div>
</div>
</section>


<div id="quarto-appendix" class="default"><aside id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Consider kernel density estimation, where one places gaussians on each data point and sums up the probability assigned to each area. The higher the curvature, the higher the reported density, since at the end of an ellipsoid the gaussians have greater overlap with each other than in its center – even though the intrinsic density of the surface is uniform.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This simplex corresponds to the ‘diffusion coordinates’ used by Coifman, as well as the setting of Fasina’s diffusion-based Fisher Information metric (CITE). <a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{macdonald2024,
  author = {MacDonald, Kincaid and Bhaskar, Dhananjay and Zhang, Yanlei
    and Adelstein, Ian and Krishnaswamy, Smita},
  title = {Bridging {Diffusion} {Geometry} to {Curvature}},
  pages = {undefined},
  date = {2024-08-12},
  langid = {en},
  abstract = {Modeling data geometry has proven a rich source of
    interpretable features: quantifying “the shape” of molecules,
    cellular trajectories, and code enhances downstream classification
    performance. Yet, creating these features requires a theoretical
    “Rosetta Stone” to bridge geometric theory into the noisy, discrete
    world of real data. Various bridging paradigms have been proposed,
    most prominently *diffusion geometry* – and have given rise to
    manifold distance estimators, and manifold embedding
    techniques.However, the estimation of curvature remains relatively
    unexplored. Curvature, being the most local geometric measure, is
    among the most challenging concepts to bridge into the discrete
    realm; for this reason, it is also a sterling test a given
    paradigm’s ability to translate geometric theory into the sampled
    data.Here, we introduce *Diffusion Curvature*, a new definition of
    scalar curvature on point clouds which inherits diffusion geometry’s
    robustness to noise and sampling. In our benchmarks, diffusion
    curvature proves itself the *only* method capable of robustly
    differentiating positive and negative curvature in high dimensions,
    while its absence of parameters requiring user tuning makes it
    significantly more user-friendly than other methods.We describe
    theoretical connections between diffusion curvature and the
    Ollivier-Ricci curvature, and introduce a general paradigm for
    comparing diffusions across graphs.We apply diffusion curvature to
    neural loss landscapes, single-cell RNA data {[}bit about what we
    find{]} motivating the practical utility of our method.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-macdonald2024" class="csl-entry quarto-appendix-citeas" role="listitem">
MacDonald, Kincaid, Dhananjay Bhaskar, Yanlei Zhang, Ian Adelstein, and
Smita Krishnaswamy. 2024. <span>“Bridging Diffusion Geometry to
Curvature.”</span> IEEE TPAMI. August 12, 2024.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        console.log("RESIZE");
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>