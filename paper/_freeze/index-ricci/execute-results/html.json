{
  "hash": "d3597c13c519a82f3b369cf41642f049",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: A Heat Diffusion-based Ricci Curvature and Applications to GNNs\nauthor:\n  - name: Kincaid MacDonald\n    orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: kincaid@aya.yale.edu\n    roles: []\n    affiliations:\n    - Yale\n  - name: Dhananjay Bhaskar\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations: \n    - MILA\n  - name: Kaly Zhang\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations: \n    - MILA\n  - name: Xingzhi Sun\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations: \n    - Yale\n  - name: Ian Adelstein\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n    - Yale Department of Math\n  - name: Smita Krishnaswamy\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n    - Yale Department of Applied Math\n    - Yale School of Medicine\nkeywords:\n    - Manifold Learning\n    - Geometric Deep Learning\n    - Graph Curvature\n    - Point Clouds\nabstract: \n    For a number of years now work has been proceeding in order to bring to perfection the crudely conceived idea of a machine that would not only supply inverse reactive current for use in unilateral phase detractors, but would also be capable of automatically synchronizing cardinal grammeters. Such a machine is the \"Turbo-Encabulator.\"\nplain-language-summary: |\n    We introduce Diffusion Curvature, a fast, differentiable, noise-robust pointwise curvature for graphs and point clouds.\nkey-points: []\ndate: last-modified\nbibliography: references.bib\ncitation:\n    container-title: IEEE TPAMI\nnumber-sections: true\n---\n\n:::{#c7f22a2b .cell .markdown}\n## Introduction\n\nRecent years have seen a growing appreciation that black-box machine learning methods can understand data better if equipped with geometric priors, both globally [cite TDA methods] and locally [cite heat & wave, energy]. This is especially true for graph-structured data. The graph manages to be the most general data structure only by ripping itself from the extrinsic context in which geometric measures like curvature are most easily derived. While pointclouds within ambient space can be viewed *extrinsically*, like an astronaut’s view of the Earth, the graph’s view of itself is solely *intrinsic*, like Poincare’s ant on a sphere.\n\nThough there exist purely intrinsic measures of geometry, they are (as intuition would suggest) more specialized and arcane then extrinsic measures, involving complex constructs like optimal transport and geodesics. The result: while point cloud-based neural networks (feedforward, CNNs, diffusion models) can easily by themselves infer geometry through their extrinsic view, GNNs are left with a much harder problem: deriving the complex machinery of, e.g., geodesics and Wasserstein distances from scratch.\n\nHere we present an alternative: a differentiable, scale-invariant, noise-resistant module that equips GNNs with graph curvatures.\n<!-- \n\nRecent years have seen a growing appreciation that black-box machine learning methods can understand data better if equipped with geometric priors, both globally [cite TDA methods] and locally [cite heat & wave, energy]. This is especially true for graph-structured data. The usefulness of graphs as a general data representation comes at a cost: that data is “ripped from its context”. To take an example, when point cloud data becomes a graph, the *extrinsic* geometric perspective afforded by a bird’s eye view of the points is reduced to an *intrinsic* perspective – the ‘ant’s eye’ view. \n\nOutline:\n-  -->\n\n\n# Diffusion Ricci Curvature\n\n\n\n\n\n\n\n\n{{< embed ../nbs/experiments/2c3-are-kernels-zeitgeibers.ipynb#fig-spread-of-diffusion-2d >}}\n\n\n\n\n\n\n\n\n\n## Data & Methods {#sec-data-methods}\n:::\n\n::: {#cell-torus .cell execution_count=1}\n``` {}\n#| label: torus\n#| fig-cap: Diffusion Curvature vs Gaussian Curvature of the Torus\n#| fig-alt: The diffusion curvature corresponds closely to the gaussian curvature of the torus\nfrom diffusion_curvature.utils import plot_3d\nfrom diffusion_curvature.datasets import torus\nfrom diffusion_curvature.core import DiffusionCurvature, get_adaptive_graph\nX_torus, ks_torus = torus(2000)\nDC = DiffusionCurvature()\nG_torus = get_adaptive_graph(X_torus, k = 5, alpha = 1)\nks_computed = DC.curvature(G_torus, dim=2, t = 25)\nplot_3d(X_torus, ks_computed, colorbar=True)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n```\n2024-03-29 11:19:48,865:[WARNING](pygsp.graphs.graph.check_weights): The main diagonal of the weight matrix is not 0!\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n2024-03-29 11:19:50,095:[WARNING](pygsp.graphs.graph.check_weights): The main diagonal of the weight matrix is not 0!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Diffusion Curvature vs Gaussian Curvature of the Torus](index-ricci_files/figure-html/torus-output-2.png){#torus width=715 height=758 fig-alt='The diffusion curvature corresponds closely to the gaussian curvature of the torus'}\n:::\n:::\n\n\n:::{#56c85e8e .cell .markdown}\n## Conclusion\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n:::\n\n",
    "supporting": [
      "index-ricci_files"
    ],
    "filters": [],
    "includes": {}
  }
}