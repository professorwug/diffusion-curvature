@inreference{2021BishopGromovInequality,
  title = {Bishop–{{Gromov}} Inequality},
  booktitle = {Wikipedia},
  date = {2021-12-08T21:05:37Z},
  url = {https://en.wikipedia.org/w/index.php?title=Bishop%E2%80%93Gromov_inequality&oldid=1059331416},
  urldate = {2023-05-04},
  abstract = {In mathematics, the Bishop–Gromov inequality is a comparison theorem in Riemannian geometry, named after Richard L. Bishop and Mikhail Gromov. It is closely related to Myers' theorem, and is the key point in the proof of Gromov's compactness theorem.},
  langid = {english},
  annotation = {Page Version ID: 1059331416},
  file = {/Users/adjourner/Zotero/storage/7XLQ97UP/Bishop–Gromov_inequality.html}
}

@article{ben-chen2008ConformalFlatteningCurvature,
  title = {Conformal {{Flattening}} by {{Curvature Prescription}} and {{Metric Scaling}}},
  author = {Ben‐Chen, Mirela and Gotsman, Craig and Bunin, Guy},
  date = {2008-04},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {27},
  number = {2},
  pages = {449--458},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2008.01142.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2008.01142.x},
  urldate = {2024-02-06},
  abstract = {Abstract             We present an efficient method to conformally parameterize 3D mesh data sets to the plane. The idea behind our method is to concentrate all the 3D curvature at a small number of select mesh vertices, called cone singularities, and then cut the mesh through those singular vertices to obtain disk topology. The singular vertices are chosen automatically. As opposed to most previous methods, our flattening process involves only the solution of linear systems of Poisson equations, thus is very efficient. Our method is shown to be faster than existing methods, yet generates parameterizations having comparable quasi‐conformal distortion.},
  langid = {english},
  file = {/Users/adjourner/Zotero/storage/HUHV3VLY/Ben‐Chen2008-Conformal Flattening by Curvature Prescription and Metric Scaling.pdf}
}

@article{bhaskar2022DiffusionCurvatureEstimating,
  title = {Diffusion Curvature for Estimating Local Curvature in High Dimensional Data},
  author = {Bhaskar, Dhananjay and MacDonald, Kincaid and Fasina, Oluwadamilola and Thomas, Dawson and Rieck, Bastian and Adelstein, Ian and Krishnaswamy, Smita},
  date = {2022},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {21738--21749},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/88438dc62fc5c8777e2b5f1b4f6d37a2-Abstract-Conference.html},
  urldate = {2023-12-01},
  file = {/Users/adjourner/Pumberton/Library/Bhaskar2022-Diffusion curvature for estimating local curvature in high dimensional data.pdf}
}

@article{cayeux2018CurvatureHypergraphsMultiMarginal,
  title = {Curvature of {{Hypergraphs}} via {{Multi-Marginal Optimal Transport}}},
  author = {Cayeux, Eric and {Shahab Asoodeh} and {Shahab Asoodeh} and Asoodeh, Shahab and {Tingran Gao} and Gao, Tingran and {James A. Evans} and Evans, James A.},
  date = {2018-12-01},
  pages = {1180--1185},
  doi = {10.1109/cdc.2018.8619706},
  abstract = {We introduce a novel definition of curvature for hypergraphs, a natural generalization of graphs, by introducing a multi-marginal optimal transport problem for a naturally defined random walk on the hypergraph. This curvature, termed coarse scalar curvature, extends a recent definition of Ricci curvature for Markov chains on metric spaces by Ollivier [Journal of Functional Analysis 256 (2009) 810–864], and is related to the scalar curvature when the hypergraph arises naturally from a Riemannian manifold. We investigate basic theoretical properties of the coarse scalar curvature and obtain several bounds. Empirical experiments demonstrate that coarse scalar curvatures detects “bridges” across connected components in hypergraphs, akin to the behavior of coarse Ricci curvatures on graphs.},
  annotation = {MAG ID: 2962985127}
}

@article{coifman2006DiffusionMaps,
  title = {Diffusion Maps},
  author = {Coifman, Ronald R. and Lafon, Stéphane},
  date = {2006-07-01},
  journaltitle = {Applied and Computational Harmonic Analysis},
  shortjournal = {Applied and Computational Harmonic Analysis},
  series = {Special {{Issue}}: {{Diffusion Maps}} and {{Wavelets}}},
  volume = {21},
  number = {1},
  pages = {5--30},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2006.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1063520306000546},
  urldate = {2023-04-24},
  abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.},
  langid = {english},
  keywords = {Diffusion metric,Diffusion processes,Dimensionality reduction,Eigenmaps,Graph Laplacian,Manifold learning},
  file = {/Users/adjourner/Zotero/storage/H7AC8LRU/Coifman and Lafon - 2006 - Diffusion maps.pdf}
}

@article{florentinmunch2019NonnegativeOllivierCurvature,
  title = {Non-Negative {{Ollivier}} Curvature on Graphs, Reverse {{Poincaré}} Inequality, {{Buser}} Inequality, {{Liouville}} Property, {{Harnack}} Inequality and Eigenvalue Estimates},
  author = {{Florentin Münch} and Münch, Florentin},
  date = {2019-07-31},
  journaltitle = {arXiv: Differential Geometry},
  abstract = {We prove that for combinatorial graphs with non-negative Ollivier curvature, one has \textbackslash [ \textbackslash |P\_t \textbackslash mu - P\_t \textbackslash nu\textbackslash |\_1 \textbackslash leq \textbackslash frac\{W\_1(\textbackslash mu,\textbackslash nu)\}\{\textbackslash sqrt\{t\}\} \textbackslash ] for all probability measures \$\textbackslash mu,\textbackslash nu\$ where \$P\_t\$ is the heat semigroup and \$W\_1\$ is the \$\textbackslash ell\_1\$-Wasserstein distance. This turns out to be an equivalent formulation of a version of reverse Poincare inequality. Furthermore, this estimate allows us to prove Buser inequality, Liouville property and the the eigenvalue estimate \$\textbackslash lambda\_1 \textbackslash geq \textbackslash log(2)/\textbackslash operatorname\{diam\}\textasciicircum 2\$.},
  annotation = {MAG ID: 2965452863}
}

@unpublished{gilmer2021LossCurvaturePerspective,
  title = {A {{Loss Curvature Perspective}} on {{Training Instability}} in {{Deep Learning}}},
  author = {Gilmer, Justin and Ghorbani, Behrooz and Garg, Ankush and Kudugunta, Sneha and Neyshabur, Behnam and Cardoze, David and Dahl, George and Nado, Zachary and Firat, Orhan},
  date = {2021-10-08},
  eprint = {2110.04369},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2110.04369},
  urldate = {2021-10-19},
  abstract = {In this work, we study the evolution of the loss Hessian across many classification tasks in order to understand the effect the curvature of the loss has on the training dynamics. Whereas prior work has focused on how different learning rates affect the loss Hessian observed during training, we also analyze the effects of model initialization, architectural choices, and common training heuristics such as gradient clipping and learning rate warmup. Our results demonstrate that successful model and hyperparameter choices allow the early optimization trajectory to either avoid -- or navigate out of -- regions of high curvature and into flatter regions that tolerate a higher learning rate. Our results suggest a unifying perspective on how disparate mitigation strategies for training instability ultimately address the same underlying failure mode of neural network optimization, namely poor conditioning. Inspired by the conditioning perspective, we show that learning rate warmup can improve training stability just as much as batch normalization, layer normalization, MetaInit, GradInit, and Fixup initialization.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/adjourner/Zotero/storage/B329ZULS/Gilmer et al_2021_A Loss Curvature Perspective on Training Instability in Deep Learning.pdf;/Users/adjourner/Zotero/storage/LMRJI5FU/2110.html}
}

@unpublished{hickok2023IntrinsicApproachScalarCurvature,
  title = {An {{Intrinsic Approach}} to {{Scalar-Curvature Estimation}} for {{Point Clouds}}},
  author = {Hickok, Abigail and Blumberg, Andrew J.},
  date = {2023-08-04},
  eprint = {2308.02615},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.02615},
  url = {http://arxiv.org/abs/2308.02615},
  urldate = {2023-09-25},
  abstract = {We introduce an intrinsic estimator for the scalar curvature of a data set presented as a finite metric space. Our estimator depends only on the metric structure of the data and not on an embedding in \$\textbackslash mathbb\{R\}\textasciicircum n\$. We show that the estimator is consistent in the sense that for points sampled from a probability measure on a compact Riemannian manifold, the estimator converges to the scalar curvature as the number of points increases. To justify its use in applications, we show that the estimator is stable with respect to perturbations of the metric structure, e.g., noise in the sample or error estimating the intrinsic metric. We validate our estimator experimentally on synthetic data that is sampled from manifolds with specified curvature.},
  keywords = {Computer Science - Computational Geometry,Statistics - Machine Learning},
  file = {/Users/adjourner/Zotero/storage/5IK4A7EY/Hickock2023 Intrinsic Curvature.pdf;/Users/adjourner/Zotero/storage/C2LKU46I/2308.html}
}

@online{huguet2023HeatDiffusionPerspective,
  title = {A {{Heat Diffusion Perspective}} on {{Geodesic Preserving Dimensionality Reduction}}},
  author = {Huguet, Guillaume and Tong, Alexander and De Brouwer, Edward and Zhang, Yanlei and Wolf, Guy and Adelstein, Ian and Krishnaswamy, Smita},
  date = {2023-05-30},
  eprint = {2305.19043},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  doi = {10.48550/arXiv.2305.19043},
  url = {http://arxiv.org/abs/2305.19043},
  urldate = {2023-09-07},
  abstract = {Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call heat geodesic embeddings. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state of the art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-sequencing datasets with both continuum and cluster structure, where our method enables interpolation of withheld timepoints of data. Finally, we show that parameters of our more general method can be configured to give results similar to PHATE (a state-of-the-art diffusion based manifold learning method) as well as SNE (an attraction/repulsion neighborhood based method that forms the basis of t-SNE).},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/adjourner/Zotero/storage/V9X5X8WN/Huguet et al. - 2023 - A Heat Diffusion Perspective on Geodesic Preservin.pdf;/Users/adjourner/Zotero/storage/SRDJ92D9/2305.html}
}

@article{janmaas2011GradientFlowsEntropy,
  title = {Gradient Flows of the Entropy for Finite {{Markov}} Chains},
  author = {{Jan Maas} and Maas, Jan},
  date = {2011-10-15},
  journaltitle = {Journal of Functional Analysis},
  volume = {261},
  number = {8},
  pages = {2250--2292},
  doi = {10.1016/j.jfa.2011.06.009},
  abstract = {Let K be an irreducible and reversible Markov kernel on a finite set X. We construct a metric W on the set of probability measures on X and show that with respect to this metric, the law of the continuous time Markov chain evolves as the gradient flow of the entropy. This result is a discrete counterpart of the Wasserstein gradient flow interpretation of the heat flow in Rn by Jordan, Kinderlehrer and Otto (1998). The metric W is similar to, but different from, the L2-Wasserstein metric, and is defined via a discrete variant of the Benamou–Brenier formula.},
  annotation = {MAG ID: 1998824578}
}

@article{janmaas2017EntropicRicciCurvature,
  title = {Entropic {{Ricci Curvature}} for {{Discrete Spaces}}},
  author = {{Jan Maas} and Maas, Jan},
  date = {2017-01-01},
  pages = {159--174},
  doi = {10.1007/978-3-319-58002-9_5},
  abstract = {We give a short overview on a recently developed notion of Ricci curvature for discrete spaces. This notion relies on geodesic convexity properties of the relative entropy along geodesics in the space of probability densities, for a metric which is similar to (but different from) the 2-Wasserstein metric. The theory can be considered as a discrete counterpart to the theory of Ricci curvature for geodesic measure spaces developed by Lott–Sturm–Villani.},
  annotation = {MAG ID: 2761645452}
}

@article{maaten2008VisualizingDataUsing,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {family=Maaten, given=Laurens, prefix=van der, useprefix=false and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
  urldate = {2023-04-24},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {/Users/adjourner/Zotero/storage/WW7K4AEX/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

@article{masahiroikeda2021CoarseRicciCurvature,
  title = {Coarse {{Ricci}} Curvature of Hypergraphs and Its Generalization},
  author = {{Masahiro Ikeda} and Ikeda, Masahiro and {Yu Kitabeppu} and Kitabeppu, Yu and {Yuuki Takai} and Takai, Yuuki},
  date = {2021},
  journaltitle = {arXiv: Metric Geometry},
  abstract = {In the present paper, we introduce a concept of Ricci curvature on hypergraphs for a nonlinear Laplacian. We prove that our definition of the Ricci curvature is a generalization of Lin-Lu-Yau coarse Ricci curvature for graphs to hypergraphs. We also show a lower bound of nonzero eigenvalues of Laplacian, gradient estimate of heat flow, and diameter bound of Bonnet-Myers type for our curvature notion. This research leads to understanding how nonlinearity of Laplacian causes complexity of curvatures.},
  annotation = {MAG ID: 3128155086}
}

@article{mazon2020HeatFlowMetric,
  title = {The Heat Flow on Metric Random Walk Spaces},
  author = {Mazón, José M. and Solera, Marcos and Toledo, Julián},
  date = {2020},
  journaltitle = {Journal of Mathematical Analysis and Applications},
  doi = {10.1016/j.jmaa.2019.123645},
  abstract = {Abstract   In this paper we study the Heat Flow on Metric Random Walk Spaces, which unifies into a broad framework the heat flow on locally finite weighted connected graphs, the heat flow determined by finite Markov chains and some nonlocal evolution problems. We give different characterizations of the ergodicity and prove that a metric random walk space with positive Ollivier-Ricci curvature is ergodic. Furthermore, we prove a Cheeger inequality and, as a consequence, we show that a Poincare inequality holds if, and only if, an isoperimetric inequality holds. We also study the Bakry-Emery curvature-dimension condition and its relation with functional inequalities like the Poincare inequality and the transport-information inequalities.},
  annotation = {MAG ID: 2987475247}
}

@article{michaelschmuckenschlager1998CurvatureNonlocalMarkov,
  title = {Curvature of {{Nonlocal Markov Generators}}},
  author = {{Michael Schmuckenschläger} and Schmuckenschläger, Michael},
  date = {1998-01-01},
  pages = {189--197},
  abstract = {Bakry’s curvature-dimension condition will be extended to certain nonlocal Markov generators. In particular this gives rise to a possible notion of curvature for graphs. 1. Definition of Curvature Let (Ω, μ) be a probability space and L a self-adjoint negative but not necessarily bounded operator on L2(μ) given by Lf(x) := ∫ (f(y)− f(x))K(x, y)μ(dy) (1) where K is a non negative symmetric kernel. Obviously L remains unchanged if we change K on the diagonal. By Pt = e we denote the continuous contraction semigroup on L2(μ) with generator L. We will assume that Pt is ergodic and that there exists an algebra A ⊆ n domL of bounded functions which is a form core of L. Then the Beurling–Deny condition implies that Pt is a symmetric Markov semigroup, i.e., Pt preserves positivity and extends to a continuous contraction semigroup on Lp(μ) for all 1 ≤ p   0. Let us say a a word about the motivation for this definition. Assume L is the Laplacian on a Riemannian manifold, then},
  annotation = {MAG ID: 2245665255}
}

@article{moon2019VisualizingStructureTransitions,
  title = {Visualizing Structure and Transitions in High-Dimensional Biological Data},
  author = {Moon, Kevin R. and family=Dijk, given=David, prefix=van, useprefix=true and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and family=Elzen, given=Antonia, prefix=van den, useprefix=false and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
  date = {2019-12},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {37},
  number = {12},
  pages = {1482--1492},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-019-0336-3},
  url = {https://www.nature.com/articles/s41587-019-0336-3},
  urldate = {2023-04-24},
  abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
  issue = {12},
  langid = {english},
  keywords = {Data mining,Machine learning},
  file = {/Users/adjourner/Zotero/storage/3LMML4XE/Moon et al. - 2019 - Visualizing structure and transitions in high-dime.pdf}
}

@article{ollivier2009RicciCurvatureMarkov,
  title = {Ricci Curvature of {{Markov}} Chains on Metric Spaces},
  author = {Ollivier, Yann},
  date = {2009-02-01},
  journaltitle = {Journal of Functional Analysis},
  volume = {256},
  number = {3},
  pages = {810--864},
  publisher = {Academic Press},
  issn = {0022-1236},
  doi = {10.1016/j.jfa.2008.11.001},
  url = {https://www.sciencedirect.com/science/article/pii/S002212360800493X},
  urldate = {2023-04-21},
  abstract = {We define the coarse Ricci curvature of metric spaces in terms of how much small balls are closer (in Wasserstein transportation distance) than their …},
  langid = {english},
  keywords = {_tablet_modified},
  file = {/Users/adjourner/Zotero/storage/H64QRWX8/Ollivier2009-Ricci curvature of Markov chains on metric spaces.pdf}
}

@inproceedings{saloff-coste2010HeatKernelIts,
  title = {The Heat Kernel and Its Estimates},
  booktitle = {Advanced {{Studies}} in {{Pure Mathematics}}},
  author = {Saloff-Coste, Laurent},
  date = {2010},
  pages = {405--436},
  location = {Kyoto University, Japan},
  doi = {10.2969/aspm/05710405},
  url = {https://projecteuclid.org/euclid.aspm/1543086330},
  urldate = {2023-04-24},
  abstract = {After a short survey of some of the reasons that make the heat kernel an important object of study, we review a number of basic heat kernel estimates. We then describe recent results concerning (a) the heat kernel on certain manifolds with ends, and (b) the heat kernel with the Neumann or Dirichlet boundary condition in inner uniform Euclidean domains.},
  eventtitle = {The {{First Mathematical Society}} of {{Japan}}–{{Seasonal Institute}} — {{Probabilistic Approach}} to {{Geometry}}},
  langid = {english},
  file = {/Users/adjourner/Zotero/storage/VRC9G7RA/Saloff-Coste - The heat kernel and its estimates.pdf}
}

@online{steinerberger2022CurvatureGraphsEquilibrium,
  title = {Curvature on {{Graphs}} via {{Equilibrium Measures}}},
  author = {Steinerberger, Stefan},
  date = {2022-09-05},
  eprint = {2202.01658},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2202.01658},
  url = {http://arxiv.org/abs/2202.01658},
  urldate = {2023-07-10},
  abstract = {We introduce a notion of curvature on finite, combinatorial graphs. It can be easily computed by solving a linear system of equations. We show that graphs with curvature bounded below by \$K{$>$}0\$ have diameter bounded by \$\textbackslash mbox\{diam\}(G) \textbackslash leq 2/K\$ (a Bonnet-Myers theorem), that \$\textbackslash mbox\{diam\}(G) = 2/K\$ implies that \$G\$ has constant curvature (a Cheng theorem) and that there is a spectral gap \$\textbackslash lambda\_1 \textbackslash geq K/(2n)\$ (a Lichnerowicz theorem). It is computed for several families of graphs and often coincides with Ollivier curvature or Lin-Lu-Yau curvature. The von Neumann minimax theorem features prominently in the proofs.},
  pubstate = {preprint},
  keywords = {Mathematics - Combinatorics,Mathematics - Differential Geometry},
  file = {/Users/adjourner/Zotero/storage/K4IHD2LJ/Steinerberger - 2022 - Curvature on Graphs via Equilibrium Measures.pdf;/Users/adjourner/Zotero/storage/JLN2PI8B/2202.html}
}

@article{sturm2006GeometryMetricMeasure,
  title = {On the Geometry of Metric Measure Spaces},
  author = {Sturm, Karl-Theodor},
  date = {2006-07-01},
  journaltitle = {Acta Mathematica},
  shortjournal = {Acta Math},
  volume = {196},
  number = {1},
  pages = {65--131},
  issn = {1871-2509},
  doi = {10.1007/s11511-006-0002-8},
  url = {https://doi.org/10.1007/s11511-006-0002-8},
  urldate = {2023-04-22},
  abstract = {We introduce and analyze lower (Ricci) curvature bounds\$\textbackslash underline\{\{Curv\}\} \{\textbackslash left( \{M,d,m\} \textbackslash right)\}\$\,⩾\,K for metric measure spaces\$\{\textbackslash left( \{M,d,m\} \textbackslash right)\}\$. Our definition is based on convexity properties of the relative entropy\$Ent\{\textbackslash left( \{ \textbackslash cdot \textbackslash left| m \textbackslash right.\} \textbackslash right)\}\$regarded as a function on the L2-Wasserstein space of probability measures on the metric space\$\{\textbackslash left( \{M,d\} \textbackslash right)\}\$. Among others, we show that\$\textbackslash underline\{\{Curv\}\} \{\textbackslash left( \{M,d,m\} \textbackslash right)\}\$\,⩾\,K implies estimates for the volume growth of concentric balls. For Riemannian manifolds,\$\textbackslash underline\{\{Curv\}\} \{\textbackslash left( \{M,d,m\} \textbackslash right)\}\$\,⩾\,K if and only if\$Ric\_\{M\} \{\textbackslash left( \{\textbackslash xi ,\textbackslash xi \} \textbackslash right)\}\$\,⩾\,K\$\{\textbackslash left| \textbackslash xi  \textbackslash right|\}\textasciicircum\{2\} \$for all \$\textbackslash xi  \textbackslash in TM\$.},
  langid = {english},
  keywords = {Dirichlet Form,Heat Kernel,Measure Space,Relative Entropy,Ricci Curvature},
  file = {/Users/adjourner/Zotero/storage/7I4VNYHR/Sturm - 2006 - On the geometry of metric measure spaces.pdf}
}

@article{supanatkamtue2020NoteBonnetMyersType,
  title = {A Note on a {{Bonnet-Myers}} Type Diameter Bound for Graphs with Positive Entropic {{Ricci}} Curvature},
  author = {{Supanat Kamtue} and {Supanat Kamtue} and Kamtue, Supanat},
  date = {2020-03-02},
  journaltitle = {arXiv: Probability},
  abstract = {An equivalent definition of entropic Ricci curvature on discrete spaces was given in terms of the global gradient estimate. With a particular choice of the density function \$\textbackslash rho\$, we obtain a localized gradient estimate, which in turns allow us to derive a Bonnet-Myers type diameter bound for graphs with positive entropic Ricci curvature. However, the case of the hypercubes indicates that the bound may be not optimal (where \$\textbackslash theta\$ is chosen to be logarithmic mean by default). If \$\textbackslash theta\$ is arithmetic mean, the Bakry-Emery criterion can be recovered and the diameter bound is optimal as it can be attained by the hypercubes.},
  annotation = {MAG ID: 3010568234}
}

@article{t.-h.hubertchan2018SpectralPropertiesHypergraph,
  title = {Spectral {{Properties}} of {{Hypergraph Laplacian}} and {{Approximation Algorithms}}},
  author = {{T.-H. Hubert Chan} and Chan, T.-H. Hubert and {Anand Louis} and Louis, Anand and {Zhihao Gavin Tang} and Tang, Zhihao Gavin and {Zhihao Gavin Tang} and {Chenzi Zhang} and Zhang, Chenzi},
  date = {2018-03-05},
  journaltitle = {Journal of the ACM},
  volume = {65},
  number = {3},
  pages = {15},
  doi = {10.1145/3178123},
  abstract = {The celebrated Cheeger’s Inequality (Alon and Milman 1985; Alon 1986) establishes a bound on the edge expansion of a graph via its spectrum. This inequality is central to a rich spectral theory of graphs, based on studying the eigenvalues and eigenvectors of the adjacency matrix (and other related matrices) of graphs. It has remained open to define a suitable spectral model for hypergraphs whose spectra can be used to estimate various combinatorial properties of the hypergraph. In this article, we introduce a new hypergraph Laplacian operator generalizing the Laplacian matrix of graphs. In particular, the operator is induced by a diffusion process on the hypergraph, such that within each hyperedge, measure flows from vertices having maximum weighted measure to those having minimum. Since the operator is nonlinear, we have to exploit other properties of the diffusion process to recover the Cheeger’s Inequality that relates hyperedge expansion with the “second eigenvalue” of the resulting Laplacian. However, we show that higher-order spectral properties cannot hold in general using the current framework. Since higher-order spectral properties do not hold for the Laplacian operator, we instead use the concept of procedural minimizers to consider higher-order Cheeger-like inequalities. For any k ∈ N, we give a polynomial-time algorithm to compute an O(log r)-approximation to the kth procedural minimizer, where r is the maximum cardinality of a hyperedge. We show that this approximation factor is optimal under the SSE hypothesis (introduced by Raghavendra and Steurer (2010)) for constant values of k. Moreover, using the factor-preserving reduction from vertex expansion in graphs to hypergraph expansion, we show that all our results for hypergraphs extend to vertex expansion in graphs.},
  annotation = {MAG ID: 2963055664}
}

@article{tomoyaakamatsu2021NewTransportDistance,
  title = {A New Transport Distance and Its Associated {{Ricci}} Curvature of Hypergraphs},
  author = {{Tomoya Akamatsu} and Akamatsu, Tomoya},
  date = {2021-05-16},
  journaltitle = {arXiv: Metric Geometry},
  abstract = {The coarse Ricci curvature of graphs introduced by Ollivier as well as its modification by Lin-Lu-Yau have been studied from various aspects. In this paper, we propose a new transport distance appropriate for hypergraphs and study a generalization of Lin-Lu-Yau type curvature of hypergraphs. As an application, we derive a Bonnet-Myers type estimate for hypergraphs under a lower Ricci curvature bound associated with our transport distance. We remark that our transport distance is new even for graphs and worthy of further study.},
  annotation = {MAG ID: 3161411498}
}

@inproceedings{tong2021DatadrivenLearningGeometric,
  title = {Data-Driven Learning of Geometric Scattering Modules for Gnns},
  booktitle = {2021 {{IEEE}} 31st {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Tong, Alexander and Wenkel, Frederick and Macdonald, Kincaid and Krishnaswamy, Smita and Wolf, Guy},
  date = {2021},
  pages = {1--6},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9596169/},
  urldate = {2023-12-01},
  file = {/Users/adjourner/Zotero/storage/ZZH42NVS/PMC10026018.html}
}

@inproceedings{tong2021DiffusionEarthMovera,
  title = {Diffusion {{Earth Mover}}’s {{Distance}} and {{Distribution Embeddings}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Tong, Alexander Y. and Huguet, Guillaume and Natik, Amine and Macdonald, Kincaid and Kuchroo, Manik and Coifman, Ronald and Wolf, Guy and Krishnaswamy, Smita},
  date = {2021-07-01},
  pages = {10336--10346},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/tong21a.html},
  urldate = {2022-11-13},
  abstract = {We propose a new fast method of measuring distances between large numbers of related high dimensional datasets called the Diffusion Earth Mover’s Distance (EMD). We model the datasets as distributions supported on common data graph that is derived from the affinity matrix computed on the combined data. In such cases where the graph is a discretization of an underlying Riemannian closed manifold, we prove that Diffusion EMD is topologically equivalent to the standard EMD with a geodesic ground distance. Diffusion EMD can be computed in \{Õ\}(n) time and is more accurate than similarly fast algorithms such as tree-based EMDs. We also show Diffusion EMD is fully differentiable, making it amenable to future uses in gradient-descent frameworks such as deep neural networks. Finally, we demonstrate an application of Diffusion EMD to single cell data collected from 210 COVID-19 patient samples at Yale New Haven Hospital. Here, Diffusion EMD can derive distances between patients on the manifold of cells at least two orders of magnitude faster than equally accurate methods. This distance matrix between patients can be embedded into a higher level patient manifold which uncovers structure and heterogeneity in patients. More generally, Diffusion EMD is applicable to all datasets that are massively collected in parallel in many medical and biological systems.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adjourner/Zotero/storage/VVY4LWZE/Tong et al. - 2021 - Diffusion Earth Mover’s Distance and Distribution .pdf;/Users/adjourner/Zotero/storage/XQSJD9S2/Tong et al. - 2021 - Diffusion Earth Mover’s Distance and Distribution .pdf}
}
