
@report{hickok_intrinsic_2023,
	title = {An Intrinsic Approach to Scalar-Curvature Estimation for Point Clouds},
	url = {http://arxiv.org/abs/2308.02615},
	abstract = {We introduce an intrinsic estimator for the scalar curvature of a data set presented as a finite metric space. Our estimator depends only on the metric structure of the data and not on an embedding in \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$. We show that the estimator is consistent in the sense that for points sampled from a probability measure on a compact Riemannian manifold, the estimator converges to the scalar curvature as the number of points increases. To justify its use in applications, we show that the estimator is stable with respect to perturbations of the metric structure, e.g., noise in the sample or error estimating the intrinsic metric. We validate our estimator experimentally on synthetic data that is sampled from manifolds with specified curvature.},
	number = {{arXiv}:2308.02615},
	institution = {{arXiv}},
	author = {Hickok, Abigail and Blumberg, Andrew J.},
	urldate = {2023-09-25},
	date = {2023-08-04},
	doi = {10.48550/arXiv.2308.02615},
	eprinttype = {arxiv},
	eprint = {2308.02615 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Computational Geometry, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/boreas/Zotero/storage/C2LKU46I/2308.html:text/html;Hickock2023 Intrinsic Curvature.pdf:/Users/boreas/Zotero/storage/5IK4A7EY/Hickock2023 Intrinsic Curvature.pdf:application/pdf},
}

@article{gilmer_loss_2021,
	title = {A Loss Curvature Perspective on Training Instability in Deep Learning},
	url = {http://arxiv.org/abs/2110.04369},
	abstract = {In this work, we study the evolution of the loss Hessian across many classification tasks in order to understand the effect the curvature of the loss has on the training dynamics. Whereas prior work has focused on how different learning rates affect the loss Hessian observed during training, we also analyze the effects of model initialization, architectural choices, and common training heuristics such as gradient clipping and learning rate warmup. Our results demonstrate that successful model and hyperparameter choices allow the early optimization trajectory to either avoid -- or navigate out of -- regions of high curvature and into flatter regions that tolerate a higher learning rate. Our results suggest a unifying perspective on how disparate mitigation strategies for training instability ultimately address the same underlying failure mode of neural network optimization, namely poor conditioning. Inspired by the conditioning perspective, we show that learning rate warmup can improve training stability just as much as batch normalization, layer normalization, {MetaInit}, {GradInit}, and Fixup initialization.},
	journaltitle = {{arXiv}:2110.04369 [cs]},
	author = {Gilmer, Justin and Ghorbani, Behrooz and Garg, Ankush and Kudugunta, Sneha and Neyshabur, Behnam and Cardoze, David and Dahl, George and Nado, Zachary and Firat, Orhan},
	urldate = {2021-10-19},
	date = {2021-10-08},
	eprinttype = {arxiv},
	eprint = {2110.04369},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/boreas/Zotero/storage/LMRJI5FU/2110.html:text/html;Gilmer et al_2021_A Loss Curvature Perspective on Training Instability in Deep Learning.pdf:/Users/boreas/Zotero/storage/B329ZULS/Gilmer et al_2021_A Loss Curvature Perspective on Training Instability in Deep Learning.pdf:application/pdf},
}

@article{bhaskar_diffusion_2022,
	title = {Diffusion curvature for estimating local curvature in high dimensional data},
	volume = {35},
	rights = {All rights reserved},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/88438dc62fc5c8777e2b5f1b4f6d37a2-Abstract-Conference.html},
	pages = {21738--21749},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Bhaskar, Dhananjay and {MacDonald}, Kincaid and Fasina, Oluwadamilola and Thomas, Dawson and Rieck, Bastian and Adelstein, Ian and Krishnaswamy, Smita},
	urldate = {2023-12-01},
	date = {2022},
	file = {Bhaskar2022-Diffusion curvature for estimating local curvature in high dimensional data.pdf:/Users/adjourner/Pumberton/Library/Bhaskar2022-Diffusion curvature for estimating local curvature in high dimensional data.pdf:application/pdf},
}

@article{supanat_kamtue_note_2020,
	title = {A note on a Bonnet-Myers type diameter bound for graphs with positive entropic Ricci curvature},
	abstract = {An equivalent definition of entropic Ricci curvature on discrete spaces was given in terms of the global gradient estimate. With a particular choice of the density function \${\textbackslash}rho\$, we obtain a localized gradient estimate, which in turns allow us to derive a Bonnet-Myers type diameter bound for graphs with positive entropic Ricci curvature. However, the case of the hypercubes indicates that the bound may be not optimal (where \${\textbackslash}theta\$ is chosen to be logarithmic mean by default). If \${\textbackslash}theta\$ is arithmetic mean, the Bakry-Emery criterion can be recovered and the diameter bound is optimal as it can be attained by the hypercubes.},
	journaltitle = {{arXiv}: Probability},
	author = {{Supanat Kamtue} and {Supanat Kamtue} and Kamtue, Supanat},
	date = {2020-03-02},
	note = {{MAG} {ID}: 3010568234},
}

@article{florentin_munch_non-negative_2019,
	title = {Non-negative Ollivier curvature on graphs, reverse Poincaré inequality, Buser inequality, Liouville property, Harnack inequality and eigenvalue estimates},
	abstract = {We prove that for combinatorial graphs with non-negative Ollivier curvature, one has {\textbackslash}[ {\textbackslash}{\textbar}P\_t {\textbackslash}mu - P\_t {\textbackslash}nu{\textbackslash}{\textbar}\_1 {\textbackslash}leq {\textbackslash}frac\{W\_1({\textbackslash}mu,{\textbackslash}nu)\}\{{\textbackslash}sqrt\{t\}\} {\textbackslash}] for all probability measures \${\textbackslash}mu,{\textbackslash}nu\$ where \$P\_t\$ is the heat semigroup and \$W\_1\$ is the \${\textbackslash}ell\_1\$-Wasserstein distance. This turns out to be an equivalent formulation of a version of reverse Poincare inequality. Furthermore, this estimate allows us to prove Buser inequality, Liouville property and the the eigenvalue estimate \${\textbackslash}lambda\_1 {\textbackslash}geq {\textbackslash}log(2)/{\textbackslash}operatorname\{diam\}{\textasciicircum}2\$.},
	journaltitle = {{arXiv}: Differential Geometry},
	author = {{Florentin Münch} and Münch, Florentin},
	date = {2019-07-31},
	note = {{MAG} {ID}: 2965452863},
}

@article{mazon_heat_2020,
	title = {the heat flow on metric random walk spaces},
	doi = {10.1016/j.jmaa.2019.123645},
	abstract = {Abstract   In this paper we study the Heat Flow on Metric Random Walk Spaces, which unifies into a broad framework the heat flow on locally finite weighted connected graphs, the heat flow determined by finite Markov chains and some nonlocal evolution problems. We give different characterizations of the ergodicity and prove that a metric random walk space with positive Ollivier-Ricci curvature is ergodic. Furthermore, we prove a Cheeger inequality and, as a consequence, we show that a Poincare inequality holds if, and only if, an isoperimetric inequality holds. We also study the Bakry-Emery curvature-dimension condition and its relation with functional inequalities like the Poincare inequality and the transport-information inequalities.},
	journaltitle = {Journal of Mathematical Analysis and Applications},
	author = {Mazón, José M. and Solera, Marcos and Toledo, Julián},
	date = {2020},
	doi = {10.1016/j.jmaa.2019.123645},
	note = {{MAG} {ID}: 2987475247},
}

@article{masahiro_ikeda_coarse_2021,
	title = {Coarse Ricci curvature of hypergraphs and its generalization},
	abstract = {In the present paper, we introduce a concept of Ricci curvature on hypergraphs for a nonlinear Laplacian. We prove that our definition of the Ricci curvature is a generalization of Lin-Lu-Yau coarse Ricci curvature for graphs to hypergraphs. We also show a lower bound of nonzero eigenvalues of Laplacian, gradient estimate of heat flow, and diameter bound of Bonnet-Myers type for our curvature notion. This research leads to understanding how nonlinearity of Laplacian causes complexity of curvatures.},
	journaltitle = {{arXiv}: Metric Geometry},
	author = {{Masahiro Ikeda} and Ikeda, Masahiro and {Yu Kitabeppu} and Kitabeppu, Yu and {Yuuki Takai} and Takai, Yuuki},
	date = {2021},
	note = {{MAG} {ID}: 3128155086},
}

@article{jan_maas_entropic_2017,
	title = {Entropic Ricci Curvature for Discrete Spaces},
	doi = {10.1007/978-3-319-58002-9_5},
	abstract = {We give a short overview on a recently developed notion of Ricci curvature for discrete spaces. This notion relies on geodesic convexity properties of the relative entropy along geodesics in the space of probability densities, for a metric which is similar to (but different from) the 2-Wasserstein metric. The theory can be considered as a discrete counterpart to the theory of Ricci curvature for geodesic measure spaces developed by Lott–Sturm–Villani.},
	pages = {159--174},
	author = {{Jan Maas} and Maas, Jan},
	date = {2017-01-01},
	doi = {10.1007/978-3-319-58002-9_5},
	note = {{MAG} {ID}: 2761645452},
}

@article{michael_schmuckenschlager_curvature_1998,
	title = {Curvature of Nonlocal Markov Generators},
	abstract = {Bakry’s curvature-dimension condition will be extended to certain nonlocal Markov generators. In particular this gives rise to a possible notion of curvature for graphs. 1. Definition of Curvature Let (Ω, μ) be a probability space and L a self-adjoint negative but not necessarily bounded operator on L2(μ) given by Lf(x) := ∫ (f(y)− f(x))K(x, y)μ(dy) (1) where K is a non negative symmetric kernel. Obviously L remains unchanged if we change K on the diagonal. By Pt = e we denote the continuous contraction semigroup on L2(μ) with generator L. We will assume that Pt is ergodic and that there exists an algebra A ⊆ n {domL} of bounded functions which is a form core of L. Then the Beurling–Deny condition implies that Pt is a symmetric Markov semigroup, i.e., Pt preserves positivity and extends to a continuous contraction semigroup on Lp(μ) for all 1 ≤ p   0. Let us say a a word about the motivation for this definition. Assume L is the Laplacian on a Riemannian manifold, then},
	pages = {189--197},
	author = {{Michael Schmuckenschläger} and Schmuckenschläger, Michael},
	date = {1998-01-01},
	note = {{MAG} {ID}: 2245665255},
}

@article{t-h_hubert_chan_spectral_2018,
	title = {Spectral Properties of Hypergraph Laplacian and Approximation Algorithms},
	volume = {65},
	doi = {10.1145/3178123},
	abstract = {The celebrated Cheeger’s Inequality (Alon and Milman 1985; Alon 1986) establishes a bound on the edge expansion of a graph via its spectrum. This inequality is central to a rich spectral theory of graphs, based on studying the eigenvalues and eigenvectors of the adjacency matrix (and other related matrices) of graphs. It has remained open to define a suitable spectral model for hypergraphs whose spectra can be used to estimate various combinatorial properties of the hypergraph.

In this article, we introduce a new hypergraph Laplacian operator generalizing the Laplacian matrix of graphs. In particular, the operator is induced by a diffusion process on the hypergraph, such that within each hyperedge, measure flows from vertices having maximum weighted measure to those having minimum. Since the operator is nonlinear, we have to exploit other properties of the diffusion process to recover the Cheeger’s Inequality that relates hyperedge expansion with the “second eigenvalue” of the resulting Laplacian. However, we show that higher-order spectral properties cannot hold in general using the current framework.

Since higher-order spectral properties do not hold for the Laplacian operator, we instead use the concept of procedural minimizers to consider higher-order Cheeger-like inequalities. For any k ∈ N, we give a polynomial-time algorithm to compute an O(log r)-approximation to the kth procedural minimizer, where r is the maximum cardinality of a hyperedge. We show that this approximation factor is optimal under the {SSE} hypothesis (introduced by Raghavendra and Steurer (2010)) for constant values of k.

Moreover, using the factor-preserving reduction from vertex expansion in graphs to hypergraph expansion, we show that all our results for hypergraphs extend to vertex expansion in graphs.},
	pages = {15},
	number = {3},
	journaltitle = {Journal of the {ACM}},
	author = {{T.-H. Hubert Chan} and Chan, T.-H. Hubert and {Anand Louis} and Louis, Anand and {Zhihao Gavin Tang} and Tang, Zhihao Gavin and {Zhihao Gavin Tang} and {Chenzi Zhang} and Zhang, Chenzi},
	date = {2018-03-05},
	doi = {10.1145/3178123},
	note = {{MAG} {ID}: 2963055664},
}

@article{cayeux_curvature_2018,
	title = {Curvature of Hypergraphs via Multi-Marginal Optimal Transport},
	doi = {10.1109/cdc.2018.8619706},
	abstract = {We introduce a novel definition of curvature for hypergraphs, a natural generalization of graphs, by introducing a multi-marginal optimal transport problem for a naturally defined random walk on the hypergraph. This curvature, termed coarse scalar curvature, extends a recent definition of Ricci curvature for Markov chains on metric spaces by Ollivier [Journal of Functional Analysis 256 (2009) 810–864], and is related to the scalar curvature when the hypergraph arises naturally from a Riemannian manifold. We investigate basic theoretical properties of the coarse scalar curvature and obtain several bounds. Empirical experiments demonstrate that coarse scalar curvatures detects “bridges” across connected components in hypergraphs, akin to the behavior of coarse Ricci curvatures on graphs.},
	pages = {1180--1185},
	author = {Cayeux, Eric and {Shahab Asoodeh} and {Shahab Asoodeh} and Asoodeh, Shahab and {Tingran Gao} and Gao, Tingran and {James A. Evans} and Evans, James A.},
	date = {2018-12-01},
	doi = {10.1109/cdc.2018.8619706},
	note = {{MAG} {ID}: 2962985127},
}

@article{jan_maas_gradient_2011,
	title = {Gradient flows of the entropy for finite Markov chains},
	volume = {261},
	doi = {10.1016/j.jfa.2011.06.009},
	abstract = {Let K be an irreducible and reversible Markov kernel on a finite set X. We construct a metric W on the set of probability measures on X and show that with respect to this metric, the law of the continuous time Markov chain evolves as the gradient flow of the entropy. This result is a discrete counterpart of the Wasserstein gradient flow interpretation of the heat flow in Rn by Jordan, Kinderlehrer and Otto (1998). The metric W is similar to, but different from, the L2-Wasserstein metric, and is defined via a discrete variant of the Benamou–Brenier formula.},
	pages = {2250--2292},
	number = {8},
	journaltitle = {Journal of Functional Analysis},
	author = {{Jan Maas} and Maas, Jan},
	date = {2011-10-15},
	doi = {10.1016/j.jfa.2011.06.009},
	note = {{MAG} {ID}: 1998824578},
}

@article{tomoya_akamatsu_new_2021,
	title = {A new transport distance and its associated Ricci curvature of hypergraphs},
	abstract = {The coarse Ricci curvature of graphs introduced by Ollivier as well as its modification by Lin-Lu-Yau have been studied from various aspects. In this paper, we propose a new transport distance appropriate for hypergraphs and study a generalization of Lin-Lu-Yau type curvature of hypergraphs. As an application, we derive a Bonnet-Myers type estimate for hypergraphs under a lower Ricci curvature bound associated with our transport distance. We remark that our transport distance is new even for graphs and worthy of further study.},
	journaltitle = {{arXiv}: Metric Geometry},
	author = {{Tomoya Akamatsu} and Akamatsu, Tomoya},
	date = {2021-05-16},
	note = {{MAG} {ID}: 3161411498},
}

@article{benchen_conformal_2008,
	title = {Conformal Flattening by Curvature Prescription and Metric Scaling},
	volume = {27},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2008.01142.x},
	doi = {10.1111/j.1467-8659.2008.01142.x},
	abstract = {Abstract
            We present an efficient method to conformally parameterize 3D mesh data sets to the plane. The idea behind our method is to concentrate all the 3D curvature at a small number of select mesh vertices, called cone singularities, and then cut the mesh through those singular vertices to obtain disk topology. The singular vertices are chosen automatically. As opposed to most previous methods, our flattening process involves only the solution of linear systems of Poisson equations, thus is very efficient. Our method is shown to be faster than existing methods, yet generates parameterizations having comparable quasi‐conformal distortion.},
	pages = {449--458},
	number = {2},
	journaltitle = {Computer Graphics Forum},
	shortjournal = {Computer Graphics Forum},
	author = {Ben‐Chen, Mirela and Gotsman, Craig and Bunin, Guy},
	urldate = {2024-02-06},
	date = {2008-04},
	langid = {english},
	file = {Ben‐Chen2008-Conformal Flattening by Curvature Prescription and Metric Scaling.pdf:/Users/boreas/Zotero/storage/HUHV3VLY/Ben‐Chen2008-Conformal Flattening by Curvature Prescription and Metric Scaling.pdf:application/pdf},
}
