# 3d1 Effects of Graph Construction on Negative Curvature

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

In 3d, we noticed that the knn value has a suspiciously large effect on
the signs of curvature measured. With $k=5$, DC has trouble picking up
any negative curvature; with $k=15$, it struggles to identify anything
as positive.

Here we’ll test the method by probing its ability to separate saddles
and spheres in various dimensions.

## Executive Summary:

1.  Both the fixed and adaptive kernel significantly outperform
    Graphtools’ graph construction on this toy data.
2.  The median heuristic is not very good at choosing the right sigma.
3.  The adaptive gaussian kernel with $k=10$ chooses the right sigma
    well, producing tightly clustered distinctions between the saddles
    and spheres.
4.  Disabling the anisotropic density normalization *helps* performance
    in high dimensions, enabling the adaptive kernel + DC to detect
    negative curvature in higher-dimensional saddles.

# With Graphtools’ Graphs

``` python
import graphtools
from diffusion_curvature.core import DiffusionCurvature
from diffusion_curvature.datasets import rejection_sample_from_saddle, sphere
from functools import partial

def graphtools_graph_from_data(X, knn = 15):
    return graphtools.Graph(X, anisotropy=1, knn=knn, decay=None).to_pygsp()
    
def get_dc_of_saddles_and_spheres(
    dim = 3,
    num_samplings = 100,
    graph_former = graphtools_graph_from_data,
    return_data = False,
):
    samplings = [2000]*num_samplings
    ks_dc_saddles = []
    ks_dc_spheres = []
    X_saddles_sampled = []
    X_spheres_sampled = []
    
    for n_points in tqdm(samplings):
        X_saddle, k_saddle = rejection_sample_from_saddle(n_points, dim)
        X_saddles_sampled.append(X_saddle)
        X_sphere, k_sphere = sphere(n_points, d=dim)
        X_spheres_sampled.append(X_sphere)
        # Compute Diffusion Curvature on Sphere
        G = graph_former(X_sphere)
        DC = DiffusionCurvature(
            laziness_method="Entropic",
            flattening_method="Fixed",
            comparison_method="Subtraction",
            graph_former = graph_former,
            points_per_cluster=None, # construct separate comparison spaces around each point
            comparison_space_size_factor=1
        )
        ks = DC.curvature(G, t=25, dim=dim, idx=0)
        ks_dc_spheres.append(ks)
        # Compute Diffusion Curvature on Saddle
        G = graph_former(X_saddle)
        DC = DiffusionCurvature(
            laziness_method="Entropic",
            flattening_method="Fixed",
            comparison_method="Subtraction",
            graph_former = graph_former,
            points_per_cluster=None, # construct separate comparison spaces around each point
            comparison_space_size_factor=1
        )
        ks = DC.curvature(G, t=25, dim=dim, idx=0)
        ks_dc_saddles.append(ks)
    
    # plot a histogram of the diffusion curvatures
    plt.hist(ks_dc_saddles, bins=50, color='orange', label = 'Saddles')
    plt.hist(ks_dc_spheres, bins=50, color='green', label = 'Spheres')
    plt.legend()
    plt.xlabel("Diffusion Curvature")
    plt.title(f"In dimension {dim}")
    # plt.xlabel('')
    # plt.ylabel('Frequency')
    if return_data: return ks_dc_saddles, ks_dc_spheres
```

``` python
get_dc_of_saddles_and_spheres(
    graph_former = partial(graphtools_graph_from_data, knn=15)
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

    TypeError: sparse array length is ambiguous; use getnnz() or shape[0]

``` python
get_dc_of_saddles_and_spheres(
    graph_former = partial(graphtools_graph_from_data, knn=10)
)
```

``` python
get_dc_of_saddles_and_spheres(
    graph_former = partial(graphtools_graph_from_data, knn=5)
)
```

``` python
get_dc_of_saddles_and_spheres(
    graph_former = partial(graphtools_graph_from_data, knn=15),
    dim = 2
)
```

``` python
get_dc_of_saddles_and_spheres(
    graph_former = partial(graphtools_graph_from_data, knn=10),
    dim = 2
)
```

``` python
get_dc_of_saddles_and_spheres(
    graph_former = partial(graphtools_graph_from_data, knn=5),
    dim = 2
)
```

# With a Plain Gaussian Kernel

We observe that the variance between samplings is drastically reduced
when using a plain gaussian kernel.

``` python
from diffusion_curvature.kernels import gaussian_kernel
from dataclasses import dataclass

@dataclass
class SimpleGraph:
    W: np.ndarray
    
def graph_from_gaussian_kernel(X, sigma = 0, alpha = 0):
    W = gaussian_kernel(
        X,
        kernel_type='fixed',
        sigma = sigma, # use median heuristic
        anisotropic_density_normalization = alpha,
    )
    G = SimpleGraph(W = W)
    return G
```

``` python
X, ks = sphere(1000)
G = graph_from_gaussian_kernel(X,sigma=0.7)
# assert np.allclose(G.W,gaussian_kernel(X, kernel_type='fixed',sigma=0, anisotropic_density_normalization=1), atol = 1e-9)
```

``` python
G.W
```

    array([[0.56991754, 0.29680986, 0.04288912, ..., 0.2351118 , 0.11747229,
            0.02305512],
           [0.29680986, 0.56991754, 0.04735469, ..., 0.07031777, 0.41685696,
            0.07559061],
           [0.04288912, 0.04735469, 0.56991754, ..., 0.01755797, 0.09412042,
            0.02866372],
           ...,
           [0.2351118 , 0.07031777, 0.01755797, ..., 0.56991754, 0.02442851,
            0.04378498],
           [0.11747229, 0.41685696, 0.09412042, ..., 0.02442851, 0.56991754,
            0.12120653],
           [0.02305512, 0.07559061, 0.02866372, ..., 0.04378498, 0.12120653,
            0.56991754]])

``` python
D = np.diag(1/np.sum(G.W, axis=1))
```

``` python
np.max(G.W)
```

    0.5699175434306182

``` python
from sklearn.metrics import pairwise_distances
from diffusion_curvature.kernels import median_heuristic
```

``` python
D = pairwise_distances(X)
median_heuristic(D)
```

    0.9991981282990747

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = graph_from_gaussian_kernel,
    dim = 2,
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-16-output-2.png)

``` python
plt.hist(kspheres)
```

    (array([ 3.,  4.,  8., 10., 20., 11., 19.,  9.,  9.,  7.]),
     array([-0.02364445, -0.02324238, -0.02284031, -0.02243824, -0.02203617,
            -0.0216341 , -0.02123203, -0.02082996, -0.02042789, -0.02002583,
            -0.01962376]),
     <BarContainer object of 10 artists>)

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-17-output-2.png)

``` python
plt.hist(ksaddles)
```

    (array([ 2.,  2.,  9., 10., 11., 17., 26., 15.,  2.,  6.]),
     array([0.00396395, 0.00460873, 0.00525351, 0.00589828, 0.00654306,
            0.00718784, 0.00783262, 0.0084774 , 0.00912218, 0.00976696,
            0.01041174]),
     <BarContainer object of 10 artists>)

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-18-output-2.png)

Aha! Turns out 0.1 is the correct sigma value for this problem.
Evidently the median heuristic is terrible for this use case.

Interestingly, with the right sigma parameter, the plain gaussian kernel
separates spheres and saddles far more effectively than graphtools.

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_gaussian_kernel, sigma=0.1, alpha=0),
    dim = 2, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-19-output-2.png)

``` python
plt.hist(kspheres)
```

    (array([ 8.,  8., 19., 24., 11., 12.,  8.,  5.,  4.,  1.]),
     array([1.01980639, 1.05976343, 1.09972048, 1.13967752, 1.17963457,
            1.21959162, 1.25954866, 1.29950571, 1.33946276, 1.3794198 ,
            1.41937685]),
     <BarContainer object of 10 artists>)

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-20-output-2.png)

What happens with anisotropy now that we have the right sigma? I expect
it to decrease the variance. It does, somewhat - but it also distorts
the numbers. It would seem that negative curvature is somehow adversely
affected by complete anisotropic normalization.

``` python
plt.hist(ksaddles)
```

    (array([ 6.,  7., 17., 21., 23., 16.,  7.,  2.,  0.,  1.]),
     array([0.21909618, 0.24295282, 0.26680946, 0.2906661 , 0.31452274,
            0.33837938, 0.36223602, 0.38609266, 0.4099493 , 0.43380594,
            0.45766258]),
     <BarContainer object of 10 artists>)

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-21-output-2.png)

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_gaussian_kernel, sigma=0.1, alpha=1),
    dim = 2, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-22-output-2.png)

``` python
plt.hist(kspheres)
```

    (array([ 4.,  8., 13., 22., 21., 15.,  9.,  4.,  2.,  2.]),
     array([0.95982742, 0.9864279 , 1.01302838, 1.03962874, 1.06622922,
            1.0928297 , 1.11943018, 1.14603066, 1.17263103, 1.19923151,
            1.22583199]),
     <BarContainer object of 10 artists>)

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-23-output-2.png)

``` python
plt.hist(ksaddles)
```

    (array([ 1.,  1.,  9., 16., 19., 20., 11., 11., 11.,  1.]),
     array([0.00093365, 0.01487017, 0.02880669, 0.04274321, 0.05667973,
            0.07061625, 0.08455276, 0.09848928, 0.1124258 , 0.12636232,
            0.14029884]),
     <BarContainer object of 10 artists>)

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-24-output-2.png)

# The Adaptive Kernel

I thought this was what graphtools was doing; evidently, I was woefully
wrong. There’s far less variance, and far more accurate readings here
than with graphtools.

``` python
from diffusion_curvature.kernels import gaussian_kernel
from dataclasses import dataclass
```

``` python
def graph_from_adaptive_gaussian_kernel(X, k=10, alpha = 1):
    W = gaussian_kernel(
        X,
        kernel_type='adaptive',
        k = k,
        anisotropic_density_normalization = alpha,
    )
    G = SimpleGraph(W = W)
    return G
```

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=10, alpha=1),
    dim = 2, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-27-output-2.png)

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=5, alpha=1),
    dim = 2, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-28-output-2.png)

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=10, alpha=1),
    dim = 3, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-29-output-2.png)

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=5, alpha=1),
    dim = 3, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-30-output-2.png)

So it appears that my adaptive kernel function does a much better job of
distinguishing between positive and negative. However, there is still
some slippage in high dimensions. Where saddles are seen as positively
curved. I suspect this relates to the problem of holes in the data.

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=10, alpha=0),
    dim = 3, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-31-output-2.png)

How bizarre, removing the anisotropic density correction, restores some
of the detection of negative curvature. Why could this be? Perhaps it’s
because positive and negatively curved areas actually have different
densities. By normalizing this, we’re forcing them to look more alike.

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=10, alpha=0),
    dim = 4, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-32-output-2.png)

``` python
ksaddles, kspheres = get_dc_of_saddles_and_spheres(
    graph_former = partial(
        graph_from_adaptive_gaussian_kernel, k=10, alpha=0),
    dim = 5, 
    return_data = True
)
```

      0%|          | 0/100 [00:00<?, ?it/s]

![](3d1%20Effects%20of%20Graph%20Construction%20on%20Negative%20Curvature%20Detection_files/figure-commonmark/cell-33-output-2.png)
