<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.358">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>diffusion_curvature - MIOFlow for Neural Flattening</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="diffusion_curvature - MIOFlow for Neural Flattening">
<meta property="og:description" content="Fast, pointwise graph curvature">
<meta property="og:site_name" content="diffusion_curvature">
<meta name="twitter:title" content="diffusion_curvature - MIOFlow for Neural Flattening">
<meta name="twitter:description" content="Fast, pointwise graph curvature">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">diffusion_curvature</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/professorwug/diffusion_curvature"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../library/Comparison Space Construction.html">library</a></li><li class="breadcrumb-item"><a href="../library/MIOFlow for Neural Flattening.html">MIOFlow for Neural Flattening</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">documentation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../documentation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diffusion Curvature</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">experiments</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3a Visual Battery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3a A Visual Battery of Benchmarks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3a1 DC of Battery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3a1 Computing the diffusion curvature of the battery</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3a1a DC of LowD High Sample Battery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3a1a Computing the diffusion curvature of the battery</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3a2 Hickok Curvature of Battery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3a2 Hickok Curvature of Battery.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3b Hickok Comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3b Hickok Comparison.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3c Sampling Experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3c Sampling Experiments on Diffusion Curvature</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3d Detecting Negative Curvature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3d Detecting Negative Curvature</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3d1 Effects of Graph Construction on Negative Curvature Detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3d1 Effects of Graph Construction on Negative Curvature</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3e Curvature with (Neural) Flattening.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3e Curvature Via (Neural) Flattening</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3e1 Flattening with Diffusion Models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3e1 Flattening with Diffusion Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/3e2 Neural Flattening with Gromov Wasserstein.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3e2 Neural Flattening with Gromov Wasserstein</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Curvature Filtrations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Curvature Filtrations.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Curvature by Quadratic Fitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scalar Curvature by Volume Comparison</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Diffusion Laziness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Measurements of Diffusion Laziness</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Examples</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Explorations in Lazy-first diffusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Explorations in Lazy-first Diffusion Curvature</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Loss Landscapes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Curvature of Loss Landscapes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Neumann Heat Kernel via Chebyshev Approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Analytic estimation of the heat kernel</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Sampling Semifinals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Flattening Semifinals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/Wasserstein Laziness of Diffusion with HeatGeo Distances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Diffusion Curvature with Wasserstein Laziness</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/continuous_normalizing_flows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mapping the Data to a Flat Space with Continuous Normalizing Flows</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">library</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Comparison Space Construction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comparison Space Construction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Construct Battery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3a Construct Battery</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Core PyKeops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core (PyKeops)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Core.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Implementation (PyGSP + JAX)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Curvature Enhanced Spectral Clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1d Curvature Clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Diffusion Entropy for Optimal t.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diffusion Entropy for Optimal t.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Diffusion Laziness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diffusion Laziness Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Diffusion Ricci Curvature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diffusion Ricci Curvature</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Creation Utils</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Heat Diffusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Heat Diffusion</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Kernels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Kernels</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/MIOFlow for Neural Flattening.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">MIOFlow for Neural Flattening</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Manifold Distances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Manifold Distances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Mean Flat Entropies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1c1 Mean Entropy of Uniform Flat Spaces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Radial Flattening Autoencoder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Radial Flattening Autoencoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Random Surfaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random Surfaces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Sampling Distance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sampling Distance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Similar Sampling Benchmarker.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Similar Sampling Benchmarker.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Utils</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../library/Volume Estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Volume Estimation with Heat Diffusion</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#implementation" id="toc-implementation" class="nav-link active" data-scroll-target="#implementation">Implementation</a>
  <ul>
  <li><a href="#ode" id="toc-ode" class="nav-link" data-scroll-target="#ode">ODE</a></li>
  <li><a href="#utils" id="toc-utils" class="nav-link" data-scroll-target="#utils">Utils</a></li>
  <li><a href="#losses" id="toc-losses" class="nav-link" data-scroll-target="#losses">Losses</a></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a></li>
  <li><a href="#geodesics" id="toc-geodesics" class="nav-link" data-scroll-target="#geodesics">Geodesics</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#evaluations" id="toc-evaluations" class="nav-link" data-scroll-target="#evaluations">Evaluations</a></li>
  <li><a href="#plots" id="toc-plots" class="nav-link" data-scroll-target="#plots">Plots</a></li>
  </ul></li>
  <li><a href="#tests" id="toc-tests" class="nav-link" data-scroll-target="#tests">Tests</a>
  <ul>
  <li><a href="#learning-pedals-flows" id="toc-learning-pedals-flows" class="nav-link" data-scroll-target="#learning-pedals-flows">Learning Pedals Flows</a>
  <ul class="collapse">
  <li><a href="#train-autoencoder-or-the-geodesic-embedding" id="toc-train-autoencoder-or-the-geodesic-embedding" class="nav-link" data-scroll-target="#train-autoencoder-or-the-geodesic-embedding">Train autoencoder or the geodesic embedding</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#modified-training-for-neural-flattening" id="toc-modified-training-for-neural-flattening" class="nav-link" data-scroll-target="#modified-training-for-neural-flattening">Modified Training for Neural Flattening</a>
  <ul>
  <li><a href="#a-uniform-density-loss" id="toc-a-uniform-density-loss" class="nav-link" data-scroll-target="#a-uniform-density-loss">A Uniform Density Loss</a></li>
  <li><a href="#a-maximum-mean-discrepancy-loss" id="toc-a-maximum-mean-discrepancy-loss" class="nav-link" data-scroll-target="#a-maximum-mean-discrepancy-loss">A Maximum Mean Discrepancy Loss</a></li>
  <li><a href="#training-functions" id="toc-training-functions" class="nav-link" data-scroll-target="#training-functions">Training Functions</a></li>
  <li><a href="#dataset-setup" id="toc-dataset-setup" class="nav-link" data-scroll-target="#dataset-setup">Dataset Setup</a></li>
  <li><a href="#training-machinery" id="toc-training-machinery" class="nav-link" data-scroll-target="#training-machinery">Training Machinery</a>
  <ul class="collapse">
  <li><a href="#revised-plotting-functions" id="toc-revised-plotting-functions" class="nav-link" data-scroll-target="#revised-plotting-functions">Revised Plotting Functions</a></li>
  <li><a href="#train-autoencoder" id="toc-train-autoencoder" class="nav-link" data-scroll-target="#train-autoencoder">Train Autoencoder</a></li>
  <li><a href="#train-ode" id="toc-train-ode" class="nav-link" data-scroll-target="#train-ode">Train ODE</a></li>
  </ul></li>
  <li><a href="#all-in-one-trainer" id="toc-all-in-one-trainer" class="nav-link" data-scroll-target="#all-in-one-trainer">All in One Trainer</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/professorwug/diffusion_curvature/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../library/Comparison Space Construction.html">library</a></li><li class="breadcrumb-item"><a href="../library/MIOFlow for Neural Flattening.html">MIOFlow for Neural Flattening</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">MIOFlow for Neural Flattening</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>

<p>::: {#cell-1 .cell 0=‘d’ 1=‘e’ 2=‘f’ 3=‘a’ 4=‘u’ 5=‘l’ 6=‘t’ 7=’_’ 8=‘e’ 9=‘x’ 10=‘p’ 11=’ ’ 12=‘m’ 13=‘i’ 14=‘o’ 15=‘f’ 16=‘l’ 17=‘o’ 18=‘w’}</p>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Standard libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nbdev.showdoc <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure environment</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false' # Tells Jax not to hog all of the memory to this process.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">## Imports for plotting</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># set_matplotlib_formats('svg', 'pdf') # For export</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> to_rgba</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">## Progress bar</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">## project specifics</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> diffusion_curvature</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusion_curvature.utils <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pygsp</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<blockquote class="blockquote">
<p>An adaptation of the OT//Flow/Trajectory framework for learning the most optimal flattening of a manifold</p>
</blockquote>
<p>The below code is adapted from the repo KrishnaswamyLab/MIOFlow. It’s our practice, ala the <em>Zetteldev Way</em>, to prefer reimplementation to straight copy-pasting. We also believe that, in hindsight, the amount of code actually needed to implement such-and-such is much smaller - and can usually fit within a single notebook, especially when we use utilities like PyTorch lightning to handle the training boilerplate.</p>
<p>To adapt MIOFlow to our setting, only a few changes need to be made:</p>
<ul>
<li>For <span class="math inline">\(L_{m}\)</span>, the manifold loss, replace <span class="math inline">\(W_{2}(\hat{\nu}, \nu)\)</span> with a loss between <span class="math inline">\(\hat{\nu}\)</span> and the uniform distribution. If we use KLD, this corresponds corresponds to <em>maximizing the entropy</em> of <span class="math inline">\(\hat{\nu}\)</span>.</li>
<li>With no intermediate flow states, local and global training become the same. Just do one of them.</li>
<li>(optional geometric regularization) When embedding <span class="math inline">\(X_{0}\)</span> with the GAE, place special priority on preserving the <em>radial</em> distances from a chosen center point.</li>
<li>(optional geometric regularization) Apply the above to the network’s output also.</li>
</ul>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<p>::: {#cell-5 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, inspect</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>__file <span class="op">=</span> inspect.getfile(<span class="kw">lambda</span>: <span class="va">None</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>ROOT_DIR <span class="op">=</span> os.path.abspath(os.path.join(os.path.dirname(__file), <span class="st">'..'</span>,<span class="st">'..'</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>DATA_DIR <span class="op">=</span> os.path.join(ROOT_DIR, <span class="st">'data'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>NTBK_DIR <span class="op">=</span> os.path.join(ROOT_DIR, <span class="st">'notebooks'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>IMGS_DIR <span class="op">=</span> os.path.join(ROOT_DIR, <span class="st">'results'</span>, <span class="st">'images'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>RES_DIR <span class="op">=</span> os.path.join(ROOT_DIR, <span class="st">'results'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># WORM_FILE = os.path.join(DATA_DIR, 'worm_TrNet2.npz')</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># EB_BODIES_FILE = os.path.join(DATA_DIR, 'natalia_eb_rna_smoothed.npz')</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># EB_BODIES_PSEUDO_4 = os.path.join(DATA_DIR, 'pseudotime-4x.npy')</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># EB_BODIES_PSEUDO_6 = os.path.join(DATA_DIR, 'pseudotime-6x.npy')</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># EB_BODIES_PSEUDO_25 = os.path.join(DATA_DIR, 'pseudotime-25x.npy')</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># EB_BODIES_PSEUDO_82 = os.path.join(DATA_DIR, 'pseudotime-82x.npy')</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># DYNGEN_INFO_FILE = os.path.join(DATA_DIR, 'cell_info.csv')</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># DYNGEN_EXPR_FILE = os.path.join(DATA_DIR, 'dyngen_expression_bif.csv')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<section id="ode" class="level2">
<h2 class="anchored" data-anchor-id="ode">ODE</h2>
<p>::: {#cell-7 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ode_solve(z0, t0, t1, f):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Simplest Euler ODE initial value solver</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    h_max <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    n_steps <span class="op">=</span> math.ceil((<span class="bu">abs</span>(t1 <span class="op">-</span> t0)<span class="op">/</span>h_max).<span class="bu">max</span>().item())</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> (t1 <span class="op">-</span> t0)<span class="op">/</span>n_steps</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> t0</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> z0</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i_step <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> z <span class="op">+</span> h <span class="op">*</span> f(z, t)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> t <span class="op">+</span> h</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ODEF(nn.Module):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_with_grad(<span class="va">self</span>, z, t, grad_outputs):</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute f and a df/dz, a df/dp, a df/dt"""</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> z.shape[<span class="dv">0</span>]</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.forward(z, t)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> grad_outputs</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        adfdz, adfdt, <span class="op">*</span>adfdp <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            (out,), (z, t) <span class="op">+</span> <span class="bu">tuple</span>(<span class="va">self</span>.parameters()), grad_outputs<span class="op">=</span>(a),</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            allow_unused<span class="op">=</span><span class="va">True</span>, retain_graph<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># grad method automatically sums gradients for batch items, we have to expand them back </span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> adfdp <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            adfdp <span class="op">=</span> torch.cat([p_grad.flatten() <span class="cf">for</span> p_grad <span class="kw">in</span> adfdp]).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>            adfdp <span class="op">=</span> adfdp.expand(batch_size, <span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> batch_size</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> adfdt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>            adfdt <span class="op">=</span> adfdt.expand(batch_size, <span class="dv">1</span>) <span class="op">/</span> batch_size</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, adfdz, adfdt, adfdp</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> flatten_parameters(<span class="va">self</span>):</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        p_shapes <span class="op">=</span> []</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        flat_parameters <span class="op">=</span> []</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            p_shapes.append(p.size())</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>            flat_parameters.append(p.flatten())</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat(flat_parameters)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ODEAdjoint(torch.autograd.Function):</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(ctx, z0, t, flat_parameters, func):</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">isinstance</span>(func, ODEF)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        bs, <span class="op">*</span>z_shape <span class="op">=</span> z0.size()</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        time_len <span class="op">=</span> t.size(<span class="dv">0</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.zeros(time_len, bs, <span class="op">*</span>z_shape).to(z0)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>            z[<span class="dv">0</span>] <span class="op">=</span> z0</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i_t <span class="kw">in</span> <span class="bu">range</span>(time_len <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>                z0 <span class="op">=</span> ode_solve(z0, t[i_t], t[i_t<span class="op">+</span><span class="dv">1</span>], func)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>                z[i_t<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> z0</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        ctx.func <span class="op">=</span> func</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>        ctx.save_for_backward(t, z.clone(), flat_parameters)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(ctx, dLdz):</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="co">        dLdz shape: time_len, batch_size, *z_shape</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        func <span class="op">=</span> ctx.func</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        t, z, flat_parameters <span class="op">=</span> ctx.saved_tensors</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        time_len, bs, <span class="op">*</span>z_shape <span class="op">=</span> z.size()</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        n_dim <span class="op">=</span> np.prod(z_shape)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        n_params <span class="op">=</span> flat_parameters.size(<span class="dv">0</span>)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dynamics of augmented system to be calculated backwards in time</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> augmented_dynamics(aug_z_i, t_i):</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>            <span class="co">"""</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="co">            tensors here are temporal slices</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a><span class="co">            t_i - is tensor with size: bs, 1</span></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="co">            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="co">            """</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>            z_i, a <span class="op">=</span> aug_z_i[:, :n_dim], aug_z_i[:, n_dim:<span class="dv">2</span><span class="op">*</span>n_dim]  <span class="co"># ignore parameters and time</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Unflatten z and a</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>            z_i <span class="op">=</span> z_i.view(bs, <span class="op">*</span>z_shape)</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>            a <span class="op">=</span> a.view(bs, <span class="op">*</span>z_shape)</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.set_grad_enabled(<span class="va">True</span>):</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>                t_i <span class="op">=</span> t_i.detach().requires_grad_(<span class="va">True</span>)</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>                z_i <span class="op">=</span> z_i.detach().requires_grad_(<span class="va">True</span>)</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>                func_eval, adfdz, adfdt, adfdp <span class="op">=</span> func.forward_with_grad(z_i, t_i, grad_outputs<span class="op">=</span>a)  <span class="co"># bs, *z_shape</span></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>                adfdz <span class="op">=</span> adfdz.to(z_i) <span class="cf">if</span> adfdz <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> torch.zeros(bs, <span class="op">*</span>z_shape).to(z_i)</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>                adfdp <span class="op">=</span> adfdp.to(z_i) <span class="cf">if</span> adfdp <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> torch.zeros(bs, n_params).to(z_i)</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>                adfdt <span class="op">=</span> adfdt.to(z_i) <span class="cf">if</span> adfdt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> torch.zeros(bs, <span class="dv">1</span>).to(z_i)</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Flatten f and adfdz</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>            func_eval <span class="op">=</span> func_eval.view(bs, n_dim)</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>            adfdz <span class="op">=</span> adfdz.view(bs, n_dim) </span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> torch.cat((func_eval, <span class="op">-</span>adfdz, <span class="op">-</span>adfdp, <span class="op">-</span>adfdt), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>        dLdz <span class="op">=</span> dLdz.view(time_len, bs, n_dim)  <span class="co"># flatten dLdz for convenience</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>            <span class="co">## Create placeholders for output gradients</span></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Prev computed backwards adjoints to be adjusted by direct gradients</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>            adj_z <span class="op">=</span> torch.zeros(bs, n_dim).to(dLdz)</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>            adj_p <span class="op">=</span> torch.zeros(bs, n_params).to(dLdz)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>            <span class="co"># In contrast to z and p we need to return gradients for all times</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>            adj_t <span class="op">=</span> torch.zeros(time_len, bs, <span class="dv">1</span>).to(dLdz)</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i_t <span class="kw">in</span> <span class="bu">range</span>(time_len<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>                z_i <span class="op">=</span> z[i_t]</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>                t_i <span class="op">=</span> t[i_t]</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>                f_i <span class="op">=</span> func(z_i, t_i).view(bs, n_dim)</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute direct gradients</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>                dLdz_i <span class="op">=</span> dLdz[i_t]</span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>                dLdt_i <span class="op">=</span> torch.bmm(torch.transpose(dLdz_i.unsqueeze(<span class="op">-</span><span class="dv">1</span>), <span class="dv">1</span>, <span class="dv">2</span>), f_i.unsqueeze(<span class="op">-</span><span class="dv">1</span>))[:, <span class="dv">0</span>]</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Adjusting adjoints with direct gradients</span></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>                adj_z <span class="op">+=</span> dLdz_i</span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>                adj_t[i_t] <span class="op">=</span> adj_t[i_t] <span class="op">-</span> dLdt_i</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Pack augmented variable</span></span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>                aug_z <span class="op">=</span> torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Solve augmented system backwards</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>                aug_ans <span class="op">=</span> ode_solve(aug_z, t_i, t[i_t<span class="op">-</span><span class="dv">1</span>], augmented_dynamics)</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Unpack solved backwards augmented system</span></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>                adj_z[:] <span class="op">=</span> aug_ans[:, n_dim:<span class="dv">2</span><span class="op">*</span>n_dim]</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>                adj_p[:] <span class="op">+=</span> aug_ans[:, <span class="dv">2</span><span class="op">*</span>n_dim:<span class="dv">2</span><span class="op">*</span>n_dim <span class="op">+</span> n_params]</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>                adj_t[i_t<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> aug_ans[:, <span class="dv">2</span><span class="op">*</span>n_dim <span class="op">+</span> n_params:]</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>                <span class="kw">del</span> aug_z, aug_ans</span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>            <span class="co">## Adjust 0 time adjoint with direct gradients</span></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute direct gradients </span></span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>            dLdz_0 <span class="op">=</span> dLdz[<span class="dv">0</span>]</span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>            dLdt_0 <span class="op">=</span> torch.bmm(torch.transpose(dLdz_0.unsqueeze(<span class="op">-</span><span class="dv">1</span>), <span class="dv">1</span>, <span class="dv">2</span>), f_i.unsqueeze(<span class="op">-</span><span class="dv">1</span>))[:, <span class="dv">0</span>]</span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Adjust adjoints</span></span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>            adj_z <span class="op">+=</span> dLdz_0</span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>            adj_t[<span class="dv">0</span>] <span class="op">=</span> adj_t[<span class="dv">0</span>] <span class="op">-</span> dLdt_0</span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> adj_z.view(bs, <span class="op">*</span>z_shape), adj_t, adj_p, <span class="va">None</span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralODE(nn.Module):</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, func):</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NeuralODE, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">isinstance</span>(func, ODEF)</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.func <span class="op">=</span> func</span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z0, t<span class="op">=</span>torch.Tensor([<span class="fl">0.</span>, <span class="fl">1.</span>]), return_whole_sequence<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> t.to(z0)</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> ODEAdjoint.<span class="bu">apply</span>(z0, t, <span class="va">self</span>.func.flatten_parameters(), <span class="va">self</span>.func)</span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_whole_sequence:</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> z</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> z[<span class="op">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="utils" class="level2">
<h2 class="anchored" data-anchor-id="utils">Utils</h2>
<p>::: {#cell-9 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> group_extract(df, group, index<span class="op">=</span><span class="st">'samples'</span>, groupby<span class="op">=</span><span class="st">'samples'</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df.groupby(groupby).get_group(group).set_index(index).values</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample(data, group, size<span class="op">=</span>(<span class="dv">100</span>, ), replace<span class="op">=</span><span class="va">False</span>, to_torch<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    sub <span class="op">=</span> group_extract(data, group)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.arange(sub.shape[<span class="dv">0</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    sampled <span class="op">=</span> sub[np.random.choice(idx, size<span class="op">=</span>size, replace<span class="op">=</span>replace)]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> sampled[:,:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    radial_dists <span class="op">=</span> sampled[:,<span class="op">-</span><span class="dv">1</span>:]</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> to_torch:</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        points <span class="op">=</span> torch.Tensor(points).<span class="bu">float</span>()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        radial_dists <span class="op">=</span> torch.Tensor(radial_dists).<span class="bu">float</span>()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_cuda:</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            points <span class="op">=</span> points.cuda()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            radial_dists <span class="op">=</span> radial_dists.cuda()</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> points, radial_dists</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_np(data):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data.detach().cpu().numpy()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_steps(groups):</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(<span class="bu">zip</span>(groups[:<span class="op">-</span><span class="dv">1</span>], groups[<span class="dv">1</span>:]))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_seeds(seed:<span class="bu">int</span>):</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> config_hold_out(df:pd.DataFrame, hold_out:<span class="bu">str</span><span class="op">=</span><span class="st">'random'</span>, hold_one_out:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    DF <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> hold_one_out: <span class="co"># </span><span class="al">NOTE</span><span class="co">: we use all data</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: if hold one out is True and hold_out not 'random', </span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we train the DAE without this sample</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        DF <span class="op">=</span> df</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> <span class="bu">sorted</span>(df.samples.unique())</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> hold_one_out <span class="kw">is</span> <span class="va">True</span> <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create tmp df without all samples</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        df_ho <span class="op">=</span> df.drop(df[df[<span class="st">'samples'</span>]<span class="op">==</span>hold_out].index, inplace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        DF <span class="op">=</span> df_ho</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> <span class="bu">sorted</span>(df_ho.samples.unique())</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f'group=</span><span class="sc">{</span>hold_out<span class="sc">}</span><span class="ss"> not in known groups </span><span class="sc">{</span>groups<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DF, groups</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> config_criterion(criterion_name:<span class="bu">str</span><span class="op">=</span><span class="st">'ot'</span>, use_cuda:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    _valid_criterion_names <span class="op">=</span> <span class="st">'ot mmd'</span>.split()</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> criterion_name <span class="op">==</span> <span class="st">'mmd'</span>:</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> MMD_loss()</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> criterion_name <span class="op">==</span> <span class="st">'ot'</span>:</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> OT_loss(use_cuda<span class="op">=</span>use_cuda)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'</span><span class="sc">{</span>criterion_name<span class="sc">}</span><span class="ss"> not implemented.</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'Please use one of </span><span class="sc">{</span>_valid_criterion_names<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> criterion</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-10 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_groups_from_df(df, samples_key<span class="op">=</span><span class="st">'samples'</span>, samples<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    ---------</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame of shape (n_cells, n_genes), where the ordering of </span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">            the columns `n_genes` corresponds to the columns of `principle_components`.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">            It is assumed that the index of `df` are the cell types (but this need not be the case. </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">            See `cell_types`). If there are additional columns (e.g. `samples_key`, `cell_type_key`)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">            should be after the gene columns.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): The name of the column in the `df` that corresponds to the time</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">            samples. Defaults to `"samples"`. If `df[samples_key]` throws a `KeyError` </span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">            either because the `df` doesnt have this column in it or typo, will resort to</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">            `samples` to determine this.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">                        </span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">        samples (np.ndarray | list): List of timepoints where each value corresponds to the </span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">            timepoint of the same row in `df`. Defaults to `None`.</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">        groups (np.ndarray): List of time groups in order (e.g. `[0, 1, 2, 3, 4, 5, 6, 7]`).</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Figure out groups from provided samples    </span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> <span class="bu">sorted</span>(df[samples_key].unique())  </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> samples <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            groups <span class="op">=</span> <span class="bu">sorted</span>(np.unique(samples))  </span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'DataFrame df has no key </span><span class="sc">{</span>samples_key<span class="sc">}</span><span class="ss"> and backup list of samples'</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f' samples is None.'</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> groups</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cell_types_from_df(df, cell_type_key<span class="op">=</span><span class="va">None</span>, cell_types<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co">    ---------</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame of shape (n_cells, n_genes), where the ordering of </span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co">            the columns `n_genes` corresponds to the columns of `principle_components`.</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="co">            It is assumed that the index of `df` are the cell types (but this need not be the case. </span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co">            See `cell_types`). If there are additional columns (e.g. `samples_key`, `cell_type_key`)</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co">            should be after the gene columns.</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">        cell_type_key (str): The column name in the provided DataFrame `df` the corresponds to the </span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">            cell's cell types. Defaults to `None` which assumes the cell type is the index of the </span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">            `df i.e. `df.index`</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co">        cell_types (np.ndarray | list): List of cell types to use from the provided DataFrame `df`.</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co">            Defaults to `None`. If `use_cell_types = True` will attempt to figure this out from</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="co">            `cell_type_key`.</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="co">        cell_types (np.ndarray): List of cell types.</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cell_types <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># No column key provided, try to use index</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cell_type_key <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>                cell_types <span class="op">=</span> <span class="bu">sorted</span>(df.index.unique())</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>                cell_types <span class="op">=</span> <span class="bu">sorted</span>(df[cell_type_key].unique())</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">KeyError</span>(</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'DataFrame df has no key </span><span class="sc">{</span>cell_type_key<span class="sc">}</span><span class="ss"> and backup list of cell types'</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>                <span class="st">' cell_types is None'</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cell_types</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sample_n_from_df(</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>    df, n, samples_key<span class="op">=</span><span class="st">'samples'</span>, samples<span class="op">=</span><span class="va">None</span>,    </span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>    groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>    drop_index<span class="op">=</span><span class="va">False</span></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments</span></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a><span class="co">    ---------</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame of shape (n_cells, n_genes), where the ordering of </span></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a><span class="co">            the columns `n_genes` corresponds to the columns of `principle_components`.</span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="co">            It is assumed that the index of `df` are the cell types (but this need not be the case. </span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a><span class="co">            See `cell_types`). If there are additional columns (e.g. `samples_key`, `cell_type_key`)</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a><span class="co">            should be after the gene columns.</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): The name of the column in the `df` that corresponds to the time</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a><span class="co">            samples. Defaults to `"samples"`. If `df[samples_key]` throws a `KeyError` </span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a><span class="co">            either because the `df` doesnt have this column in it or typo, will resort to</span></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a><span class="co">            `samples` to determine this.</span></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a><span class="co">                        </span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a><span class="co">        samples (np.ndarray | list): List of timepoints where each value corresponds to the </span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a><span class="co">            timepoint of the same row in `df`. Defaults to `None`.</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a><span class="co">        groups (np.ndarray): List of time groups in order (e.g. `[0, 1, 2, 3, 4, 5, 6, 7]`).</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a><span class="co">            Defaults to `None`. If `None` will attempt to figure this out from provided</span></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a><span class="co">            `samples_key` or `samples`.</span></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a><span class="co">        drop_index (bool): Whether or not to drop index from `df`. Defaults to `False`.</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a><span class="co">        counts_n (pd.DataFrame): subsetted `df` where all rows correspond to `sample==n`.</span></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> groups <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span>  get_groups_from_df(df, samples_key, samples)</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>        counts_n <span class="op">=</span> df.reset_index(drop<span class="op">=</span>drop_index)[df[samples_key] <span class="op">==</span> groups[n]]</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> samples <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>            counts_n <span class="op">=</span> df.reset_index(drop<span class="op">=</span>drop_index)[samples <span class="op">==</span> groups[n]]</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'DataFrame df has no key </span><span class="sc">{</span>samples_key<span class="sc">}</span><span class="ss"> and backup list of samples'</span></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f' samples is None.'</span></span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> counts_n</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_times_from_groups(groups, where<span class="op">=</span><span class="st">'start'</span>, start<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments</span></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a><span class="co">    ---------</span></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a><span class="co">        groups (list): the list of the numerical groups in the data, e.g. </span></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a><span class="co">            `[0, 1, 2, 3, 4]`, if the data has five groups.</span></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a><span class="co">        where (str): Choices are `"start"`, and `"end"`. Defaults to `"end"`. Whether or not</span></span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a><span class="co">            to start the trajectories at `t_0` (`"start"`) or `t_n` (`"end"`). </span></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a><span class="co">        start (int): Defaults to `0`. Where in `generate_tjnet_trajectories` the trajectories started.</span></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a><span class="co">            This is used if attempting to generate outside of `t0`. Note this works relative to `where`.</span></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a><span class="co">            E.g. if `where="end"` and `start=0` then this is the same as `groups[-1]`.</span></span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a><span class="co">        times (list): The `groups` starting at `start` working from `end`.</span></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>    _valid_where <span class="op">=</span> <span class="st">'start end'</span>.split()</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> where <span class="kw">not</span> <span class="kw">in</span> _valid_where:</span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f'</span><span class="sc">{</span>where<span class="sc">}</span><span class="ss"> not known. Should be one of </span><span class="sc">{</span>_valid_where<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> groups</span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> where <span class="op">==</span> <span class="st">'end'</span>:</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>        times <span class="op">=</span> times[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> times[start:]</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> times</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="losses" class="level2">
<h2 class="anchored" data-anchor-id="losses">Losses</h2>
<p>::: {#cell-12 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MMD_loss(nn.Module):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    https://github.com/ZongxianLee/MMD_Loss.Pytorch/blob/master/mmd_loss.py</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, kernel_mul <span class="op">=</span> <span class="fl">2.0</span>, kernel_num <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MMD_loss, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel_num <span class="op">=</span> kernel_num</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel_mul <span class="op">=</span> kernel_mul</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fix_sigma <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> guassian_kernel(<span class="va">self</span>, source, target, kernel_mul<span class="op">=</span><span class="fl">2.0</span>, kernel_num<span class="op">=</span><span class="dv">5</span>, fix_sigma<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> <span class="bu">int</span>(source.size()[<span class="dv">0</span>])<span class="op">+</span><span class="bu">int</span>(target.size()[<span class="dv">0</span>])</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> torch.cat([source, target], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        total0 <span class="op">=</span> total.unsqueeze(<span class="dv">0</span>).expand(<span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">1</span>)))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        total1 <span class="op">=</span> total.unsqueeze(<span class="dv">1</span>).expand(<span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">1</span>)))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        L2_distance <span class="op">=</span> ((total0<span class="op">-</span>total1)<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">2</span>) </span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> fix_sigma:</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>            bandwidth <span class="op">=</span> fix_sigma</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>            bandwidth <span class="op">=</span> torch.<span class="bu">sum</span>(L2_distance.data) <span class="op">/</span> (n_samples<span class="op">**</span><span class="dv">2</span><span class="op">-</span>n_samples)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        bandwidth <span class="op">/=</span> kernel_mul <span class="op">**</span> (kernel_num <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        bandwidth_list <span class="op">=</span> [bandwidth <span class="op">*</span> (kernel_mul<span class="op">**</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(kernel_num)]</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        kernel_val <span class="op">=</span> [torch.exp(<span class="op">-</span>L2_distance <span class="op">/</span> bandwidth_temp) <span class="cf">for</span> bandwidth_temp <span class="kw">in</span> bandwidth_list]</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(kernel_val)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, source, target):</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> <span class="bu">int</span>(source.size()[<span class="dv">0</span>])</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        kernels <span class="op">=</span> <span class="va">self</span>.guassian_kernel(source, target, kernel_mul<span class="op">=</span><span class="va">self</span>.kernel_mul, kernel_num<span class="op">=</span><span class="va">self</span>.kernel_num, fix_sigma<span class="op">=</span><span class="va">self</span>.fix_sigma)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        XX <span class="op">=</span> kernels[:batch_size, :batch_size]</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        YY <span class="op">=</span> kernels[batch_size:, batch_size:]</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        XY <span class="op">=</span> kernels[:batch_size, batch_size:]</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        YX <span class="op">=</span> kernels[batch_size:, :batch_size]</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.mean(XX <span class="op">+</span> YY <span class="op">-</span> XY <span class="op">-</span>YX)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-13 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ot</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OT_loss(nn.Module):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    _valid <span class="op">=</span> <span class="st">'emd sinkhorn sinkhorn_knopp_unbalanced'</span>.split()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, which<span class="op">=</span><span class="st">'emd'</span>, use_cuda<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> which <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>._valid:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f'</span><span class="sc">{</span>which<span class="sc">}</span><span class="ss"> not known (</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>_valid<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> which <span class="op">==</span> <span class="st">'emd'</span>:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fn <span class="op">=</span> <span class="kw">lambda</span> m, n, M: ot.emd(m, n, M)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> which <span class="op">==</span> <span class="st">'sinkhorn'</span>:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fn <span class="op">=</span> <span class="kw">lambda</span> m, n, M : ot.sinkhorn(m, n, M, <span class="fl">2.0</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> which <span class="op">==</span> <span class="st">'sinkhorn_knopp_unbalanced'</span>:</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fn <span class="op">=</span> <span class="kw">lambda</span> m, n, M : ot.unbalanced.sinkhorn_knopp_unbalanced(m, n, M, <span class="fl">1.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, source, target, use_cuda<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_cuda <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            use_cuda <span class="op">=</span> <span class="va">self</span>.use_cuda</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> torch.from_numpy(ot.unif(source.size()[<span class="dv">0</span>]))</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        nu <span class="op">=</span> torch.from_numpy(ot.unif(target.size()[<span class="dv">0</span>]))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        M <span class="op">=</span> torch.cdist(source, target)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        pi <span class="op">=</span> <span class="va">self</span>.fn(mu, nu, M.detach().cpu())</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">type</span>(pi) <span class="kw">is</span> np.ndarray:</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            pi <span class="op">=</span> torch.tensor(pi)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">type</span>(pi) <span class="kw">is</span> torch.Tensor:</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>            pi <span class="op">=</span> pi.clone().detach()</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        pi <span class="op">=</span> pi.cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> pi</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        M <span class="op">=</span> M.to(pi.device)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.<span class="bu">sum</span>(pi <span class="op">*</span> M)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-14 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Density_loss(nn.Module):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hinge_value<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hinge_value <span class="op">=</span> hinge_value</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, source, target, groups <span class="op">=</span> <span class="va">None</span>, to_ignore <span class="op">=</span> <span class="va">None</span>, top_k <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> groups <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for global loss</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>            c_dist <span class="op">=</span> torch.stack([</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                torch.cdist(source[i], target[i]) </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>                <span class="co"># </span><span class="al">NOTE</span><span class="co">: check if this should be 1 indexed</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(groups))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> groups[i] <span class="op">!=</span> to_ignore</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for local loss</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>             c_dist <span class="op">=</span> torch.stack([</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>                torch.cdist(source, target)                 </span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        values, _ <span class="op">=</span> torch.topk(c_dist, top_k, dim<span class="op">=</span><span class="dv">2</span>, largest<span class="op">=</span><span class="va">False</span>, <span class="bu">sorted</span><span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        values <span class="op">-=</span> <span class="va">self</span>.hinge_value</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        values[values<span class="op">&lt;</span><span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.mean(values)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-15 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Local_density_loss(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sources, targets, groups, to_ignore, top_k <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(source, target)</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># c_dist = torch.cdist(source, target) </span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        c_dist <span class="op">=</span> torch.stack([</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>            torch.cdist(sources[i], targets[i]) </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co">: check if should be from range 1 or not.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(groups))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> groups[i] <span class="op">!=</span> to_ignore</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        vals, inds <span class="op">=</span> torch.topk(c_dist, top_k, dim<span class="op">=</span><span class="dv">2</span>, largest<span class="op">=</span><span class="va">False</span>, <span class="bu">sorted</span><span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> vals[inds[inds]]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.mean(values)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="models" class="level2">
<h2 class="anchored" data-anchor-id="models">Models</h2>
<p>::: {#cell-17 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn  <span class="im">import</span> functional <span class="im">as</span> F </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ToyODE(nn.Module):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    ODE derivative network</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    feature_dims (int) default '5': dimension of the inputs, either in ambient space or embedded space.</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    layer (list of int) defaulf ''[64]'': the hidden layers of the network.</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    activation (torch.nn) default '"ReLU"': activation function applied in between layers.</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">    scales (NoneType|list of float) default 'None': the initial scale for the noise in the trajectories. One scale per bin, add more if using an adaptative ODE solver.</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    n_aug (int) default '1': number of added dimensions to the input of the network. Total dimensions are features_dim + 1 (time) + n_aug. </span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Method</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">    forward (Callable)</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">        forward pass of the ODE derivative network.</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co">        t (torch.tensor): time of the evaluation.</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.tensor): position of the evalutation.</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Return:</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">        derivative at time t and position x.   </span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        feature_dims<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        layers<span class="op">=</span>[<span class="dv">64</span>],</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'ReLU'</span>,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        scales<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        n_aug<span class="op">=</span><span class="dv">2</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ToyODE, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> [feature_dims<span class="op">+</span><span class="dv">1</span><span class="op">+</span>n_aug, <span class="op">*</span>layers, feature_dims]</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        pairs <span class="op">=</span> <span class="bu">zip</span>(steps, steps[<span class="dv">1</span>:])</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        chain <span class="op">=</span> <span class="bu">list</span>(itertools.chain(<span class="op">*</span><span class="bu">list</span>(<span class="bu">zip</span>(</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">map</span>(<span class="kw">lambda</span> e: nn.Linear(<span class="op">*</span>e), pairs), </span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>            itertools.repeat(<span class="bu">getattr</span>(nn, activation)())</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        ))))[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.chain <span class="op">=</span> chain</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.seq <span class="op">=</span> (nn.Sequential(<span class="op">*</span>chain))</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> nn.Parameter(torch.tensor(scales, requires_grad<span class="op">=</span><span class="va">True</span>).<span class="bu">float</span>()) <span class="cf">if</span> scales <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_aug <span class="op">=</span> n_aug        </span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, t, x): <span class="co">#NOTE the forward pass when we use torchdiffeq must be forward(self,t,x)</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        zero <span class="op">=</span> torch.tensor([<span class="dv">0</span>]).cuda() <span class="cf">if</span> x.is_cuda <span class="cf">else</span> torch.tensor([<span class="dv">0</span>])</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        zeros <span class="op">=</span> zero.repeat(x.size()[<span class="dv">0</span>],<span class="va">self</span>.n_aug)</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        time <span class="op">=</span> t.repeat(x.size()[<span class="dv">0</span>],<span class="dv">1</span>)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>        aug <span class="op">=</span> torch.cat((x,time,zeros),dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.seq(aug)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.alpha <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.randn(x.size(),requires_grad<span class="op">=</span><span class="va">False</span>).cuda() <span class="cf">if</span> x.is_cuda <span class="cf">else</span> torch.randn(x.size(),requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        dxdt <span class="op">=</span> x <span class="op">+</span> z<span class="op">*</span><span class="va">self</span>.alpha[<span class="bu">int</span>(t<span class="op">-</span><span class="dv">1</span>)] <span class="cf">if</span> <span class="va">self</span>.alpha <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> x</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dxdt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-18 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn  <span class="im">import</span> functional <span class="im">as</span> F </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Autoencoder(nn.Module):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" </span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Geodesic Autoencoder</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    encoder_layers (list of int) default '[100, 100, 20]': encoder_layers[0] is the feature dimension, and encoder_layers[-1] the embedded dimension.</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    decoder_layers (list of int) defaulf '[20, 100, 100]': decoder_layers[0] is the embbeded dim and decoder_layers[-1] the feature dim.</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    activation (torch.nn) default '"Tanh"': activation function applied in between layers.</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">    use_cuda (bool) default to False: Whether to use GPU or CPU.</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Method</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">    encode</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">        forward pass of the encoder</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.tensor): observations</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Return:</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co">        the encoded observations</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">    decode</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">        forward pass of the decoder</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">        z (torch.tensor): embedded observations</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Return:</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">        the decoded observations</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co">    forward (Callable):</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co">        full forward pass, encoder and decoder</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="co">        x (torch.tensor): observations</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co">        Return:</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co">        denoised observations</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        encoder_layers <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">20</span>],</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        decoder_layers <span class="op">=</span> [<span class="dv">20</span>, <span class="dv">100</span>, <span class="dv">100</span>],</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        activation <span class="op">=</span> <span class="st">'Tanh'</span>,</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        use_cuda <span class="op">=</span> <span class="va">False</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    ):        </span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Autoencoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> decoder_layers <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>            decoder_layers <span class="op">=</span> [<span class="op">*</span>encoder_layers[::<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> use_cuda <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        encoder_shapes <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(encoder_layers, encoder_layers[<span class="dv">1</span>:]))</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>        decoder_shapes <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(decoder_layers, decoder_layers[<span class="dv">1</span>:]))</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>        encoder_linear <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> a: nn.Linear(<span class="op">*</span>a), encoder_shapes))</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        decoder_linear <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> a: nn.Linear(<span class="op">*</span>a), decoder_shapes))</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>        encoder_riffle <span class="op">=</span> <span class="bu">list</span>(itertools.chain(<span class="op">*</span><span class="bu">zip</span>(encoder_linear, itertools.repeat(<span class="bu">getattr</span>(nn, activation)()))))[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>        encoder <span class="op">=</span> nn.Sequential(<span class="op">*</span>encoder_riffle).to(device)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>        decoder_riffle <span class="op">=</span> <span class="bu">list</span>(itertools.chain(<span class="op">*</span><span class="bu">zip</span>(decoder_linear, itertools.repeat(<span class="bu">getattr</span>(nn, activation)()))))[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>        decoder <span class="op">=</span> nn.Sequential(<span class="op">*</span>decoder_riffle).to(device)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-19 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchdiffeq <span class="im">import</span> odeint_adjoint <span class="im">as</span> odeint</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ToyModel(nn.Module):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" </span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Neural ODE</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">        func (nn.Module): The network modeling the derivative.</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">        method (str) defaulf '"rk4"': any methods from torchdiffeq.</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">        rtol (NoneType | float): the relative tolerance of the ODE solver.</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">        atol (NoneType | float): the absolute tolerance. of the ODE solver.</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co">        use_norm (bool): if True keeps the norm of func.</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">        norm (list of torch.tensor): the norm of the derivative.</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Method</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co">        forward (Callable)</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co">            x (torch.tensor): the initial sample</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co">            t (torch.tensor) time points where we suppose x is from t[0]</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co">            return the last sample or the whole seq.      </span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, func, method<span class="op">=</span><span class="st">'rk4'</span>, rtol<span class="op">=</span><span class="va">None</span>, atol<span class="op">=</span><span class="va">None</span>, use_norm<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ToyModel, <span class="va">self</span>).<span class="fu">__init__</span>()        </span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.func <span class="op">=</span> func</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.method <span class="op">=</span> method</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rtol<span class="op">=</span>rtol</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.atol<span class="op">=</span>atol</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_norm <span class="op">=</span> use_norm</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm<span class="op">=</span>[]</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t, return_whole_sequence<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_norm:</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> time <span class="kw">in</span> t: </span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.norm.append(torch.linalg.norm(<span class="va">self</span>.func(time,x)).<span class="bu">pow</span>(<span class="dv">2</span>))</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.atol <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> <span class="va">self</span>.rtol <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> odeint(<span class="va">self</span>.func,x ,t, method<span class="op">=</span><span class="va">self</span>.method)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.atol <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="va">self</span>.rtol <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> odeint(<span class="va">self</span>.func,x ,t, method<span class="op">=</span><span class="va">self</span>.method, atol<span class="op">=</span><span class="va">self</span>.atol)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.atol <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> <span class="va">self</span>.rtol <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> odeint(<span class="va">self</span>.func,x ,t, method<span class="op">=</span><span class="va">self</span>.method, rtol<span class="op">=</span><span class="va">self</span>.rtol)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> odeint(<span class="va">self</span>.func,x ,t, method<span class="op">=</span><span class="va">self</span>.method, atol<span class="op">=</span><span class="va">self</span>.atol, rtol<span class="op">=</span><span class="va">self</span>.rtol)          </span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> <span class="kw">not</span> return_whole_sequence <span class="cf">else</span> x</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-20 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchdiffeq <span class="im">import</span> odeint_adjoint <span class="im">as</span> odeint</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchsde</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ToySDEModel(nn.Module):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" </span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Neural SDE model</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">        func (nn.Module): drift term.</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">        genc (nn.Module): diffusion term.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">        method (str): method of the SDE solver.</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Method</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">        forward (Callable)</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">            x (torch.tensor): the initial sample</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">            t (torch.tensor) time points where we suppose x is from t[0]</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">            return the last sample or the whole seq.  </span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, func, method<span class="op">=</span><span class="st">'euler'</span>, noise_type<span class="op">=</span><span class="st">'diagonal'</span>, sde_type<span class="op">=</span><span class="st">'ito'</span>, </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">2</span>, gunc<span class="op">=</span><span class="va">None</span>, dt<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ToySDEModel, <span class="va">self</span>).<span class="fu">__init__</span>()        </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.func <span class="op">=</span> func</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.method <span class="op">=</span> method</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_type <span class="op">=</span> noise_type</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sde_type <span class="op">=</span> sde_type</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> gunc <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._gunc_args <span class="op">=</span> <span class="st">'y'</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gunc <span class="op">=</span> nn.Linear(in_features, out_features)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._gunc_args <span class="op">=</span> <span class="st">'t,y'</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gunc <span class="op">=</span> gunc</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dt <span class="op">=</span> dt</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> f(<span class="va">self</span>, t, y):</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.func(t, y)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> g(<span class="va">self</span>, t, y):</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gunc(t, y) <span class="cf">if</span> <span class="va">self</span>._gunc_args <span class="op">==</span> <span class="st">'t,y'</span> <span class="cf">else</span> <span class="va">self</span>.gunc(y)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.3</span> <span class="op">*</span> torch.sigmoid(torch.cos(t) <span class="op">*</span> torch.exp(<span class="op">-</span>y))</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t, return_whole_sequence<span class="op">=</span><span class="va">False</span>, dt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        dt <span class="op">=</span> <span class="va">self</span>.dt <span class="cf">if</span> <span class="va">self</span>.dt <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="fl">0.1</span> <span class="cf">if</span> dt <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> dt        </span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torchsde.sdeint(<span class="va">self</span>, x, t, method<span class="op">=</span><span class="va">self</span>.method, dt<span class="op">=</span>dt)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> <span class="kw">not</span> return_whole_sequence <span class="cf">else</span> x</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-21 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_model(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    feature_dims<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    layers<span class="op">=</span>[<span class="dv">64</span>],</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    output_dims<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">'ReLU'</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    which<span class="op">=</span><span class="st">'ode'</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">'rk4'</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    rtol<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    atol<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    scales<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    n_aug<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    noise_type<span class="op">=</span><span class="st">'diagonal'</span>, sde_type<span class="op">=</span><span class="st">'ito'</span>,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    use_norm<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    use_cuda<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">2</span>, gunc<span class="op">=</span><span class="va">None</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Creates the 'ode' model or 'sde' model or the Geodesic Autoencoder. </span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co">    See the parameters of the respective classes.</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> which <span class="op">==</span> <span class="st">'ode'</span>:</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        ode <span class="op">=</span> ToyODE(feature_dims, layers, activation,scales,n_aug)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> ToyModel(ode,method,rtol, atol, use_norm<span class="op">=</span>use_norm)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> which <span class="op">==</span> <span class="st">'sde'</span>:</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        ode <span class="op">=</span> ToyODE(feature_dims, layers, activation,scales,n_aug)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> ToySDEModel(</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            ode, method, noise_type, sde_type,</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            in_features<span class="op">=</span>in_features, out_features<span class="op">=</span>out_features, gunc<span class="op">=</span>gunc</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> ToyGeo(feature_dims, layers, output_dims, activation)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda:</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        model.cuda()</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="geodesics" class="level2">
<h2 class="anchored" data-anchor-id="geodesics">Geodesics</h2>
<p>::: {#cell-23 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphtools</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> csr_matrix</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> pairwise_distances</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DiffusionDistance:</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    class DiffusionDistance        </span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        X (np.array) data </span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">        t_max (int), 2^t_max is the max scale of the Diffusion kernel</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">        knn (int) = 5 number of neighbors for the KNN in the alpha decay kernel construction, same default as in PHATE</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Anisotropy (int): the alpha in Coifman Lafon 2006, 1: double normalization 0: usual random walk</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">        log (bool) log(P) or not</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">        normalize (bool) min-max normalization of the distance matrix</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">        phate (bool) is PHATE op if true (should be the same as graphtool)</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, t_max<span class="op">=</span><span class="dv">5</span>, knn<span class="op">=</span><span class="dv">5</span>, anisotropy<span class="op">=</span><span class="dv">1</span>, log<span class="op">=</span><span class="va">False</span>, normalize<span class="op">=</span><span class="va">False</span>, symmetrize<span class="op">=</span><span class="va">False</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_max <span class="op">=</span> t_max</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.knn <span class="op">=</span> knn</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.aniso <span class="op">=</span> anisotropy</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log <span class="op">=</span> log</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.normalize <span class="op">=</span> normalize</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.symmetrize <span class="op">=</span> symmetrize</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.P <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pi <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.G <span class="op">=</span> <span class="va">None</span>    </span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_stationnary_distrib(<span class="va">self</span>): </span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        pi <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.K, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pi <span class="op">=</span> (pi<span class="op">/</span>np.<span class="bu">sum</span>(pi)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.pi</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_custom_diffusion_distance(<span class="va">self</span>): </span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.P</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        P_d <span class="op">=</span> P <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.log <span class="cf">else</span> csr_matrix((np.log(P.data),P.indices,P.indptr), shape<span class="op">=</span>P.shape)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> pairwise_distances(P_d,P_d,metric<span class="op">=</span><span class="st">'l1'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="va">self</span>.t_max): </span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>            P <span class="op">=</span> P <span class="op">@</span> P </span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.log<span class="op">==</span><span class="va">True</span>:</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>                dist <span class="op">=</span> pairwise_distances(P,P,metric<span class="op">=</span><span class="st">'l1'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>                np.fill_diagonal(dist,<span class="dv">1</span>)</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>                dist <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.log(dist)</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>                dist <span class="op">=</span> pairwise_distances(P_d,P_d,metric<span class="op">=</span><span class="st">'l1'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> G <span class="op">+</span> <span class="dv">2</span><span class="op">**</span>(<span class="op">-</span>t<span class="op">/</span><span class="fl">2.0</span>) <span class="op">*</span> dist</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.log<span class="op">==</span><span class="va">True</span>:</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>            dist <span class="op">=</span> pairwise_distances(<span class="va">self</span>.pi,<span class="va">self</span>.pi,metric<span class="op">=</span><span class="st">'l1'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>            np.fill_diagonal(dist,<span class="dv">1</span>)</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>            dist <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.log(dist)</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>            dist <span class="op">=</span> pairwise_distances(<span class="va">self</span>.pi,<span class="va">self</span>.pi,metric<span class="op">=</span><span class="st">'l1'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)     </span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> G <span class="op">+</span> <span class="dv">2</span><span class="op">**</span>(<span class="op">-</span>(<span class="va">self</span>.t_max<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="fl">2.0</span>) <span class="op">*</span> dist</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.G <span class="op">=</span> G <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.normalize <span class="cf">else</span> (G <span class="op">-</span> np.<span class="bu">min</span>(G))<span class="op">/</span>(np.<span class="bu">max</span>(G)<span class="op">-</span>np.<span class="bu">min</span>(G))</span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.G</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>        graph <span class="op">=</span> graphtools.Graph(X, knn<span class="op">=</span><span class="va">self</span>.knn,anisotropy<span class="op">=</span><span class="va">self</span>.aniso)</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> graph.K.toarray()</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.P <span class="op">=</span> graph.diff_op.toarray() </span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compute_stationnary_distrib()</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compute_custom_diffusion_distance()       </span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.G <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.symmetrize <span class="cf">else</span> (<span class="va">self</span>.G <span class="op">+</span> np.transpose(<span class="va">self</span>.G))<span class="op">/</span><span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<div id="cell-24" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| export </span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from scipy.spatial.distance import pdist, squareform</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># import phate</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># class PhateDistance:</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">#     """</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">#     class</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     -----</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">#         Arguments</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">#         ---------</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">#         X (np.array) data </span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">#         knn (int) = 5 number of neighbors for the KNN in the alpha decay kernel construction, same default as in PHATE</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">#         Anisotropy (int): the alpha in Coifman Lafon 2006, 1: double normalization 0: usual random walk</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">#         verbose (bool): verbose param. in PHATE.</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     """</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, knn=5, anisotropy=0,verbose=False) -&gt; None:</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.knn = knn</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.aniso = anisotropy</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.verbose = verbose</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">#     def fit(self, X # Dataset to fit.</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">#             ):</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">#         """</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">#         Parameters</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co">#         ----------</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">#             X: (np.array) Dataset to fit. </span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">#         Returns</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">#         -------</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">#         np.array</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">#             the L2 between Potential of Heat-diffusion</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co">#         """</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co">#         graph = phate.PHATE(knn=self.knn, verbose=self.verbose, n_landmark=X.shape[0]).fit(X)</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co">#         diff_pot = graph.diff_potential </span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.G = squareform(pdist(diff_pot))</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="co">#         return self.G</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>::: {#cell-25 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import phate</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> csr_matrix</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse.linalg <span class="im">import</span> eigs</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphtools</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DiffusionAffinity:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    class DiffusionAffinity        </span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        X (np.array) data </span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">        t_max (int), 2^t_max is the max scale of the Diffusion kernel</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">        knn (int) = 5 number of neighbors for the KNN in the alpha decay kernel construction, same default as in PHATE</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Anisotropy (int): the alpha in Coifman Lafon 2006, 1: double normalization 0: usual random walk</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">        t_diff (int) the power of the diffusion affinity matrix</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">        topeig (int) in the the top k eigenvalues to consider in the spectral decomposition</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">        return the l2 between the row of the affinity matrix A^t</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, knn<span class="op">=</span><span class="dv">5</span>, anisotropy<span class="op">=</span><span class="dv">0</span>, t_diff<span class="op">=</span><span class="dv">1</span>, topeig<span class="op">=</span><span class="dv">100</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.knn <span class="op">=</span> knn</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.aniso <span class="op">=</span> anisotropy</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_diff <span class="op">=</span> t_diff</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.topeig <span class="op">=</span> topeig</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.G <span class="op">=</span> <span class="va">None</span>    </span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.graph <span class="op">=</span> graphtools.Graph(X, knn<span class="op">=</span><span class="va">self</span>.knn,anisotropy<span class="op">=</span><span class="va">self</span>.aniso) </span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> <span class="va">self</span>.graph.diff_aff</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_diff <span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.G <span class="op">=</span> sklearn.metrics.pairwise.pairwise_distances(<span class="va">self</span>.A,<span class="va">self</span>.A,metric<span class="op">=</span><span class="st">'l2'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> <span class="va">self</span>.topeig <span class="cf">if</span> <span class="va">self</span>.topeig <span class="op">&lt;</span> X.shape[<span class="dv">0</span>] <span class="cf">else</span> X.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>            w, v <span class="op">=</span> eigs(<span class="va">self</span>.A,k<span class="op">=</span>k, which<span class="op">=</span><span class="st">'LR'</span>)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>            W <span class="op">=</span> np.diag(w)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>            A_t <span class="op">=</span> np.real(v <span class="op">@</span> (W<span class="op">**</span><span class="va">self</span>.t_diff) <span class="op">@</span> v.T)</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.G <span class="op">=</span> sklearn.metrics.pairwise.pairwise_distances(A_t,A_t,metric<span class="op">=</span><span class="st">'l2'</span>,n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.G</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-26 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance_matrix</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">class DiffusionDistance</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    kernel</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    X</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    t_max</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">    stationnary_distrib</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> old_DiffusionDistance:</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    kernel <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    t_max <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, kernel, t_max) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel<span class="op">=</span> kernel</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_max <span class="op">=</span> t_max</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_density_norm_matrix(<span class="va">self</span>):</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.kernel(<span class="va">self</span>.X)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> np.diag(np.<span class="bu">sum</span>(K, axis<span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.M <span class="op">=</span> np.linalg.inv(Q).dot(K).dot(np.linalg.inv(Q))</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.M</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_diffusion_Matrix(<span class="va">self</span>): </span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> np.diag(np.<span class="bu">sum</span>(<span class="va">self</span>.M, axis<span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.P <span class="op">=</span> np.linalg.inv(D).dot(<span class="va">self</span>.M)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.P</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_stationnary_distrib(<span class="va">self</span>): </span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        pi <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.M, axis <span class="op">=</span> <span class="dv">1</span>)<span class="op">/</span>np.<span class="bu">sum</span>(<span class="va">self</span>.M)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pi <span class="op">=</span> pi</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.pi</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> distance_matrix_Pt(<span class="va">self</span>, t): </span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>        Pt <span class="op">=</span> np.linalg.matrix_power(<span class="va">self</span>.P, <span class="dv">2</span><span class="op">**</span>(<span class="va">self</span>.t_max<span class="op">-</span>t))</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> distance_matrix(Pt,Pt,<span class="dv">1</span>)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_custom_diffusion_distance(<span class="va">self</span>): </span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> np.zeros((<span class="va">self</span>.X.shape[<span class="dv">0</span>], <span class="va">self</span>.X.shape[<span class="dv">0</span>]))</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="va">self</span>.t_max): </span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> G <span class="op">+</span> <span class="dv">2</span><span class="op">**</span>(<span class="op">-</span>(<span class="va">self</span>.t_max<span class="op">-</span>t)<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span> <span class="va">self</span>.distance_matrix_Pt(t)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> G <span class="op">+</span> <span class="dv">2</span><span class="op">**</span>(<span class="op">-</span>(<span class="va">self</span>.t_max<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span> distance_matrix(<span class="va">self</span>.pi[:, <span class="va">None</span>],<span class="va">self</span>.pi[:, <span class="va">None</span>],<span class="dv">1</span>)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.G <span class="op">=</span> G</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.G</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X <span class="op">=</span> X</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compute_density_norm_matrix()</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compute_diffusion_Matrix()</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compute_stationnary_distrib()</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compute_custom_diffusion_distance()</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.G</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-27 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_distance(</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    distance_type:<span class="bu">str</span><span class="op">=</span><span class="st">'gaussian'</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    rbf_length_scale:<span class="bu">float</span><span class="op">=</span><span class="fl">0.5</span>, t_max:<span class="bu">int</span><span class="op">=</span><span class="dv">5</span>, knn:<span class="bu">int</span><span class="op">=</span><span class="dv">5</span>, anisotropy:<span class="bu">int</span><span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    _valid_distance_types <span class="op">=</span> <span class="st">'gaussian alpha_decay phate'</span>.split()</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> distance_type <span class="op">==</span> <span class="st">'gaussian'</span>:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: rename / retool old_DiffusionDistance into single</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">#      DiffusionDistance class that "automagically" figures out</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#      implementation via input params</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        dist <span class="op">=</span> old_DiffusionDistance(RBF(rbf_length_scale), t_max<span class="op">=</span>t_max)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> distance_type <span class="op">==</span> <span class="st">'alpha_decay'</span>:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        dist <span class="op">=</span> DiffusionDistance(knn<span class="op">=</span>knn, t_max<span class="op">=</span>t_max)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># elif distance_type == 'phate':</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     dist = PhateDistance(knn=knn, anisotropy=anisotropy)</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'distance_type=</span><span class="sc">{</span>distance_type<span class="sc">}</span><span class="ss"> is not an implemented distance.</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'Please see </span><span class="sc">{</span>_valid_distance_types<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>::: {#cell-29 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, sys, json, math, itertools</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd, numpy <span class="im">as</span> np</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># from tqdm  tqdm</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.utils import sample, generate_steps</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.losses import MMD_loss, OT_loss, Density_loss, Local_density_loss</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    model, df, groups, optimizer, n_batches<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span>MMD_loss(),</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    use_cuda<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>(<span class="dv">100</span>, ),</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    local_loss<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    global_loss<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    hold_one_out<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    hold_out<span class="op">=</span><span class="st">'random'</span>,</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    top_k <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    hinge_value <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    use_density_loss<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use_local_density=False,</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    lambda_density <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    use_emb<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    use_gae<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    use_gaussian:<span class="bu">bool</span><span class="op">=</span><span class="va">True</span>, </span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>    add_noise:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>, </span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>    noise_scale:<span class="bu">float</span><span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    use_penalty<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    lambda_energy<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    reverse:<span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="co">    MIOFlow training loop</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a><span class="co">    Notes:</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a><span class="co">        - The argument `model` must have a method `forward` that accepts two arguments</span></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="co">            in its function signature:</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a><span class="co">                ```python</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a><span class="co">                model.forward(x, t)</span></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="co">                ```</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="co">            where, `x` is the input tensor and `t` is a `torch.Tensor` of time points (float).</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="co">        - The training loop is divided in two parts; local (predict t+1 from t), and global (predict the entire trajectory).</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a><span class="co">                        </span></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a><span class="co">        model (nn.Module): the initialized pytorch ODE model.</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): the DataFrame from which to extract batch data.</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a><span class="co">        groups (list): the list of the numerical groups in the data, e.g. </span></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="co">            `[1.0, 2.0, 3.0, 4.0, 5.0]`, if the data has five groups.</span></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer (torch.optim): an optimizer initilized with the model's parameters.</span></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a><span class="co">        n_batches (int): Default to '20', the number of batches from which to randomly sample each consecutive pair</span></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a><span class="co">            of groups.</span></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a><span class="co">        criterion (Callable | nn.Loss): a loss function.</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to send the model and data to cuda. </span></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size (tuple): Defaults to `(100, )`</span></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to sample data points with replacement.</span></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a><span class="co">        local_loss (bool): Defaults to `True`. Whether or not to use a local loss in the model.</span></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a><span class="co">            See notes for more detail.</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a><span class="co">        global_loss (bool): Defaults to `False`. Whether or not to use a global loss in the model.</span></span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a><span class="co">        hold_one_out (bool): Defaults to `False`. Whether or not to randomly hold one time pair</span></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a><span class="co">            e.g. t_1 to t_2 out when computing the global loss.</span></span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a><span class="co">        hold_out (str | int): Defaults to `"random"`. Which time point to hold out when calculating the</span></span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a><span class="co">            global loss.</span></span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a><span class="co">        apply_losses_in_time (bool): Defaults to `True`. Applies the losses and does back propegation</span></span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a><span class="co">            as soon as a loss is calculated. See notes for more detail.</span></span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a><span class="co">        top_k (int): Default to '5'. The k for the k-NN used in the density loss.</span></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a><span class="co">        hinge_value (float): Defaults to `0.01`. The hinge value for density loss.</span></span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a><span class="co">        use_density_loss (bool): Defaults to `True`. Whether or not to add density regularization.</span></span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a><span class="co">        lambda_density (float): Defaults to `1.0`. The weight for density loss.</span></span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (NoneType|nn.Module): Default to 'None'. The full geodesic Autoencoder.</span></span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a><span class="co">        use_emb (bool): Defaults to `True`. Whether or not to use the embedding model.</span></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a><span class="co">        use_gae (bool): Defaults to `False`. Whether or not to use the full Geodesic AutoEncoder.</span></span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a><span class="co">        use_gaussian (bool): Defaults to `True`. Whether to use random or gaussian noise.</span></span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a><span class="co">        add_noise (bool): Defaults to `False`. Whether or not to add noise.</span></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_scale (float): Defaults to `0.30`. How much to scale the noise by.</span></span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a><span class="co">        logger (NoneType|Logger): Default to 'None'. The logger to record information.</span></span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a><span class="co">        use_penalty (bool): Defaults to `False`. Whether or not to use $L_e$ during training (norm of the derivative).</span></span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a><span class="co">        lambda_energy (float): Default to '1.0'. The weight of the energy penalty.</span></span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a><span class="co">        reverse (bool): Whether to train time backwards.</span></span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> (use_emb <span class="kw">or</span> use_gae):</span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a>        use_emb <span class="op">=</span> <span class="va">False</span></span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a>        use_gae <span class="op">=</span> <span class="va">False</span></span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a>        warnings.warn(<span class="st">'</span><span class="ch">\'</span><span class="st">autoencoder</span><span class="ch">\'</span><span class="st"> is </span><span class="ch">\'</span><span class="st">None</span><span class="ch">\'</span><span class="st">, but </span><span class="ch">\'</span><span class="st">use_emb</span><span class="ch">\'</span><span class="st"> or </span><span class="ch">\'</span><span class="st">use_gae</span><span class="ch">\'</span><span class="st"> is True, both will be set to False.'</span>)</span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>    noise_fn <span class="op">=</span> torch.randn <span class="cf">if</span> use_gaussian <span class="cf">else</span> torch.rand</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> noise(data):</span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> noise_fn(<span class="op">*</span>data.shape).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> noise_fn(<span class="op">*</span>data.shape)</span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the indicies for the steps that should be used</span></span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>    steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> reverse:</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> groups[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Storage variables for losses</span></span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a>    batch_losses <span class="op">=</span> []</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a>    globe_losses <span class="op">=</span> []</span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a>        groups_ho <span class="op">=</span> [g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out]</span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a>        local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups_ho) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a>        local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> steps}</span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a>    density_fn <span class="op">=</span> Density_loss(hinge_value) <span class="co"># if not use_local_density else Local_density_loss()</span></span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Send model to cuda and specify it as training mode</span></span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda:</span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_batches)):</span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply local loss</span></span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> local_loss <span class="kw">and</span> <span class="kw">not</span> global_loss:</span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for storing the local loss with calling `.item()` so `loss.backward()` can still be used</span></span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a>            batch_loss <span class="op">=</span> []</span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> hold_one_out:</span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a>                groups <span class="op">=</span> [g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out] <span class="co"># </span><span class="al">TODO</span><span class="co">: Currently does not work if hold_out='random'. Do to_ignore before. </span></span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a>                steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> step_idx, (t0, t1) <span class="kw">in</span> <span class="bu">enumerate</span>(steps):  </span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> hold_out <span class="kw">in</span> [t0, t1] <span class="kw">and</span> hold_one_out: <span class="co"># </span><span class="al">TODO</span><span class="co">: This `if` can be deleted since the groups does not include the ho timepoint anymore</span></span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span>                              <span class="co"># i.e. it is always False. </span></span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a>                optimizer.zero_grad()</span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a>                <span class="co">#sampling, predicting, and evaluating the loss.</span></span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a>                <span class="co"># sample data</span></span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a>                data_t0 <span class="op">=</span> sample(df, t0, size<span class="op">=</span>sample_size, replace<span class="op">=</span>sample_with_replacement, to_torch<span class="op">=</span><span class="va">True</span>, use_cuda<span class="op">=</span>use_cuda)</span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a>                data_t1 <span class="op">=</span> sample(df, t1, size<span class="op">=</span>sample_size, replace<span class="op">=</span>sample_with_replacement, to_torch<span class="op">=</span><span class="va">True</span>, use_cuda<span class="op">=</span>use_cuda)</span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a>                time <span class="op">=</span> torch.Tensor([t0, t1]).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.Tensor([t0, t1])</span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> add_noise:</span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a>                    data_t0 <span class="op">+=</span> noise(data_t0) <span class="op">*</span> noise_scale</span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a>                    data_t1 <span class="op">+=</span> noise(data_t1) <span class="op">*</span> noise_scale</span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> use_gae:</span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a>                    data_t0 <span class="op">=</span> autoencoder.encoder(data_t0)</span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a>                    data_t1 <span class="op">=</span> autoencoder.encoder(data_t1)</span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a>                <span class="co"># prediction</span></span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a>                data_tp <span class="op">=</span> model(data_t0, time)</span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> use_emb:        </span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a>                    data_tp, data_t1 <span class="op">=</span> autoencoder.encoder(data_tp), autoencoder.encoder(data_t1)</span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a>                <span class="co"># loss between prediction and sample t1</span></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> criterion(data_tp, data_t1)</span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> use_density_loss:                </span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a>                    density_loss <span class="op">=</span> density_fn(data_tp, data_t1, top_k<span class="op">=</span>top_k)</span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a>                    density_loss <span class="op">=</span> density_loss.to(loss.device)</span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">+=</span> lambda_density <span class="op">*</span> density_loss</span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> use_penalty:</span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a>                    penalty <span class="op">=</span> <span class="bu">sum</span>(model.norm)</span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">+=</span> lambda_energy <span class="op">*</span> penalty</span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a>                <span class="co"># apply local loss as we calculate it</span></span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> apply_losses_in_time <span class="kw">and</span> local_loss:</span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a>                    loss.backward()</span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a>                    optimizer.step()</span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a>                    model.norm<span class="op">=</span>[]</span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a>                <span class="co"># save loss in storage variables </span></span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a>                local_losses[<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>].append(loss.item())</span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a>                batch_loss.append(loss)</span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a>            <span class="co"># convert the local losses into a tensor of len(steps)</span></span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a>            batch_loss <span class="op">=</span> torch.Tensor(batch_loss).<span class="bu">float</span>()</span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> use_cuda:</span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a>                batch_loss <span class="op">=</span> batch_loss.cuda()</span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> apply_losses_in_time:</span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a>                batch_loss.backward()</span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a>            <span class="co"># store average / sum of local losses for training</span></span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a>            ave_local_loss <span class="op">=</span> torch.mean(batch_loss)</span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a>            sum_local_loss <span class="op">=</span> torch.<span class="bu">sum</span>(batch_loss)            </span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a>            batch_losses.append(ave_local_loss.item())</span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply global loss</span></span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> global_loss <span class="kw">and</span> <span class="kw">not</span> local_loss:</span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a>            <span class="co">#sampling, predicting, and evaluating the loss.</span></span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sample data</span></span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a>            data_ti <span class="op">=</span> [</span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a>                sample(</span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a>                    df, group, size<span class="op">=</span>sample_size, replace<span class="op">=</span>sample_with_replacement, </span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a>                    to_torch<span class="op">=</span><span class="va">True</span>, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> group <span class="kw">in</span> groups</span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a>            time <span class="op">=</span> torch.Tensor(groups).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.Tensor(groups)</span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> add_noise:</span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>                data_ti <span class="op">=</span> [</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a>                    data <span class="op">+</span> noise(data) <span class="op">*</span> noise_scale <span class="cf">for</span> data <span class="kw">in</span> data_ti</span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> use_gae:</span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a>                data_ti <span class="op">=</span> [autoencoder.encoder(data) <span class="cf">for</span> data <span class="kw">in</span> data_ti]</span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a>            <span class="co"># prediction</span></span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a>            data_tp <span class="op">=</span> model(data_ti[<span class="dv">0</span>], time, return_whole_sequence<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> use_emb:        </span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a>                data_tp <span class="op">=</span> [autoencoder.encoder(data) <span class="cf">for</span> data <span class="kw">in</span> data_tp]</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a>                data_ti <span class="op">=</span> [autoencoder.encoder(data) <span class="cf">for</span> data <span class="kw">in</span> data_ti]</span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a>            <span class="co">#ignoring one time point</span></span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a>            to_ignore <span class="op">=</span> <span class="va">None</span> <span class="co">#TODO: This assignment of `to_ingnore`, could be moved at the beginning of the function. </span></span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="op">==</span> <span class="st">'random'</span>:</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a>                to_ignore <span class="op">=</span> np.random.choice(groups)</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a>                to_ignore <span class="op">=</span> hold_out</span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> hold_one_out:</span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'Unknown group to hold out'</span>)</span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a>                <span class="cf">pass</span></span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="bu">sum</span>([</span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a>                criterion(data_tp[i], data_ti[i]) </span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(groups))</span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> groups[i] <span class="op">!=</span> to_ignore</span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> use_density_loss:                </span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a>                density_loss <span class="op">=</span> density_fn(data_tp, data_ti, groups, to_ignore, top_k)</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a>                density_loss <span class="op">=</span> density_loss.to(loss.device)</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">+=</span> lambda_density <span class="op">*</span> density_loss</span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> use_penalty:</span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a>                penalty <span class="op">=</span> <span class="bu">sum</span>([model.norm[<span class="op">-</span>(i<span class="op">+</span><span class="dv">1</span>)] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(groups))</span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> groups[i] <span class="op">!=</span> to_ignore])</span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">+=</span> lambda_energy <span class="op">*</span> penalty</span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a>                                       </span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a>            model.norm<span class="op">=</span>[]</span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a>            globe_losses.append(loss.item())</span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> local_loss <span class="kw">and</span> global_loss:</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co">: weighted local / global loss has been removed to improve runtime</span></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'A form of loss must be specified.'</span>)</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a>                     </span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>    print_loss <span class="op">=</span> globe_losses <span class="cf">if</span> global_loss <span class="cf">else</span> batch_losses </span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> logger <span class="kw">is</span> <span class="va">None</span>:      </span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a>        tqdm.write(<span class="ss">f'Train loss: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(np.mean(print_loss), <span class="dv">5</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f'Train loss: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(np.mean(print_loss), <span class="dv">5</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> local_losses, batch_losses, globe_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-30 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.utils import generate_steps</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_ae(</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    model, df, groups, optimizer,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    n_epochs<span class="op">=</span><span class="dv">60</span>, criterion<span class="op">=</span>nn.MSELoss(), dist<span class="op">=</span><span class="va">None</span>, recon <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    use_cuda<span class="op">=</span><span class="va">False</span>, sample_size<span class="op">=</span>(<span class="dv">100</span>, ),</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    noise_min_scale<span class="op">=</span><span class="fl">0.09</span>,</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    noise_max_scale<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    hold_one_out:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>,</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    hold_out<span class="op">=</span><span class="st">'random'</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Geodesic Autoencoder training loop.</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Notes:</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co">        - We can train only the encoder the fit the geodesic distance (recon=False), or the full geodesic Autoencoder (recon=True),</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co">            i.e. matching the distance and reconstruction of the inputs.</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co">        model (nn.Module): the initialized pytorch Geodesic Autoencoder model.</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): the DataFrame from which to extract batch data.</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="co">        groups (list): the list of the numerical groups in the data, e.g. </span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a><span class="co">            `[1.0, 2.0, 3.0, 4.0, 5.0]`, if the data has five groups.</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer (torch.optim): an optimizer initilized with the model's parameters.</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="co">        n_epochs (int): Default to '60'. The number of training epochs.</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a><span class="co">        criterion (torch.nn). Default to 'nn.MSELoss()'. The criterion to minimize. </span></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a><span class="co">        dist (NoneType|Class). Default to 'None'. The distance Class with a 'fit(X)' method for a dataset 'X'. Computes the pairwise distances in 'X'.</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'True'. Whether or not the apply the reconstruction loss. </span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to send the model and data to cuda. </span></span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size (tuple): Defaults to `(100, )`.</span></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to sample data points with replacement.</span></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_min_scale (float): Default to '0.0'. The minimum noise scale. </span></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_max_scale (float): Default to '1.0'. The maximum noise scale. The true scale is sampled between these two bounds for each epoch. </span></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a><span class="co">        hold_one_out (bool): Default to False, whether or not to ignore a timepoint during training.</span></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a><span class="co">        hold_out (str|int): Default to 'random', the timepoint to hold out, either a specific element of 'groups' or a random one. </span></span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>    steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_epochs)):</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ignoring one time point</span></span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a>        to_ignore <span class="op">=</span> <span class="va">None</span></span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="op">==</span> <span class="st">'random'</span>:</span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a>            to_ignore <span class="op">=</span> np.random.choice(groups)</span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a>            to_ignore <span class="op">=</span> hold_out</span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> hold_one_out:</span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'Unknown group to hold out'</span>)</span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true" tabindex="-1"></a>        noise_scale <span class="op">=</span> torch.FloatTensor(<span class="dv">1</span>).uniform_(noise_min_scale, noise_max_scale)</span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true" tabindex="-1"></a>        data_ti <span class="op">=</span> torch.vstack([sample(df, group, size<span class="op">=</span>sample_size, replace<span class="op">=</span>sample_with_replacement, to_torch<span class="op">=</span><span class="va">True</span>, use_cuda<span class="op">=</span>use_cuda)[<span class="dv">0</span>] <span class="cf">for</span> group <span class="kw">in</span> groups <span class="cf">if</span> group <span class="op">!=</span> to_ignore])</span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> (noise_scale<span class="op">*</span>torch.randn(data_ti.size())).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> noise_scale<span class="op">*</span>torch.randn(data_ti.size())</span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true" tabindex="-1"></a>        encode_dt <span class="op">=</span> model.encoder(data_ti <span class="op">+</span> noise)</span>
<span id="cb21-82"><a href="#cb21-82" aria-hidden="true" tabindex="-1"></a>        recon_dt <span class="op">=</span> model.decoder(encode_dt) <span class="cf">if</span> recon <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb21-83"><a href="#cb21-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-84"><a href="#cb21-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> recon:</span>
<span id="cb21-85"><a href="#cb21-85" aria-hidden="true" tabindex="-1"></a>            loss_recon <span class="op">=</span> criterion(recon_dt,data_ti)</span>
<span id="cb21-86"><a href="#cb21-86" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_recon</span>
<span id="cb21-87"><a href="#cb21-87" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb21-88"><a href="#cb21-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch<span class="op">%</span><span class="dv">50</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb21-89"><a href="#cb21-89" aria-hidden="true" tabindex="-1"></a>                tqdm.write(<span class="ss">f'Train loss recon: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(np.mean(loss_recon.item()), <span class="dv">5</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb21-90"><a href="#cb21-90" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-91"><a href="#cb21-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dist <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb21-92"><a href="#cb21-92" aria-hidden="true" tabindex="-1"></a>            dist_geo <span class="op">=</span> dist.fit(data_ti.cpu().numpy())</span>
<span id="cb21-93"><a href="#cb21-93" aria-hidden="true" tabindex="-1"></a>            dist_geo <span class="op">=</span> torch.from_numpy(dist_geo).<span class="bu">float</span>().cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.from_numpy(dist_geo).<span class="bu">float</span>()</span>
<span id="cb21-94"><a href="#cb21-94" aria-hidden="true" tabindex="-1"></a>            dist_emb <span class="op">=</span> torch.cdist(encode_dt,encode_dt)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb21-95"><a href="#cb21-95" aria-hidden="true" tabindex="-1"></a>            loss_dist <span class="op">=</span> criterion(dist_emb,dist_geo)</span>
<span id="cb21-96"><a href="#cb21-96" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_recon <span class="op">+</span> loss_dist <span class="cf">if</span> recon <span class="cf">else</span> loss_dist</span>
<span id="cb21-97"><a href="#cb21-97" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb21-98"><a href="#cb21-98" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch<span class="op">%</span><span class="dv">50</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb21-99"><a href="#cb21-99" aria-hidden="true" tabindex="-1"></a>                tqdm.write(<span class="ss">f'Train loss dist: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(np.mean(loss_dist.item()), <span class="dv">5</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb21-100"><a href="#cb21-100" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-101"><a href="#cb21-101" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb21-102"><a href="#cb21-102" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb21-103"><a href="#cb21-103" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-104"><a href="#cb21-104" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb21-105"><a href="#cb21-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-31 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.plots import plot_comparision, plot_losses</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.eval import generate_plot_data</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_regimen(</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    n_local_epochs, n_epochs, n_post_local_epochs,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    exp_dir, </span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">BEGIN</span><span class="co">: train params</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    model, df, groups, optimizer, n_batches<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span>MMD_loss(), use_cuda<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    hold_one_out<span class="op">=</span><span class="va">False</span>, hold_out<span class="op">=</span><span class="st">'random'</span>, </span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    hinge_value<span class="op">=</span><span class="fl">0.01</span>, use_density_loss<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    top_k <span class="op">=</span> <span class="dv">5</span>, lambda_density <span class="op">=</span> <span class="fl">1.0</span>, </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span><span class="va">None</span>, use_emb<span class="op">=</span><span class="va">True</span>, use_gae<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>(<span class="dv">100</span>, ), </span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    add_noise<span class="op">=</span><span class="va">False</span>, noise_scale<span class="op">=</span><span class="fl">0.1</span>, use_gaussian<span class="op">=</span><span class="va">True</span>,  </span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    use_penalty<span class="op">=</span><span class="va">False</span>, lambda_energy<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">END</span><span class="co">: train params</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    steps<span class="op">=</span><span class="va">None</span>, plot_every<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    n_points<span class="op">=</span><span class="dv">100</span>, n_trajectories<span class="op">=</span><span class="dv">100</span>, n_bins<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    local_losses<span class="op">=</span><span class="va">None</span>, batch_losses<span class="op">=</span><span class="va">None</span>, globe_losses<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    reverse_schema<span class="op">=</span><span class="va">True</span>, reverse_n<span class="op">=</span><span class="dv">4</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    recon <span class="op">=</span> use_gae <span class="kw">and</span> <span class="kw">not</span> use_emb</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> steps <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> local_losses <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>            groups_ho <span class="op">=</span> [g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out]</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups_ho) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> reverse_schema:</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>                local_losses <span class="op">=</span> {</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>local_losses, </span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>{<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups_ho[::<span class="op">-</span><span class="dv">1</span>]) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups)}</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> reverse_schema:</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>                local_losses <span class="op">=</span> {</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>local_losses, </span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>{<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups[::<span class="op">-</span><span class="dv">1</span>])}</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch_losses <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>        batch_losses <span class="op">=</span> []</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> globe_losses <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>        globe_losses <span class="op">=</span> []</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>    reverse <span class="op">=</span> <span class="va">False</span></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_local_epochs), desc<span class="op">=</span><span class="st">'Pretraining Epoch'</span>):</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>        reverse <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> reverse_schema <span class="kw">and</span> epoch <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">False</span></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>        l_loss, b_loss, g_loss <span class="op">=</span> train(</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>            model, df, groups, optimizer, n_batches, </span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>            criterion <span class="op">=</span> criterion, use_cuda <span class="op">=</span> use_cuda,</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>            local_loss<span class="op">=</span><span class="va">True</span>, global_loss<span class="op">=</span><span class="va">False</span>, apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>            hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out, </span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>            hinge_value<span class="op">=</span>hinge_value,</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>            use_density_loss <span class="op">=</span> use_density_loss,    </span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> top_k, lambda_density <span class="op">=</span> lambda_density, </span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>            autoencoder <span class="op">=</span> autoencoder, use_emb <span class="op">=</span> use_emb, use_gae <span class="op">=</span> use_gae, sample_size<span class="op">=</span>sample_size, </span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>            sample_with_replacement<span class="op">=</span>sample_with_replacement, logger<span class="op">=</span>logger,</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>            add_noise<span class="op">=</span>add_noise, noise_scale<span class="op">=</span>noise_scale, use_gaussian<span class="op">=</span>use_gaussian, </span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a>            use_penalty<span class="op">=</span>use_penalty, lambda_energy<span class="op">=</span>lambda_energy, reverse<span class="op">=</span>reverse</span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> l_loss.items():  </span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a>            local_losses[k].extend(v)</span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a>        batch_losses.extend(b_loss)</span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a>        globe_losses.extend(g_loss)</span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> epoch <span class="op">%</span> plot_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a>            generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a>                model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a>                sample_with_replacement<span class="op">=</span>sample_with_replacement, use_cuda<span class="op">=</span>use_cuda, </span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a>                samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span>logger,</span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a>                autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a>            plot_comparision(</span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a>                df, generated, trajectories,</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a>                palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a>                save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a>                <span class="bu">file</span><span class="op">=</span><span class="ss">f'2d_comparision_local_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png'</span>,</span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_epochs), desc<span class="op">=</span><span class="st">'Epoch'</span>):</span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a>        reverse <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> reverse_schema <span class="kw">and</span> epoch <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">False</span></span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a>        l_loss, b_loss, g_loss <span class="op">=</span> train(</span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a>            model, df, groups, optimizer, n_batches, </span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a>            criterion <span class="op">=</span> criterion, use_cuda <span class="op">=</span> use_cuda,</span>
<span id="cb22-98"><a href="#cb22-98" aria-hidden="true" tabindex="-1"></a>            local_loss<span class="op">=</span><span class="va">False</span>, global_loss<span class="op">=</span><span class="va">True</span>, apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-99"><a href="#cb22-99" aria-hidden="true" tabindex="-1"></a>            hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out, </span>
<span id="cb22-100"><a href="#cb22-100" aria-hidden="true" tabindex="-1"></a>            hinge_value<span class="op">=</span>hinge_value,</span>
<span id="cb22-101"><a href="#cb22-101" aria-hidden="true" tabindex="-1"></a>            use_density_loss <span class="op">=</span> use_density_loss,       </span>
<span id="cb22-102"><a href="#cb22-102" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> top_k, lambda_density <span class="op">=</span> lambda_density, </span>
<span id="cb22-103"><a href="#cb22-103" aria-hidden="true" tabindex="-1"></a>            autoencoder <span class="op">=</span> autoencoder, use_emb <span class="op">=</span> use_emb, use_gae <span class="op">=</span> use_gae, sample_size<span class="op">=</span>sample_size, </span>
<span id="cb22-104"><a href="#cb22-104" aria-hidden="true" tabindex="-1"></a>            sample_with_replacement<span class="op">=</span>sample_with_replacement, logger<span class="op">=</span>logger, </span>
<span id="cb22-105"><a href="#cb22-105" aria-hidden="true" tabindex="-1"></a>            add_noise<span class="op">=</span>add_noise, noise_scale<span class="op">=</span>noise_scale, use_gaussian<span class="op">=</span>use_gaussian,</span>
<span id="cb22-106"><a href="#cb22-106" aria-hidden="true" tabindex="-1"></a>            use_penalty<span class="op">=</span>use_penalty, lambda_energy<span class="op">=</span>lambda_energy, reverse<span class="op">=</span>reverse</span>
<span id="cb22-107"><a href="#cb22-107" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-108"><a href="#cb22-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> l_loss.items():  </span>
<span id="cb22-109"><a href="#cb22-109" aria-hidden="true" tabindex="-1"></a>            local_losses[k].extend(v)</span>
<span id="cb22-110"><a href="#cb22-110" aria-hidden="true" tabindex="-1"></a>        batch_losses.extend(b_loss)</span>
<span id="cb22-111"><a href="#cb22-111" aria-hidden="true" tabindex="-1"></a>        globe_losses.extend(g_loss)</span>
<span id="cb22-112"><a href="#cb22-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> epoch <span class="op">%</span> plot_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-113"><a href="#cb22-113" aria-hidden="true" tabindex="-1"></a>            generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb22-114"><a href="#cb22-114" aria-hidden="true" tabindex="-1"></a>                model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb22-115"><a href="#cb22-115" aria-hidden="true" tabindex="-1"></a>                sample_with_replacement<span class="op">=</span>sample_with_replacement, use_cuda<span class="op">=</span>use_cuda, </span>
<span id="cb22-116"><a href="#cb22-116" aria-hidden="true" tabindex="-1"></a>                samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span>logger,</span>
<span id="cb22-117"><a href="#cb22-117" aria-hidden="true" tabindex="-1"></a>                autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb22-118"><a href="#cb22-118" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-119"><a href="#cb22-119" aria-hidden="true" tabindex="-1"></a>            plot_comparision(</span>
<span id="cb22-120"><a href="#cb22-120" aria-hidden="true" tabindex="-1"></a>                df, generated, trajectories,</span>
<span id="cb22-121"><a href="#cb22-121" aria-hidden="true" tabindex="-1"></a>                palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb22-122"><a href="#cb22-122" aria-hidden="true" tabindex="-1"></a>                save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb22-123"><a href="#cb22-123" aria-hidden="true" tabindex="-1"></a>                <span class="bu">file</span><span class="op">=</span><span class="ss">f'2d_comparision_local_</span><span class="sc">{</span>n_local_epochs<span class="sc">}</span><span class="ss">_global_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png'</span>,</span>
<span id="cb22-124"><a href="#cb22-124" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb22-125"><a href="#cb22-125" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-126"><a href="#cb22-126" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-127"><a href="#cb22-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_post_local_epochs), desc<span class="op">=</span><span class="st">'Posttraining Epoch'</span>):</span>
<span id="cb22-128"><a href="#cb22-128" aria-hidden="true" tabindex="-1"></a>        reverse <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> reverse_schema <span class="kw">and</span> epoch <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">False</span></span>
<span id="cb22-129"><a href="#cb22-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-130"><a href="#cb22-130" aria-hidden="true" tabindex="-1"></a>        l_loss, b_loss, g_loss <span class="op">=</span> train(</span>
<span id="cb22-131"><a href="#cb22-131" aria-hidden="true" tabindex="-1"></a>            model, df, groups, optimizer, n_batches, </span>
<span id="cb22-132"><a href="#cb22-132" aria-hidden="true" tabindex="-1"></a>            criterion <span class="op">=</span> criterion, use_cuda <span class="op">=</span> use_cuda,</span>
<span id="cb22-133"><a href="#cb22-133" aria-hidden="true" tabindex="-1"></a>            local_loss<span class="op">=</span><span class="va">True</span>, global_loss<span class="op">=</span><span class="va">False</span>, apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-134"><a href="#cb22-134" aria-hidden="true" tabindex="-1"></a>            hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out, </span>
<span id="cb22-135"><a href="#cb22-135" aria-hidden="true" tabindex="-1"></a>            hinge_value<span class="op">=</span>hinge_value,</span>
<span id="cb22-136"><a href="#cb22-136" aria-hidden="true" tabindex="-1"></a>            use_density_loss <span class="op">=</span> use_density_loss,       </span>
<span id="cb22-137"><a href="#cb22-137" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> top_k, lambda_density <span class="op">=</span> lambda_density,  </span>
<span id="cb22-138"><a href="#cb22-138" aria-hidden="true" tabindex="-1"></a>            autoencoder <span class="op">=</span> autoencoder, use_emb <span class="op">=</span> use_emb, use_gae <span class="op">=</span> use_gae, sample_size<span class="op">=</span>sample_size, </span>
<span id="cb22-139"><a href="#cb22-139" aria-hidden="true" tabindex="-1"></a>            sample_with_replacement<span class="op">=</span>sample_with_replacement, logger<span class="op">=</span>logger, </span>
<span id="cb22-140"><a href="#cb22-140" aria-hidden="true" tabindex="-1"></a>            add_noise<span class="op">=</span>add_noise, noise_scale<span class="op">=</span>noise_scale, use_gaussian<span class="op">=</span>use_gaussian,</span>
<span id="cb22-141"><a href="#cb22-141" aria-hidden="true" tabindex="-1"></a>            use_penalty<span class="op">=</span>use_penalty, lambda_energy<span class="op">=</span>lambda_energy, reverse<span class="op">=</span>reverse</span>
<span id="cb22-142"><a href="#cb22-142" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-143"><a href="#cb22-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> l_loss.items():  </span>
<span id="cb22-144"><a href="#cb22-144" aria-hidden="true" tabindex="-1"></a>            local_losses[k].extend(v)</span>
<span id="cb22-145"><a href="#cb22-145" aria-hidden="true" tabindex="-1"></a>        batch_losses.extend(b_loss)</span>
<span id="cb22-146"><a href="#cb22-146" aria-hidden="true" tabindex="-1"></a>        globe_losses.extend(g_loss)</span>
<span id="cb22-147"><a href="#cb22-147" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> epoch <span class="op">%</span> plot_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-148"><a href="#cb22-148" aria-hidden="true" tabindex="-1"></a>            generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb22-149"><a href="#cb22-149" aria-hidden="true" tabindex="-1"></a>                model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb22-150"><a href="#cb22-150" aria-hidden="true" tabindex="-1"></a>                sample_with_replacement<span class="op">=</span>sample_with_replacement, use_cuda<span class="op">=</span>use_cuda, </span>
<span id="cb22-151"><a href="#cb22-151" aria-hidden="true" tabindex="-1"></a>                samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span>logger,</span>
<span id="cb22-152"><a href="#cb22-152" aria-hidden="true" tabindex="-1"></a>                autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb22-153"><a href="#cb22-153" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-154"><a href="#cb22-154" aria-hidden="true" tabindex="-1"></a>            plot_comparision(</span>
<span id="cb22-155"><a href="#cb22-155" aria-hidden="true" tabindex="-1"></a>                df, generated, trajectories,</span>
<span id="cb22-156"><a href="#cb22-156" aria-hidden="true" tabindex="-1"></a>                palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb22-157"><a href="#cb22-157" aria-hidden="true" tabindex="-1"></a>                save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb22-158"><a href="#cb22-158" aria-hidden="true" tabindex="-1"></a>                <span class="bu">file</span><span class="op">=</span><span class="ss">f'2d_comparision_local_</span><span class="sc">{</span>n_local_epochs<span class="sc">}</span><span class="ss">_global_</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">_post_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png'</span>,</span>
<span id="cb22-159"><a href="#cb22-159" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb22-160"><a href="#cb22-160" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-161"><a href="#cb22-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-162"><a href="#cb22-162" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> reverse_schema:</span>
<span id="cb22-163"><a href="#cb22-163" aria-hidden="true" tabindex="-1"></a>        _temp <span class="op">=</span> {}</span>
<span id="cb22-164"><a href="#cb22-164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hold_one_out:</span>
<span id="cb22-165"><a href="#cb22-165" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps([g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out]):</span>
<span id="cb22-166"><a href="#cb22-166" aria-hidden="true" tabindex="-1"></a>                a <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb22-167"><a href="#cb22-167" aria-hidden="true" tabindex="-1"></a>                b <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb22-168"><a href="#cb22-168" aria-hidden="true" tabindex="-1"></a>                _temp[a] <span class="op">=</span> []</span>
<span id="cb22-169"><a href="#cb22-169" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i, value <span class="kw">in</span> <span class="bu">enumerate</span>(local_losses[a]):</span>
<span id="cb22-170"><a href="#cb22-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-171"><a href="#cb22-171" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-172"><a href="#cb22-172" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(local_losses[b].pop(<span class="dv">0</span>))</span>
<span id="cb22-173"><a href="#cb22-173" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb22-174"><a href="#cb22-174" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb22-175"><a href="#cb22-175" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb22-176"><a href="#cb22-176" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> _temp</span>
<span id="cb22-177"><a href="#cb22-177" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-178"><a href="#cb22-178" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups):</span>
<span id="cb22-179"><a href="#cb22-179" aria-hidden="true" tabindex="-1"></a>                a <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb22-180"><a href="#cb22-180" aria-hidden="true" tabindex="-1"></a>                b <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb22-181"><a href="#cb22-181" aria-hidden="true" tabindex="-1"></a>                _temp[a] <span class="op">=</span> []</span>
<span id="cb22-182"><a href="#cb22-182" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i, value <span class="kw">in</span> <span class="bu">enumerate</span>(local_losses[a]):</span>
<span id="cb22-183"><a href="#cb22-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-184"><a href="#cb22-184" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-185"><a href="#cb22-185" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(local_losses[b].pop(<span class="dv">0</span>))</span>
<span id="cb22-186"><a href="#cb22-186" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb22-187"><a href="#cb22-187" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb22-188"><a href="#cb22-188" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb22-189"><a href="#cb22-189" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> _temp</span>
<span id="cb22-190"><a href="#cb22-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-191"><a href="#cb22-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-192"><a href="#cb22-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-193"><a href="#cb22-193" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb22-194"><a href="#cb22-194" aria-hidden="true" tabindex="-1"></a>        plot_losses(</span>
<span id="cb22-195"><a href="#cb22-195" aria-hidden="true" tabindex="-1"></a>            local_losses, batch_losses, globe_losses, </span>
<span id="cb22-196"><a href="#cb22-196" aria-hidden="true" tabindex="-1"></a>            save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb22-197"><a href="#cb22-197" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span><span class="op">=</span><span class="ss">f'losses_l</span><span class="sc">{</span>n_local_epochs<span class="sc">}</span><span class="ss">_e</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">_ple</span><span class="sc">{</span>n_post_local_epochs<span class="sc">}</span><span class="ss">.png'</span></span>
<span id="cb22-198"><a href="#cb22-198" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-199"><a href="#cb22-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-200"><a href="#cb22-200" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> local_losses, batch_losses, globe_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="evaluations" class="level2">
<h2 class="anchored" data-anchor-id="evaluations">Evaluations</h2>
<p>::: {#cell-33 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_points(</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    model, df, n_points<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    samples_key<span class="op">=</span><span class="st">'samples'</span>, sample_time<span class="op">=</span><span class="va">None</span>, autoencoder<span class="op">=</span><span class="va">None</span>, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co">            See `MIOFlow.ode` for more.</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co">        n_points (int): Number of points to generate.</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co">            initial timepoint.</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_time (list | None): Defaults to `None`. If `None` uses the group numbers in order as the </span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="co">            timepoints as specified in the column `df[samples_key]`.</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="co">        generated (float[float[]]): a list with shape `(len(sample_time), n_points, len(df.columns) - 1)`</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated points.</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    to_torch <span class="op">=</span> <span class="va">True</span> <span class="co">#if use_cuda else False</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> <span class="bu">sorted</span>(df[samples_key].unique())</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sample_time <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>        sample_time <span class="op">=</span> groups</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    data_t0 <span class="op">=</span> sample(</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>        df, np.<span class="bu">min</span>(groups), size<span class="op">=</span>(n_points, ), </span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>        replace<span class="op">=</span>sample_with_replacement, to_torch<span class="op">=</span>to_torch, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> recon:</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        data_t0 <span class="op">=</span> torch.Tensor(data_t0).<span class="bu">float</span>()</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        data_t0 <span class="op">=</span> autoencoder.encoder(data_t0)</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>    time <span class="op">=</span>  torch.Tensor(sample_time).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.Tensor(sample_time)</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> model(data_t0, time, return_whole_sequence<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> recon:</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">=</span> autoencoder.decoder(generated)</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> to_np(generated)</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_trajectories(</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>    model, df, n_trajectories<span class="op">=</span><span class="dv">30</span>, n_bins<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>, samples_key<span class="op">=</span><span class="st">'samples'</span>,autoencoder<span class="op">=</span><span class="va">None</span>, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.</span></span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a><span class="co">            See `MIOFlow.ode` for more.</span></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.</span></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a><span class="co">        n_trajectories (int): Number of trajectories to generate.</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a><span class="co">        n_bins (int): Number of bins to use for the trajectories. More makes it smoother. Defaults to `100`.</span></span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling</span></span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a><span class="co">            initial timepoint.</span></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.</span></span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.</span></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.</span></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.</span></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a><span class="co">        trajectories (float[float[]]): a list with shape `(n_bins, n_points, len(df.columns) - 1)`</span></span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated trajectories.</span></span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> <span class="bu">sorted</span>(df[samples_key].unique())</span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>    sample_time <span class="op">=</span> np.linspace(np.<span class="bu">min</span>(groups), np.<span class="bu">max</span>(groups), n_bins)</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>    trajectories <span class="op">=</span> generate_points(model, df, n_trajectories, sample_with_replacement, use_cuda, samples_key, sample_time,autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon)</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectories</span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_plot_data(</span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a>    model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>, samples_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span><span class="va">None</span>, autoencoder<span class="op">=</span><span class="va">None</span>, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.</span></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a><span class="co">            See `MIOFlow.ode` for more.</span></span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.</span></span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a><span class="co">        n_points (int): Number of points to generate.</span></span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a><span class="co">        n_trajectories (int): Number of trajectories to generate.</span></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a><span class="co">        n_bins (int): Number of bins to use for the trajectories. More makes it smoother. Defaults to `100`.</span></span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling</span></span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a><span class="co">            initial timepoint.</span></span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.</span></span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.</span></span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.</span></span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.</span></span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a><span class="co">        points (float[float[]]): a list with shape `(len(df[sample_key].unique()), n_points, len(df.columns) - 1)`</span></span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated points.</span></span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a><span class="co">        trajectories (float[float[]]): a list with shape `(n_bins, n_points, len(df.columns) - 1)`</span></span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated trajectories.</span></span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> logger: logger.info(<span class="ss">f'Generating points'</span>)</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> generate_points(model, df, n_points, sample_with_replacement, use_cuda, samples_key, <span class="va">None</span>, autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon)</span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> logger: logger.info(<span class="ss">f'Generating trajectories'</span>)</span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a>    trajectories <span class="op">=</span> generate_trajectories(model, df, n_trajectories, n_bins, sample_with_replacement, use_cuda, samples_key, autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon)</span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> points, trajectories</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="plots" class="level2">
<h2 class="anchored" data-anchor-id="plots">Plots</h2>
<p>::: {#cell-35 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>sns.color_palette(<span class="st">"bright"</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_comparision(</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    df, generated, trajectories,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">False</span>, path<span class="op">=</span><span class="st">"../../results/"</span>, <span class="bu">file</span><span class="op">=</span><span class="st">'comparision.png'</span>,</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isdir(path):</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        os.makedirs(path)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> is_3d:</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> new_plot_comparisions(</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>            df, generated, trajectories,</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>            palette<span class="op">=</span>palette, df_time_key<span class="op">=</span>df_time_key,</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>x, y<span class="op">=</span>y, z<span class="op">=</span>z, is_3d<span class="op">=</span>is_3d,            </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>            groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>            save<span class="op">=</span>save, path<span class="op">=</span>path, <span class="bu">file</span><span class="op">=</span><span class="bu">file</span>,</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span><span class="op">/</span>s, <span class="dv">8</span><span class="op">/</span>s), dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> <span class="bu">sorted</span>(df[df_time_key].unique())</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>            df[x], df[y], df[z],</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>palette, alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>df[df_time_key], </span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>            s<span class="op">=</span>df[df_time_key], </span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="st">'X'</span>,</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        sns.scatterplot(</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>df, x<span class="op">=</span>x, y<span class="op">=</span>y, palette<span class="op">=</span>palette, alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span>df_time_key, style<span class="op">=</span>df_time_key, size<span class="op">=</span>df_time_key,</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>            markers<span class="op">=</span>{g: <span class="st">'X'</span> <span class="cf">for</span> g <span class="kw">in</span> states},</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>            sizes<span class="op">=</span>{g: <span class="dv">100</span> <span class="cf">for</span> g <span class="kw">in</span> states}, </span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>            ax<span class="op">=</span>ax, legend<span class="op">=</span><span class="va">False</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(generated, np.ndarray):</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">=</span> to_np(generated)</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.concatenate(generated, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>    n_gen <span class="op">=</span> <span class="bu">int</span>(points.shape[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(states))</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [state <span class="cf">for</span> state <span class="kw">in</span> states <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_gen)]</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>            points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>], points[:, <span class="dv">2</span>],</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>palette,</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>colors, </span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a>        sns.scatterplot(</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>points[:, <span class="dv">0</span>], y<span class="op">=</span>points[:, <span class="dv">1</span>], palette<span class="op">=</span>palette,</span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span>colors, </span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>            ax<span class="op">=</span>ax, legend<span class="op">=</span><span class="va">False</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>    ax.legend(title<span class="op">=</span><span class="st">'Timepoint'</span>, loc<span class="op">=</span><span class="st">'upper left'</span>, labels<span class="op">=</span>[<span class="st">'Ground Truth'</span>, <span class="st">'Predicted'</span>])</span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'ODE Points compared to Ground Truth'</span>)</span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trajectory <span class="kw">in</span> np.transpose(trajectories, axes<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)):</span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>            plt.plot(trajectory[:, <span class="dv">0</span>], trajectory[:, <span class="dv">1</span>], trajectory[:, <span class="dv">2</span>], alpha<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">'Black'</span>)<span class="op">;</span></span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trajectory <span class="kw">in</span> np.transpose(trajectories, axes<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)):</span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a>            plt.plot(trajectory[:, <span class="dv">0</span>], trajectory[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">'Black'</span>)<span class="op">;</span></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> save:</span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: savefig complains image is too large but saves it anyway. </span></span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a>            fig.savefig(os.path.expanduser(os.path.join(path, <span class="bu">file</span>)))</span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span> </span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-36 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>sns.color_palette(<span class="st">"bright"</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_losses(local_losses<span class="op">=</span><span class="va">None</span>, batch_losses<span class="op">=</span><span class="va">None</span>, globe_losses<span class="op">=</span><span class="va">None</span>, save<span class="op">=</span><span class="va">False</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'losses.png'</span>):</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isdir(path):</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        os.makedirs(path)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span><span class="op">/</span>s, <span class="dv">6</span><span class="op">/</span>s), dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    cur <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    sub <span class="op">=</span> <span class="bu">sum</span>([<span class="dv">1</span> <span class="cf">for</span> e <span class="kw">in</span> [local_losses, batch_losses, globe_losses] <span class="cf">if</span> e <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>])</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> local_losses <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,sub,cur)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        df_lloss <span class="op">=</span> pd.DataFrame(local_losses)</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">'Loss per step'</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>        sns.lineplot(data<span class="op">=</span>df_lloss, ax<span class="op">=</span>ax)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch_losses <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">+=</span> <span class="dv">1</span>        </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,sub,cur)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        df_bloss <span class="op">=</span> pd.DataFrame(batch_losses, columns<span class="op">=</span>[<span class="st">'Batch Loss'</span>])        </span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">'Batch loss per batch'</span>)        </span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        sns.lineplot(data<span class="op">=</span>df_bloss, lw<span class="op">=</span><span class="dv">1</span>, ax<span class="op">=</span>ax)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> globe_losses <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,sub,cur)</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>        df_gloss <span class="op">=</span> pd.DataFrame(globe_losses, columns<span class="op">=</span>[<span class="st">'Global Loss'</span>])        </span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">'Global Loss per batch'</span>)<span class="op">;</span></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        sns.lineplot(data<span class="op">=</span>df_gloss, ax<span class="op">=</span>ax)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> save:</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: savefig complains image is too large but saves it anyway. </span></span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>            fig.savefig(os.path.expanduser(os.path.join(path, <span class="bu">file</span>)))</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-37 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Patch</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.lines <span class="im">import</span> Line2D</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rcParams, cycler</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> new_plot_comparisions(</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    df, generated, trajectories,</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    palette <span class="op">=</span> <span class="st">'viridis'</span>,</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, </span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">False</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'comparision.png'</span>,</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> groups <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> <span class="bu">sorted</span>(df[df_time_key].unique())</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    cmap <span class="op">=</span> plt.cm.viridis</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    sns.set_palette(palette)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    plt.rcParams.update({</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.prop_cycle'</span>: plt.cycler(color<span class="op">=</span>cmap(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(groups) <span class="op">+</span> <span class="dv">1</span>))),</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.axisbelow'</span>: <span class="va">False</span>,</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.edgecolor'</span>: <span class="st">'lightgrey'</span>,</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.facecolor'</span>: <span class="st">'None'</span>,</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.grid'</span>: <span class="va">False</span>,</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.labelcolor'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.spines.right'</span>: <span class="va">False</span>,</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.spines.top'</span>: <span class="va">False</span>,</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">'figure.facecolor'</span>: <span class="st">'white'</span>,</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lines.solid_capstyle'</span>: <span class="st">'round'</span>,</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">'patch.edgecolor'</span>: <span class="st">'w'</span>,</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">'patch.force_edgecolor'</span>: <span class="va">True</span>,</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">'text.color'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.bottom'</span>: <span class="va">False</span>,</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.color'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.direction'</span>: <span class="st">'out'</span>,</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.top'</span>: <span class="va">False</span>,</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.color'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.direction'</span>: <span class="st">'out'</span>,</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.left'</span>: <span class="va">False</span>,</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.right'</span>: <span class="va">False</span>, </span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">'font.size'</span>:<span class="dv">12</span>, </span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.titlesize'</span>:<span class="dv">10</span>,</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.labelsize'</span>:<span class="dv">12</span></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>    n_cols <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>    n_rols <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>    grid_figsize <span class="op">=</span> [<span class="dv">12</span>, <span class="dv">8</span>]</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>    dpi <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>    grid_figsize <span class="op">=</span> (grid_figsize[<span class="dv">0</span>] <span class="op">*</span> n_cols, grid_figsize[<span class="dv">1</span>] <span class="op">*</span> n_rols)</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="va">None</span>, grid_figsize, dpi<span class="op">=</span>dpi)</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>    hspace <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>    wspace <span class="op">=</span> <span class="va">None</span></span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>    gspec <span class="op">=</span> plt.GridSpec(n_rols, n_cols, fig, hspace<span class="op">=</span>hspace, wspace<span class="op">=</span>wspace)</span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>    outline_width <span class="op">=</span> (<span class="fl">0.3</span>, <span class="fl">0.05</span>)</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>    bg_width, gap_width <span class="op">=</span> outline_width</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>    point <span class="op">=</span> np.sqrt(size)</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>    gap_size <span class="op">=</span> (point <span class="op">+</span> (point <span class="op">*</span> gap_width) <span class="op">*</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>    bg_size <span class="op">=</span> (np.sqrt(gap_size) <span class="op">+</span> (point <span class="op">*</span> bg_width) <span class="op">*</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>    plt.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>    is_3d <span class="op">=</span> <span class="va">False</span></span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if is_3d:        </span></span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     ax = fig.add_subplot(1,1,1,projection='3d')</span></span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># else:</span></span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     ax = fig.add_subplot(1,1,1)</span></span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a>    axs <span class="op">=</span> []</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, gs <span class="kw">in</span> <span class="bu">enumerate</span>(gspec):        </span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(gs)</span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="fl">0.3</span>   </span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a>                df[x], df[y],</span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span>df[df_time_key],</span>
<span id="cb26-84"><a href="#cb26-84" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>size,</span>
<span id="cb26-85"><a href="#cb26-85" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.7</span> <span class="op">*</span> n,</span>
<span id="cb26-86"><a href="#cb26-86" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'X'</span>,</span>
<span id="cb26-87"><a href="#cb26-87" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb26-88"><a href="#cb26-88" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb26-89"><a href="#cb26-89" aria-hidden="true" tabindex="-1"></a>                cmap<span class="op">=</span>cmap</span>
<span id="cb26-90"><a href="#cb26-90" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb26-91"><a href="#cb26-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-92"><a href="#cb26-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trajectory <span class="kw">in</span> np.transpose(trajectories, axes<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)):</span>
<span id="cb26-93"><a href="#cb26-93" aria-hidden="true" tabindex="-1"></a>                plt.plot(trajectory[:, <span class="dv">0</span>], trajectory[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'Black'</span>)<span class="op">;</span></span>
<span id="cb26-94"><a href="#cb26-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-95"><a href="#cb26-95" aria-hidden="true" tabindex="-1"></a>        states <span class="op">=</span> <span class="bu">sorted</span>(df[df_time_key].unique())</span>
<span id="cb26-96"><a href="#cb26-96" aria-hidden="true" tabindex="-1"></a>        points <span class="op">=</span> np.concatenate(generated, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-97"><a href="#cb26-97" aria-hidden="true" tabindex="-1"></a>        n_gen <span class="op">=</span> <span class="bu">int</span>(points.shape[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(states))</span>
<span id="cb26-98"><a href="#cb26-98" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> [state <span class="cf">for</span> state <span class="kw">in</span> states <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_gen)]</span>
<span id="cb26-99"><a href="#cb26-99" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb26-100"><a href="#cb26-100" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> <span class="st">'.'</span></span>
<span id="cb26-101"><a href="#cb26-101" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb26-102"><a href="#cb26-102" aria-hidden="true" tabindex="-1"></a>                points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>],</span>
<span id="cb26-103"><a href="#cb26-103" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span><span class="st">'black'</span>,</span>
<span id="cb26-104"><a href="#cb26-104" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>bg_size,</span>
<span id="cb26-105"><a href="#cb26-105" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="dv">1</span> <span class="op">*</span> n,</span>
<span id="cb26-106"><a href="#cb26-106" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span>o,</span>
<span id="cb26-107"><a href="#cb26-107" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb26-108"><a href="#cb26-108" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span></span>
<span id="cb26-109"><a href="#cb26-109" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb26-110"><a href="#cb26-110" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb26-111"><a href="#cb26-111" aria-hidden="true" tabindex="-1"></a>                points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>],</span>
<span id="cb26-112"><a href="#cb26-112" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb26-113"><a href="#cb26-113" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>gap_size,</span>
<span id="cb26-114"><a href="#cb26-114" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="dv">1</span> <span class="op">*</span> n,</span>
<span id="cb26-115"><a href="#cb26-115" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span>o,</span>
<span id="cb26-116"><a href="#cb26-116" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb26-117"><a href="#cb26-117" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span></span>
<span id="cb26-118"><a href="#cb26-118" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb26-119"><a href="#cb26-119" aria-hidden="true" tabindex="-1"></a>        pnts <span class="op">=</span> ax.scatter(</span>
<span id="cb26-120"><a href="#cb26-120" aria-hidden="true" tabindex="-1"></a>                points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>],</span>
<span id="cb26-121"><a href="#cb26-121" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span>colors,</span>
<span id="cb26-122"><a href="#cb26-122" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>size,</span>
<span id="cb26-123"><a href="#cb26-123" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.7</span> <span class="op">*</span> n,</span>
<span id="cb26-124"><a href="#cb26-124" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span>o,</span>
<span id="cb26-125"><a href="#cb26-125" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb26-126"><a href="#cb26-126" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb26-127"><a href="#cb26-127" aria-hidden="true" tabindex="-1"></a>                cmap<span class="op">=</span>cmap</span>
<span id="cb26-128"><a href="#cb26-128" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb26-129"><a href="#cb26-129" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb26-130"><a href="#cb26-130" aria-hidden="true" tabindex="-1"></a>        legend_elements <span class="op">=</span> [        </span>
<span id="cb26-131"><a href="#cb26-131" aria-hidden="true" tabindex="-1"></a>            Line2D(</span>
<span id="cb26-132"><a href="#cb26-132" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'o'</span>, </span>
<span id="cb26-133"><a href="#cb26-133" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span>cmap((i) <span class="op">/</span> (<span class="bu">len</span>(states)<span class="op">-</span><span class="dv">1</span>)), label<span class="op">=</span><span class="ss">f'T</span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">'</span>, </span>
<span id="cb26-134"><a href="#cb26-134" aria-hidden="true" tabindex="-1"></a>                markerfacecolor<span class="op">=</span>cmap((i) <span class="op">/</span> (<span class="bu">len</span>(states)<span class="op">-</span><span class="dv">1</span>)), markersize<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb26-135"><a href="#cb26-135" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb26-136"><a href="#cb26-136" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, state <span class="kw">in</span> <span class="bu">enumerate</span>(states)</span>
<span id="cb26-137"><a href="#cb26-137" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb26-138"><a href="#cb26-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-139"><a href="#cb26-139" aria-hidden="true" tabindex="-1"></a>        leg <span class="op">=</span> plt.legend(handles<span class="op">=</span>legend_elements, loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb26-140"><a href="#cb26-140" aria-hidden="true" tabindex="-1"></a>        ax.add_artist(leg)</span>
<span id="cb26-141"><a href="#cb26-141" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-142"><a href="#cb26-142" aria-hidden="true" tabindex="-1"></a>        legend_elements <span class="op">=</span> [        </span>
<span id="cb26-143"><a href="#cb26-143" aria-hidden="true" tabindex="-1"></a>            Line2D(</span>
<span id="cb26-144"><a href="#cb26-144" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'X'</span>, color<span class="op">=</span><span class="st">'w'</span>, </span>
<span id="cb26-145"><a href="#cb26-145" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="st">'Ground Truth'</span>, markerfacecolor<span class="op">=</span>cmap(<span class="dv">0</span>), markersize<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.3</span></span>
<span id="cb26-146"><a href="#cb26-146" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb26-147"><a href="#cb26-147" aria-hidden="true" tabindex="-1"></a>            Line2D([<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'w'</span>, label<span class="op">=</span><span class="st">'Predicted'</span>, markerfacecolor<span class="op">=</span>cmap(<span class="fl">.999</span>), markersize<span class="op">=</span><span class="dv">15</span>),</span>
<span id="cb26-148"><a href="#cb26-148" aria-hidden="true" tabindex="-1"></a>            Line2D([<span class="dv">0</span>], [<span class="dv">0</span>], color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Trajectory'</span>)</span>
<span id="cb26-149"><a href="#cb26-149" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb26-150"><a href="#cb26-150" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb26-151"><a href="#cb26-151" aria-hidden="true" tabindex="-1"></a>        leg <span class="op">=</span> plt.legend(handles<span class="op">=</span>legend_elements, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb26-152"><a href="#cb26-152" aria-hidden="true" tabindex="-1"></a>        ax.add_artist(leg)</span>
<span id="cb26-153"><a href="#cb26-153" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-154"><a href="#cb26-154" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">""</span>)</span>
<span id="cb26-155"><a href="#cb26-155" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="st">""</span>)</span>
<span id="cb26-156"><a href="#cb26-156" aria-hidden="true" tabindex="-1"></a>        ax.get_xaxis().get_major_formatter().set_scientific(<span class="va">False</span>)</span>
<span id="cb26-157"><a href="#cb26-157" aria-hidden="true" tabindex="-1"></a>        ax.get_yaxis().get_major_formatter().set_scientific(<span class="va">False</span>)</span>
<span id="cb26-158"><a href="#cb26-158" aria-hidden="true" tabindex="-1"></a>        kwargs <span class="op">=</span> <span class="bu">dict</span>(bottom<span class="op">=</span><span class="va">False</span>, left<span class="op">=</span><span class="va">False</span>, labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-159"><a href="#cb26-159" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(which<span class="op">=</span><span class="st">"both"</span>, <span class="op">**</span>kwargs)</span>
<span id="cb26-160"><a href="#cb26-160" aria-hidden="true" tabindex="-1"></a>        ax.set_frame_on(<span class="va">False</span>)</span>
<span id="cb26-161"><a href="#cb26-161" aria-hidden="true" tabindex="-1"></a>        ax.patch.set_alpha(<span class="dv">0</span>)</span>
<span id="cb26-162"><a href="#cb26-162" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-163"><a href="#cb26-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-164"><a href="#cb26-164" aria-hidden="true" tabindex="-1"></a>        axs.append(ax)</span>
<span id="cb26-165"><a href="#cb26-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-166"><a href="#cb26-166" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> save:</span>
<span id="cb26-167"><a href="#cb26-167" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: savefig complains image is too large but saves it anyway. </span></span>
<span id="cb26-168"><a href="#cb26-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb26-169"><a href="#cb26-169" aria-hidden="true" tabindex="-1"></a>            fig.savefig(os.path.expanduser(os.path.join(path, <span class="bu">file</span>)))</span>
<span id="cb26-170"><a href="#cb26-170" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb26-171"><a href="#cb26-171" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span> </span>
<span id="cb26-172"><a href="#cb26-172" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb26-173"><a href="#cb26-173" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
</section>
<section id="tests" class="level1">
<h1>Tests</h1>
<section id="learning-pedals-flows" class="level2">
<h2 class="anchored" data-anchor-id="learning-pedals-flows">Learning Pedals Flows</h2>
<p>::: {#cell-40 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> construct_diamond(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    points_per_petal:<span class="bu">int</span><span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    petal_width:<span class="bu">float</span><span class="op">=</span><span class="fl">0.25</span>,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    direction:<span class="bu">str</span><span class="op">=</span><span class="st">'y'</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">        points_per_petal (int). Defaults to `200`. Number of points per petal.</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">        petal_width (float): Defaults to `0.25`. How narrow the diamonds are.</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">        direction (str): Defaults to 'y'. Options `'y'` or `'x'`. Whether to make vertical</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">            or horizontal diamonds.</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co">    ---------</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co">        points (numpy.ndarray): the 2d array of points. </span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    n_side <span class="op">=</span> <span class="bu">int</span>(points_per_petal<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    axis_1 <span class="op">=</span> np.concatenate((</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>                np.linspace(<span class="dv">0</span>, petal_width, <span class="bu">int</span>(n_side<span class="op">/</span><span class="dv">2</span>)), </span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>                np.linspace(petal_width, <span class="dv">0</span>, <span class="bu">int</span>(n_side<span class="op">/</span><span class="dv">2</span>))</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>            ))</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    axis_2 <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n_side)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> (axis_1, axis_2) <span class="cf">if</span> direction <span class="op">==</span> <span class="st">'y'</span> <span class="cf">else</span> (axis_2, axis_1)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.vstack(axes).T</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.vstack((points, <span class="op">-</span><span class="dv">1</span><span class="op">*</span>points))</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.vstack((points, np.vstack((points[:, <span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span><span class="op">*</span>points[:, <span class="dv">1</span>])).T))</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> points</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_diamonds(</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    points_per_petal:<span class="bu">int</span><span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    petal_width:<span class="bu">float</span><span class="op">=</span><span class="fl">0.25</span>,</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    colors:<span class="bu">int</span><span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    scale_factor:<span class="bu">float</span><span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    use_gaussian:<span class="bu">bool</span><span class="op">=</span><span class="va">True</span>   </span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a><span class="co">        points_per_petal (int). Defaults to `200`. Number of points per petal.</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a><span class="co">        petal_width (float): Defaults to `0.25`. How narrow the diamonds are.</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a><span class="co">        colors (int): Defaults to `5`. The number of timesteps (colors) to produce.</span></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a><span class="co">        scale_factor (float): Defaults to `30`. How much to scale the noise by </span></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a><span class="co">            (larger values make samller noise).</span></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a><span class="co">        use_gaussian (bool): Defaults to `True`. Whether to use random or gaussian noise.</span></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a><span class="co">    ---------</span></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pandas.DataFrame): DataFrame with columns `samples`, `x`, `y`, where `samples`</span></span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a><span class="co">            are the time index (corresponds to colors) </span></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span>    </span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>    upper <span class="op">=</span> construct_diamond(points_per_petal, petal_width, <span class="st">'y'</span>)</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>    lower <span class="op">=</span> construct_diamond(points_per_petal, petal_width, <span class="st">'x'</span>)</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.vstack((upper, lower)) </span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>    noise_fn <span class="op">=</span> np.random.randn <span class="cf">if</span> use_gaussian <span class="cf">else</span> np.random.rand</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>    noise <span class="op">=</span> noise_fn(<span class="op">*</span>data.shape) <span class="op">/</span> scale_factor</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> data <span class="op">+</span> noise</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(data, columns<span class="op">=</span>[<span class="st">'d1'</span>, <span class="st">'d2'</span>])</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>    c_values <span class="op">=</span> np.linspace(colors, <span class="dv">1</span>, colors)</span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>    c_thresholds <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">0</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span>(colors<span class="op">+</span><span class="dv">1</span>), colors)</span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>    df.insert(<span class="dv">0</span>, <span class="st">'samples'</span>, colors)</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'samples'</span>] <span class="op">=</span> colors </span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> value, threshold <span class="kw">in</span> <span class="bu">zip</span>(c_values, c_thresholds):</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> ((np.<span class="bu">abs</span>(df.d1) <span class="op">&lt;=</span> threshold) <span class="op">&amp;</span> (np.<span class="bu">abs</span>(df.d2) <span class="op">&lt;=</span> threshold))</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>        df.loc[index, <span class="st">'samples'</span>] <span class="op">=</span> value</span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>    df.set_index(<span class="st">'samples'</span>)</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<div id="cell-41" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-42" class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>phate_dims <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> make_diamonds()</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">8</span>), dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, hue<span class="op">=</span><span class="st">'samples'</span>, palette<span class="op">=</span><span class="st">'viridis'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>
<div id="cell-43" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">samples</th>
<th data-quarto-table-cell-role="th">d1</th>
<th data-quarto-table-cell-role="th">d2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>-0.054423</td>
<td>0.024457</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>0.001634</td>
<td>0.039475</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>0.007258</td>
<td>0.029124</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>0.086090</td>
<td>0.076865</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>0.005981</td>
<td>0.052903</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">795</td>
<td>5</td>
<td>-0.962857</td>
<td>-0.035675</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">796</td>
<td>5</td>
<td>-0.947822</td>
<td>0.035047</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">797</td>
<td>5</td>
<td>-0.969853</td>
<td>0.017697</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">798</td>
<td>5</td>
<td>-1.033862</td>
<td>-0.005428</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">799</td>
<td>5</td>
<td>-0.998611</td>
<td>0.025784</td>
</tr>
</tbody>
</table>

<p>800 rows × 3 columns</p>
</div>
</div>
</div>
<div id="cell-44" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>ss, ds <span class="op">=</span> sample(df,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-45" class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>ss.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(100, 2)</code></pre>
</div>
</div>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>ss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 0.27157391,  0.62604272,  0.73097067],
       [ 0.38694295, -0.403171  ,  0.82929386],
       [ 0.48196983, -0.79177453,  0.37523084],
       [ 0.08681756, -0.09821649,  0.99137088],
       [-0.11470718,  0.99013678,  0.08044518],
       [ 0.69314334, -0.10035174,  0.71377996],
       [ 0.03424484,  0.77489109,  0.63116645],
       [-0.27861278, -0.94448326,  0.17414445],
       [ 0.31599603,  0.94004565,  0.12829921],
       [ 0.79543347,  0.20352313,  0.57084493],
       [ 0.09300924,  0.33415584,  0.93791746],
       [ 0.19852387, -0.86337481,  0.46386658],
       [ 0.11907419, -0.51259469,  0.85033407],
       [-0.93584405,  0.27535147,  0.21994884],
       [-0.19015764,  0.3361746 ,  0.92240268],
       [-0.7024748 ,  0.33248706,  0.62927062],
       [-0.99910545, -0.03007562,  0.02972804],
       [ 0.754344  ,  0.56387073,  0.33617692],
       [-0.21035747,  0.52578493,  0.82419655],
       [-0.04928899,  0.21502587,  0.97536376],
       [ 0.04450301,  0.72333292,  0.68906383],
       [ 0.04877282,  0.64915893,  0.75908754],
       [-0.85778484,  0.51168714,  0.04880007],
       [ 0.13322896,  0.92571238,  0.35398678],
       [ 0.26638696,  0.27492114,  0.92382702],
       [-0.7170259 , -0.69208954,  0.08298153],
       [ 0.22801951, -0.94511252,  0.2340287 ],
       [-0.70776471,  0.07432202,  0.70252783],
       [-0.37892994, -0.91795935,  0.11731463],
       [ 0.63804077,  0.70000429,  0.32077714],
       [-0.80099009,  0.46969424,  0.37121719],
       [ 0.3500824 ,  0.53986592,  0.76549794],
       [-0.17083217, -0.3056379 ,  0.93669731],
       [ 0.32761014, -0.71428927,  0.61843547],
       [ 0.58550921,  0.33803752,  0.73682399],
       [ 0.84813579, -0.5118667 ,  0.13659491],
       [ 0.0805938 , -0.62054209,  0.78002061],
       [ 0.9259989 , -0.33077552,  0.1819714 ],
       [-0.30252999, -0.83336119,  0.46258483],
       [-0.3373477 ,  0.30175274,  0.89170725],
       [-0.11260072, -0.43517205,  0.89327844],
       [-0.28039085,  0.48488428,  0.82841306],
       [ 0.16288095, -0.79091795,  0.58984625],
       [-0.70320277, -0.6697118 ,  0.23872988],
       [ 0.34036898,  0.19366488,  0.92013199],
       [ 0.75838607, -0.27890634,  0.58911954],
       [-0.66559143, -0.74631343,  0.00207651],
       [-0.21293666,  0.79068241,  0.57400288],
       [ 0.46885588, -0.06231271,  0.88107394],
       [-0.553701  ,  0.19589749,  0.80934503],
       [ 0.22000234,  0.43473512,  0.87327221],
       [ 0.48923616,  0.62128586,  0.61208811],
       [ 0.54333403, -0.05634587,  0.83762359],
       [-0.0500215 ,  0.36898847,  0.92808694],
       [ 0.74761139,  0.59952619,  0.28573688],
       [-0.74902309,  0.65104454,  0.12290415],
       [ 0.12112421,  0.65329435,  0.74735227],
       [-0.02989288, -0.27653027,  0.96054017],
       [-0.59071382,  0.67445038,  0.4429152 ],
       [-0.85197951,  0.15406399,  0.50039504],
       [ 0.37622225, -0.9136526 ,  0.1539342 ],
       [-0.36601067,  0.18083303,  0.91287217],
       [-0.71264429,  0.33643341,  0.6155897 ],
       [ 0.61885117,  0.70997792,  0.33608718],
       [ 0.60122802, -0.79332677,  0.09569484],
       [ 0.33824539, -0.25005799,  0.90722713],
       [ 0.43017125, -0.54199876,  0.72193493],
       [ 0.69618527,  0.25596036,  0.67067903],
       [-0.96568476, -0.05697052,  0.25339162],
       [ 0.8594382 ,  0.3111978 ,  0.40561302],
       [ 0.13488979, -0.64255576,  0.75427239],
       [-0.93281793,  0.31493649,  0.17511629],
       [-0.89427198, -0.23624001,  0.38008984],
       [-0.07174845, -0.44144892,  0.89441322],
       [-0.35289549, -0.34722829,  0.86884825],
       [ 0.48665834,  0.32060757,  0.81263426],
       [ 0.5823224 ,  0.68026034,  0.44513649],
       [ 0.34364089, -0.93845943,  0.03471077],
       [ 0.93707674, -0.07328768,  0.34134455],
       [-0.21790403,  0.58994187,  0.77748725],
       [-0.62423709, -0.77755919,  0.07569519],
       [ 0.34382703,  0.91790981,  0.19805192],
       [-0.50780285,  0.77405083,  0.37812905],
       [-0.31291505, -0.19707716,  0.92910966],
       [ 0.49987591,  0.85573022,  0.13360339],
       [ 0.73487149, -0.6132566 ,  0.28962086],
       [-0.3342423 , -0.07725393,  0.93931566],
       [-0.05248339,  0.04709945,  0.99751047],
       [ 0.67798958, -0.43808361,  0.59026509],
       [ 0.48563848,  0.15405512,  0.86047794],
       [ 0.05803646, -0.79567211,  0.60294085],
       [ 0.6448063 ,  0.58608339,  0.49064355],
       [-0.9686536 , -0.13990892,  0.20526981],
       [-0.68365255, -0.72100442,  0.11301243],
       [-0.14754701, -0.80015042,  0.58136838],
       [ 0.28491858,  0.91157653,  0.29639438],
       [ 0.80740145, -0.553112  ,  0.20535336],
       [-0.06275003,  0.92451312,  0.37594938],
       [-0.9124631 , -0.37166011,  0.17111356],
       [-0.44415729,  0.71037651,  0.54597574]])</code></pre>
</div>
</div>
<section id="train-autoencoder-or-the-geodesic-embedding" class="level3">
<h3 class="anchored" data-anchor-id="train-autoencoder-or-the-geodesic-embedding">Train autoencoder or the geodesic embedding</h3>
<p>Set seeds and check GPU</p>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>set_seeds(<span class="dv">0</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>use_cuda <span class="op">=</span> torch.cuda.is_available()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Handle hold-out training condition</p>
<div id="cell-50" class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is True if we want to holdout (or skip) one timepoint during training. It is used to test the accuracy of the trajectories on unseen data.</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>hold_one_out <span class="op">=</span> <span class="va">False</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># It can be a group number or 'random', works in tandem with hold_one_out</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>hold_out <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The dimensions in the input space, it is columns - 1 because we assume one column is equal to "samples".</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>model_features <span class="op">=</span> <span class="bu">len</span>(df.columns) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> <span class="bu">sorted</span>(df.samples.unique())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># These determine the logic flow for training: </span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=True use_gae=False is only the encoder to match the approximation of the geodesic.</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=False use_gae=True the full Geodesic Autoencoder (GAE), i.e. matching the geodesic and a reconstruction loss.</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=False use_gae=False Is not using the GAE.</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=True use_gae=True, is redundant and should raise an error. </span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>use_emb <span class="op">=</span> <span class="va">True</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>use_gae <span class="op">=</span> <span class="va">False</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>need_to_train_gae <span class="op">=</span> (use_emb <span class="kw">or</span> use_gae) <span class="kw">and</span> use_emb <span class="op">!=</span> use_gae</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># If the reconstruction loss needs to be computed.</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>recon <span class="op">=</span> use_gae <span class="kw">and</span> <span class="kw">not</span> use_emb </span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="co"># These are training GAE hyperparameters needed for training</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Distance_type in ['gaussian', 'alpha_decay'], and Gaussian scale</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>distance_type <span class="op">=</span> <span class="st">'gaussian'</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>rbf_length_scale<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> setup_distance(distance_type, rbf_length_scale<span class="op">=</span>rbf_length_scale)</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="co">#Can be changed depending on the dataset</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>n_epochs_emb <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>samples_size_emb <span class="op">=</span> (<span class="dv">30</span>, )</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Layers for the Geodesic Autoencoder</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>gae_embedded_dim <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>encoder_layers <span class="op">=</span> [model_features, <span class="dv">8</span>, gae_embedded_dim]</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>gae <span class="op">=</span> Autoencoder(</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    encoder_layers <span class="op">=</span> encoder_layers,</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    decoder_layers <span class="op">=</span> encoder_layers[::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">'ReLU'</span>, use_cuda <span class="op">=</span> use_cuda</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(gae.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Actually train the GAE</p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Added in extra cell just for iterative programming / running of code</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   but could be added to code block above</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> need_to_train_gae:</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    start_time_geo <span class="op">=</span> time.time()</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> train_ae(</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        gae, df, groups, optimizer, </span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        n_epochs<span class="op">=</span>n_epochs_emb, sample_size<span class="op">=</span>samples_size_emb,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        noise_min_scale<span class="op">=</span><span class="fl">0.09</span>, noise_max_scale<span class="op">=</span><span class="fl">0.15</span>, </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        dist<span class="op">=</span>dist, recon<span class="op">=</span>recon, use_cuda<span class="op">=</span>use_cuda,</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    run_time_geo <span class="op">=</span> time.time() <span class="op">-</span> start_time_geo</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(run_time_geo)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    autoencoder <span class="op">=</span> gae</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    autoencoder <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f8c3e81070664e899c694e6583d94442","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train loss dist: 8.06369
Train loss dist: 5.86898
Train loss dist: 4.80734
Train loss dist: 4.44481
Train loss dist: 3.72138
Train loss dist: 3.4606
Train loss dist: 3.55053
Train loss dist: 3.68203
Train loss dist: 3.43065
Train loss dist: 3.38494
Train loss dist: 3.22954
Train loss dist: 2.94018
Train loss dist: 2.84655
Train loss dist: 2.86098
Train loss dist: 2.86014
Train loss dist: 2.77656
Train loss dist: 2.46977
Train loss dist: 2.62306
Train loss dist: 2.48746
Train loss dist: 2.3401
37.821186542510986</code></pre>
</div>
</div>
<p>Specify Parameters</p>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>set_seeds(<span class="dv">10</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Directory where results are saved</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>exp_name <span class="op">=</span> <span class="st">'test'</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># density loss knn</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>use_density_loss <span class="op">=</span> <span class="va">True</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Weight of density (not percentage of total loss)</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>lambda_density <span class="op">=</span> <span class="dv">35</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co"># For petal=LeakyReLU / dyngen=CELU</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> <span class="st">'LeakyReLU'</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Can change but we never really do, mostly depends on the dataset.</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">16</span>]</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale of the noise in the trajectories. Either len(groups)*[float] or None. Should be None if using an adaptative ODE solver.</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>sde_scales <span class="op">=</span> <span class="bu">len</span>(groups)<span class="op">*</span>[<span class="fl">0.1</span>] </span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> recon:    </span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>    model_features <span class="op">=</span> gae_embedded_dim</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    model_features, layers, </span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span>activation, scales<span class="op">=</span>sde_scales, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basically "batch size"</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>sample_size<span class="op">=</span>(<span class="dv">60</span>, )</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Training specification</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>n_local_epochs <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>n_post_local_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the reverse trajectories to train</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>reverse_schema <span class="op">=</span> <span class="va">True</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co"># each reverse_n epoch</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>reverse_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>criterion_name <span class="op">=</span> <span class="st">'ot'</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> config_criterion(criterion_name)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters())</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Bookkeeping variables</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>batch_losses <span class="op">=</span> []</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>globe_losses <span class="op">=</span> []</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>    local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups)}</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a><span class="co"># For creating output.</span></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>n_points <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>n_trajectories <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>n_bins <span class="op">=</span> <span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-57" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>opts <span class="op">=</span> {</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'phate_dims'</span>: phate_dims,</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_cuda'</span>: use_cuda,</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'model_features'</span>: model_features,</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'exp_name'</span>: exp_name,</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'groups'</span>: groups,</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sample_size'</span>: sample_size,</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_emb'</span>: use_emb,</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_local_epochs'</span>: n_local_epochs,</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_epochs'</span>: n_epochs,</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_post_local_epochs'</span>: n_post_local_epochs,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'criterion_name'</span>: criterion_name,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hold_one_out'</span>: hold_one_out,</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_density_loss'</span>: use_density_loss,</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_points'</span>: n_points,</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_trajectories'</span>: n_trajectories,</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_bins'</span>: n_bins,</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'autoencoder'</span>: autoencoder,</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'activation_ode'</span>: activation,</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'layer'</span>: layers,</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lambda_density'</span>:lambda_density,</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_gae'</span>: use_gae,</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sde_scales'</span>: sde_scales,</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hold_out'</span>:hold_out,</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'encoder_layers'</span>: encoder_layers,</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_epochs_emb'</span>: n_epochs_emb,</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'samples_size_emb'</span>: samples_size_emb,</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'recon'</span>: recon,</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'distance_type'</span>:distance_type,</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rbf_length_scale'</span>:rbf_length_scale,</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'reverse_schema'</span>: reverse_schema,</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'reverse_n'</span>: reverse_n</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-58" class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>local_losses, batch_losses, globe_losses <span class="op">=</span> training_regimen(</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># local, global, local train structure</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    n_local_epochs<span class="op">=</span>n_local_epochs, </span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    n_epochs<span class="op">=</span>n_epochs, </span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    n_post_local_epochs<span class="op">=</span>n_post_local_epochs,</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># where results are stored</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    exp_dir<span class="op">=</span><span class="st">"../../results/"</span>, </span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">BEGIN</span><span class="co">: train params</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model, df<span class="op">=</span>df, groups<span class="op">=</span>groups, optimizer<span class="op">=</span>optimizer, </span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span>criterion, use_cuda<span class="op">=</span>use_cuda,</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out,</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    use_density_loss<span class="op">=</span>use_density_loss, </span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    lambda_density<span class="op">=</span>lambda_density,</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span>autoencoder, use_emb<span class="op">=</span>use_emb, use_gae<span class="op">=</span>use_gae, </span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>sample_size, logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>    reverse_schema<span class="op">=</span>reverse_schema, reverse_n<span class="op">=</span>reverse_n,</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>    use_penalty<span class="op">=</span><span class="va">True</span>, lambda_energy<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">END</span><span class="co">: train params</span></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>    plot_every<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    n_points<span class="op">=</span>n_points, n_trajectories<span class="op">=</span>n_trajectories, n_bins<span class="op">=</span>n_bins, </span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#local_losses=local_losses, batch_losses=batch_losses, globe_losses=globe_losses</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>run_time <span class="op">=</span> time.time() <span class="op">-</span> start_time <span class="op">+</span> run_time_geo <span class="cf">if</span> use_emb <span class="kw">or</span> use_gae <span class="cf">else</span> time.time() <span class="op">-</span> start_time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d1d0ccc6bbc24f7ea748923102e65e55","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ac72d52b973646729f54bff89c7ed4ac","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-error">
<pre><code>AttributeError: 'tuple' object has no attribute 'is_cuda'</code></pre>
</div>
</div>
<div id="cell-59" class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>plot_losses(</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    local_losses, batch_losses, globe_losses, </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'losses.png'</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-43-output-1.png" class="img-fluid"></p>
</div>
</div>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    model, df, n_points, n_trajectories, n_bins, use_cuda<span class="op">=</span>use_cuda, samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-61" class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>plot_comparision(</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    df, generated, trajectories,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'2d_comparision.png'</span>,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-45-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
</section>
<section id="modified-training-for-neural-flattening" class="level1">
<h1>Modified Training for Neural Flattening</h1>
<p>Here we make our mark on the previously untouched code, by modifying the guts of MIOFlow to make it a <em>neural flattener</em>. This means:</p>
<ol type="1">
<li>Instead of taking a list of pointclouds, including intermediate timescales, it only takes one pointcloud.</li>
<li>The <span class="math inline">\(L_e\)</span> loss will be replaced by the negative entropy of the local density.</li>
<li>Local and global training are now the same and will be combined.</li>
<li>(Optional) An additional loss will be added which preserves the radial distances between a center point and the rest of the dataset.</li>
</ol>
<section id="a-uniform-density-loss" class="level2">
<h2 class="anchored" data-anchor-id="a-uniform-density-loss">A Uniform Density Loss</h2>
<p>There are many possible ways to regularize the points to be uniformly distributed. The below has the advantages of being distance based, not kernel based. However, it remains to be seen whether gradients flow through this well. Minimizing the variance will penalize it to make the mean distances similar. Hopefully it sees that this can be easily achieved by flowing points out of dense regions into sparser ones.</p>
<p><span class="math display">\[
L_{ud} = \text{var}_{x_{i}}(\mathbb{E}_{x_{j}\in N_{k}(x_{i})}\|x_{i}-x_{j}\|_{2})
\]</span></p>
<p>::: {#cell-65 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UniformDensityLoss(nn.Module):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co">        Penalizes the input points X to be uniformly distributed in the space.</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co">        The penalty is the variance of the mean distance between the top_k nearest neighbors of each point. </span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co">        With uniform distribution, the mean distance should be the same for all points, and thus the variance should be 0.</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, X, top_k <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(source, target)</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        c_dist <span class="op">=</span> torch.cdist(X, X) </span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        vals, inds <span class="op">=</span> torch.topk(c_dist, <span class="dv">10</span>, dim<span class="op">=</span><span class="dv">1</span>, largest<span class="op">=</span><span class="va">False</span>, <span class="bu">sorted</span><span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.var(torch.mean(vals, dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="a-maximum-mean-discrepancy-loss" class="level2">
<h2 class="anchored" data-anchor-id="a-maximum-mean-discrepancy-loss">A Maximum Mean Discrepancy Loss</h2>
<p>(In Progress)</p>
<p>::: {#cell-68 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MMD_loss_to_uniform(nn.Module):</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co">    https://github.com/ZongxianLee/MMD_Loss.Pytorch/blob/master/mmd_loss.py</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, kernel_mul <span class="op">=</span> <span class="fl">2.0</span>, kernel_num <span class="op">=</span> <span class="dv">5</span>, use_cuda<span class="op">=</span>torch.cuda.is_available()):</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MMD_loss_to_uniform, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel_num <span class="op">=</span> kernel_num</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel_mul <span class="op">=</span> kernel_mul</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fix_sigma <span class="op">=</span> <span class="va">None</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.uniformity <span class="op">=</span> torch.rand(<span class="dv">10000</span>,<span class="dv">2</span>,device<span class="op">=</span><span class="st">'cuda'</span>) <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.rand(<span class="dv">10000</span>,<span class="dv">2</span>)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gaussian_kernel(<span class="va">self</span>, source, target, kernel_mul<span class="op">=</span><span class="fl">2.0</span>, kernel_num<span class="op">=</span><span class="dv">5</span>, fix_sigma<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> <span class="bu">int</span>(source.size()[<span class="dv">0</span>])<span class="op">+</span><span class="bu">int</span>(target.size()[<span class="dv">0</span>])</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> torch.cat([source, target], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>        total0 <span class="op">=</span> total.unsqueeze(<span class="dv">0</span>).expand(<span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">1</span>)))</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>        total1 <span class="op">=</span> total.unsqueeze(<span class="dv">1</span>).expand(<span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">0</span>)), <span class="bu">int</span>(total.size(<span class="dv">1</span>)))</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>        L2_distance <span class="op">=</span> ((total0<span class="op">-</span>total1)<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">2</span>) </span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> fix_sigma:</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>            bandwidth <span class="op">=</span> fix_sigma</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>            bandwidth <span class="op">=</span> torch.<span class="bu">sum</span>(L2_distance.data) <span class="op">/</span> (n_samples<span class="op">**</span><span class="dv">2</span><span class="op">-</span>n_samples)</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>        bandwidth <span class="op">/=</span> kernel_mul <span class="op">**</span> (kernel_num <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>        bandwidth_list <span class="op">=</span> [bandwidth <span class="op">*</span> (kernel_mul<span class="op">**</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(kernel_num)]</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>        kernel_val <span class="op">=</span> [torch.exp(<span class="op">-</span>L2_distance <span class="op">/</span> bandwidth_temp) <span class="cf">for</span> bandwidth_temp <span class="kw">in</span> bandwidth_list]</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(kernel_val)</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, source):</span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> <span class="va">self</span>.uniformity</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> <span class="bu">int</span>(source.size()[<span class="dv">0</span>])</span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>        kernels <span class="op">=</span> <span class="va">self</span>.gaussian_kernel(source, target, kernel_mul<span class="op">=</span><span class="va">self</span>.kernel_mul, kernel_num<span class="op">=</span><span class="va">self</span>.kernel_num, fix_sigma<span class="op">=</span><span class="va">self</span>.fix_sigma)</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>        XX <span class="op">=</span> torch.mean(kernels[:batch_size, :batch_size])</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>        YY <span class="op">=</span> torch.mean(kernels[batch_size:, batch_size:])</span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>        XY <span class="op">=</span> torch.mean(kernels[:batch_size, batch_size:])</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>        YX <span class="op">=</span> torch.mean(kernels[batch_size:, :batch_size])</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> XX <span class="op">+</span> YY <span class="op">-</span> XY <span class="op">-</span> YX</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="training-functions" class="level2">
<h2 class="anchored" data-anchor-id="training-functions">Training Functions</h2>
<p>::: {#cell-70 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, sys, json, math, itertools</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd, numpy <span class="im">as</span> np</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># from tqdm import tqdm</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.utils import sample, generate_steps</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.losses import MMD_loss, OT_loss, Density_loss, Local_density_loss</span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_neural_flattener(</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>    model, df, groups, optimizer, n_batches<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    use_cuda<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> UniformDensityLoss(),</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>(<span class="dv">100</span>, ),</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>    hold_one_out<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>    hold_out<span class="op">=</span><span class="st">'random'</span>,</span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>    apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>    top_k <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>    hinge_value <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>    use_density_loss<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use_local_density=False,</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>    lambda_density <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a>    use_emb<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>    use_gae<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a>    use_gaussian:<span class="bu">bool</span><span class="op">=</span><span class="va">True</span>, </span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>    add_noise:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>, </span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>    noise_scale:<span class="bu">float</span><span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a>    use_penalty<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a>    use_radial_distance_loss<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a>    lambda_energy<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a>    reverse:<span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span></span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a><span class="co">    MIOFlow training loop</span></span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a><span class="co">    Notes:</span></span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a><span class="co">        - The argument `model` must have a method `forward` that accepts two arguments</span></span>
<span id="cb52-55"><a href="#cb52-55" aria-hidden="true" tabindex="-1"></a><span class="co">            in its function signature:</span></span>
<span id="cb52-56"><a href="#cb52-56" aria-hidden="true" tabindex="-1"></a><span class="co">                ```python</span></span>
<span id="cb52-57"><a href="#cb52-57" aria-hidden="true" tabindex="-1"></a><span class="co">                model.forward(x, t)</span></span>
<span id="cb52-58"><a href="#cb52-58" aria-hidden="true" tabindex="-1"></a><span class="co">                ```</span></span>
<span id="cb52-59"><a href="#cb52-59" aria-hidden="true" tabindex="-1"></a><span class="co">            where, `x` is the input tensor and `t` is a `torch.Tensor` of time points (float).</span></span>
<span id="cb52-60"><a href="#cb52-60" aria-hidden="true" tabindex="-1"></a><span class="co">        - The training loop is divided in two parts; local (predict t+1 from t), and global (predict the entire trajectory).</span></span>
<span id="cb52-61"><a href="#cb52-61" aria-hidden="true" tabindex="-1"></a><span class="co">                        </span></span>
<span id="cb52-62"><a href="#cb52-62" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb52-63"><a href="#cb52-63" aria-hidden="true" tabindex="-1"></a><span class="co">        model (nn.Module): the initialized pytorch ODE model.</span></span>
<span id="cb52-64"><a href="#cb52-64" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-65"><a href="#cb52-65" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): the DataFrame from which to extract batch data.</span></span>
<span id="cb52-66"><a href="#cb52-66" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-67"><a href="#cb52-67" aria-hidden="true" tabindex="-1"></a><span class="co">        groups (list): the list of the numerical groups in the data, e.g. </span></span>
<span id="cb52-68"><a href="#cb52-68" aria-hidden="true" tabindex="-1"></a><span class="co">            `[1.0, 2.0, 3.0, 4.0, 5.0]`, if the data has five groups.</span></span>
<span id="cb52-69"><a href="#cb52-69" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb52-70"><a href="#cb52-70" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer (torch.optim): an optimizer initilized with the model's parameters.</span></span>
<span id="cb52-71"><a href="#cb52-71" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-72"><a href="#cb52-72" aria-hidden="true" tabindex="-1"></a><span class="co">        n_batches (int): Default to '20', the number of batches from which to randomly sample each consecutive pair</span></span>
<span id="cb52-73"><a href="#cb52-73" aria-hidden="true" tabindex="-1"></a><span class="co">            of groups.</span></span>
<span id="cb52-74"><a href="#cb52-74" aria-hidden="true" tabindex="-1"></a><span class="co">                    </span></span>
<span id="cb52-75"><a href="#cb52-75" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to send the model and data to cuda. </span></span>
<span id="cb52-76"><a href="#cb52-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-77"><a href="#cb52-77" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_size (tuple): Defaults to `(100, )`</span></span>
<span id="cb52-78"><a href="#cb52-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-79"><a href="#cb52-79" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to sample data points with replacement.</span></span>
<span id="cb52-80"><a href="#cb52-80" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-81"><a href="#cb52-81" aria-hidden="true" tabindex="-1"></a><span class="co">        local_loss (bool): Defaults to `True`. Whether or not to use a local loss in the model.</span></span>
<span id="cb52-82"><a href="#cb52-82" aria-hidden="true" tabindex="-1"></a><span class="co">            See notes for more detail.</span></span>
<span id="cb52-83"><a href="#cb52-83" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb52-84"><a href="#cb52-84" aria-hidden="true" tabindex="-1"></a><span class="co">        global_loss (bool): Defaults to `False`. Whether or not to use a global loss in the model.</span></span>
<span id="cb52-85"><a href="#cb52-85" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-86"><a href="#cb52-86" aria-hidden="true" tabindex="-1"></a><span class="co">        hold_one_out (bool): Defaults to `False`. Whether or not to randomly hold one time pair</span></span>
<span id="cb52-87"><a href="#cb52-87" aria-hidden="true" tabindex="-1"></a><span class="co">            e.g. t_1 to t_2 out when computing the global loss.</span></span>
<span id="cb52-88"><a href="#cb52-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-89"><a href="#cb52-89" aria-hidden="true" tabindex="-1"></a><span class="co">        hold_out (str | int): Defaults to `"random"`. Which time point to hold out when calculating the</span></span>
<span id="cb52-90"><a href="#cb52-90" aria-hidden="true" tabindex="-1"></a><span class="co">            global loss.</span></span>
<span id="cb52-91"><a href="#cb52-91" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb52-92"><a href="#cb52-92" aria-hidden="true" tabindex="-1"></a><span class="co">        apply_losses_in_time (bool): Defaults to `True`. Applies the losses and does back propegation</span></span>
<span id="cb52-93"><a href="#cb52-93" aria-hidden="true" tabindex="-1"></a><span class="co">            as soon as a loss is calculated. See notes for more detail.</span></span>
<span id="cb52-94"><a href="#cb52-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-95"><a href="#cb52-95" aria-hidden="true" tabindex="-1"></a><span class="co">        top_k (int): Default to '5'. The k for the k-NN used in the density loss.</span></span>
<span id="cb52-96"><a href="#cb52-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-97"><a href="#cb52-97" aria-hidden="true" tabindex="-1"></a><span class="co">        hinge_value (float): Defaults to `0.01`. The hinge value for density loss.</span></span>
<span id="cb52-98"><a href="#cb52-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-99"><a href="#cb52-99" aria-hidden="true" tabindex="-1"></a><span class="co">        use_density_loss (bool): Defaults to `True`. Whether or not to add density regularization.</span></span>
<span id="cb52-100"><a href="#cb52-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-101"><a href="#cb52-101" aria-hidden="true" tabindex="-1"></a><span class="co">        lambda_density (float): Defaults to `1.0`. The weight for density loss.</span></span>
<span id="cb52-102"><a href="#cb52-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-103"><a href="#cb52-103" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (NoneType|nn.Module): Default to 'None'. The full geodesic Autoencoder.</span></span>
<span id="cb52-104"><a href="#cb52-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-105"><a href="#cb52-105" aria-hidden="true" tabindex="-1"></a><span class="co">        use_emb (bool): Defaults to `True`. Whether or not to use the embedding model.</span></span>
<span id="cb52-106"><a href="#cb52-106" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-107"><a href="#cb52-107" aria-hidden="true" tabindex="-1"></a><span class="co">        use_gae (bool): Defaults to `False`. Whether or not to use the full Geodesic AutoEncoder.</span></span>
<span id="cb52-108"><a href="#cb52-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-109"><a href="#cb52-109" aria-hidden="true" tabindex="-1"></a><span class="co">        use_gaussian (bool): Defaults to `True`. Whether to use random or gaussian noise.</span></span>
<span id="cb52-110"><a href="#cb52-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-111"><a href="#cb52-111" aria-hidden="true" tabindex="-1"></a><span class="co">        add_noise (bool): Defaults to `False`. Whether or not to add noise.</span></span>
<span id="cb52-112"><a href="#cb52-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-113"><a href="#cb52-113" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_scale (float): Defaults to `0.30`. How much to scale the noise by.</span></span>
<span id="cb52-114"><a href="#cb52-114" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-115"><a href="#cb52-115" aria-hidden="true" tabindex="-1"></a><span class="co">        logger (NoneType|Logger): Default to 'None'. The logger to record information.</span></span>
<span id="cb52-116"><a href="#cb52-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-117"><a href="#cb52-117" aria-hidden="true" tabindex="-1"></a><span class="co">        use_penalty (bool): Defaults to `False`. Whether or not to use $L_e$ during training (norm of the derivative).</span></span>
<span id="cb52-118"><a href="#cb52-118" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb52-119"><a href="#cb52-119" aria-hidden="true" tabindex="-1"></a><span class="co">        lambda_energy (float): Default to '1.0'. The weight of the energy penalty.</span></span>
<span id="cb52-120"><a href="#cb52-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-121"><a href="#cb52-121" aria-hidden="true" tabindex="-1"></a><span class="co">        reverse (bool): Whether to train time backwards.</span></span>
<span id="cb52-122"><a href="#cb52-122" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb52-123"><a href="#cb52-123" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> (use_emb <span class="kw">or</span> use_gae):</span>
<span id="cb52-124"><a href="#cb52-124" aria-hidden="true" tabindex="-1"></a>        use_emb <span class="op">=</span> <span class="va">False</span></span>
<span id="cb52-125"><a href="#cb52-125" aria-hidden="true" tabindex="-1"></a>        use_gae <span class="op">=</span> <span class="va">False</span></span>
<span id="cb52-126"><a href="#cb52-126" aria-hidden="true" tabindex="-1"></a>        warnings.warn(<span class="st">'</span><span class="ch">\'</span><span class="st">autoencoder</span><span class="ch">\'</span><span class="st"> is </span><span class="ch">\'</span><span class="st">None</span><span class="ch">\'</span><span class="st">, but </span><span class="ch">\'</span><span class="st">use_emb</span><span class="ch">\'</span><span class="st"> or </span><span class="ch">\'</span><span class="st">use_gae</span><span class="ch">\'</span><span class="st"> is True, both will be set to False.'</span>)</span>
<span id="cb52-127"><a href="#cb52-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-128"><a href="#cb52-128" aria-hidden="true" tabindex="-1"></a>    noise_fn <span class="op">=</span> torch.randn <span class="cf">if</span> use_gaussian <span class="cf">else</span> torch.rand</span>
<span id="cb52-129"><a href="#cb52-129" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> noise(data):</span>
<span id="cb52-130"><a href="#cb52-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> noise_fn(<span class="op">*</span>data.shape).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> noise_fn(<span class="op">*</span>data.shape)</span>
<span id="cb52-131"><a href="#cb52-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the indicies for the steps that should be used</span></span>
<span id="cb52-132"><a href="#cb52-132" aria-hidden="true" tabindex="-1"></a>    steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb52-133"><a href="#cb52-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-134"><a href="#cb52-134" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> reverse:</span>
<span id="cb52-135"><a href="#cb52-135" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> groups[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb52-136"><a href="#cb52-136" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb52-137"><a href="#cb52-137" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-138"><a href="#cb52-138" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Storage variables for losses</span></span>
<span id="cb52-139"><a href="#cb52-139" aria-hidden="true" tabindex="-1"></a>    batch_losses <span class="op">=</span> []</span>
<span id="cb52-140"><a href="#cb52-140" aria-hidden="true" tabindex="-1"></a>    globe_losses <span class="op">=</span> []</span>
<span id="cb52-141"><a href="#cb52-141" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb52-142"><a href="#cb52-142" aria-hidden="true" tabindex="-1"></a>        groups_ho <span class="op">=</span> [g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out]</span>
<span id="cb52-143"><a href="#cb52-143" aria-hidden="true" tabindex="-1"></a>        local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups_ho) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb52-144"><a href="#cb52-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb52-145"><a href="#cb52-145" aria-hidden="true" tabindex="-1"></a>        local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> steps}</span>
<span id="cb52-146"><a href="#cb52-146" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-147"><a href="#cb52-147" aria-hidden="true" tabindex="-1"></a>    density_fn <span class="op">=</span> Density_loss(hinge_value) <span class="co"># if not use_local_density else Local_density_loss()</span></span>
<span id="cb52-148"><a href="#cb52-148" aria-hidden="true" tabindex="-1"></a>    uniform_density_loss <span class="op">=</span> UniformDensityLoss()</span>
<span id="cb52-149"><a href="#cb52-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-150"><a href="#cb52-150" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Send model to cuda and specify it as training mode</span></span>
<span id="cb52-151"><a href="#cb52-151" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda:</span>
<span id="cb52-152"><a href="#cb52-152" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb52-153"><a href="#cb52-153" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-154"><a href="#cb52-154" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb52-155"><a href="#cb52-155" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-156"><a href="#cb52-156" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_batches)):</span>
<span id="cb52-157"><a href="#cb52-157" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-158"><a href="#cb52-158" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb52-159"><a href="#cb52-159" aria-hidden="true" tabindex="-1"></a>        <span class="co">#sampling, predicting, and evaluating the loss.</span></span>
<span id="cb52-160"><a href="#cb52-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-161"><a href="#cb52-161" aria-hidden="true" tabindex="-1"></a>        <span class="co"># data_ti is a list of sampled tensors from each provided timestep.</span></span>
<span id="cb52-162"><a href="#cb52-162" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In our case, there is only one timestep, so it is a list of one tensor.</span></span>
<span id="cb52-163"><a href="#cb52-163" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sample data from input pointcloud; looks like</span></span>
<span id="cb52-164"><a href="#cb52-164" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [timestep1, timestep2, timestep3, ...]</span></span>
<span id="cb52-165"><a href="#cb52-165" aria-hidden="true" tabindex="-1"></a>        data_ti <span class="op">=</span> []</span>
<span id="cb52-166"><a href="#cb52-166" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> group <span class="kw">in</span> groups:</span>
<span id="cb52-167"><a href="#cb52-167" aria-hidden="true" tabindex="-1"></a>            samples, radial_dists <span class="op">=</span> sample(</span>
<span id="cb52-168"><a href="#cb52-168" aria-hidden="true" tabindex="-1"></a>                df, group, size<span class="op">=</span>sample_size, replace<span class="op">=</span>sample_with_replacement, </span>
<span id="cb52-169"><a href="#cb52-169" aria-hidden="true" tabindex="-1"></a>                to_torch<span class="op">=</span><span class="va">True</span>, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb52-170"><a href="#cb52-170" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb52-171"><a href="#cb52-171" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("samples shape", samples.shape)</span></span>
<span id="cb52-172"><a href="#cb52-172" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("samples equal radial dists", (samples[:,-1] == radial_dists).all())</span></span>
<span id="cb52-173"><a href="#cb52-173" aria-hidden="true" tabindex="-1"></a>            data_ti.append(samples)</span>
<span id="cb52-174"><a href="#cb52-174" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb52-175"><a href="#cb52-175" aria-hidden="true" tabindex="-1"></a>        <span class="co"># data_ti = [</span></span>
<span id="cb52-176"><a href="#cb52-176" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     sample(</span></span>
<span id="cb52-177"><a href="#cb52-177" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         df, group, size=sample_size, replace=sample_with_replacement, </span></span>
<span id="cb52-178"><a href="#cb52-178" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         to_torch=True, use_cuda=use_cuda</span></span>
<span id="cb52-179"><a href="#cb52-179" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     )</span></span>
<span id="cb52-180"><a href="#cb52-180" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     for group in groups</span></span>
<span id="cb52-181"><a href="#cb52-181" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ]</span></span>
<span id="cb52-182"><a href="#cb52-182" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 0 is the input data, 1 is our desired transformation</span></span>
<span id="cb52-183"><a href="#cb52-183" aria-hidden="true" tabindex="-1"></a>        time <span class="op">=</span> torch.Tensor([<span class="dv">0</span>,<span class="dv">1</span>]).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.Tensor([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb52-184"><a href="#cb52-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-185"><a href="#cb52-185" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print("data_ti shape", data_ti[0].shape, data_ti[0:5])</span></span>
<span id="cb52-186"><a href="#cb52-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-187"><a href="#cb52-187" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  Add noise to data_ti and project it into the latent space.</span></span>
<span id="cb52-188"><a href="#cb52-188" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_noise:</span>
<span id="cb52-189"><a href="#cb52-189" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("adding noise")</span></span>
<span id="cb52-190"><a href="#cb52-190" aria-hidden="true" tabindex="-1"></a>            data_ti <span class="op">=</span> [</span>
<span id="cb52-191"><a href="#cb52-191" aria-hidden="true" tabindex="-1"></a>                data <span class="op">+</span> noise(data) <span class="op">*</span> noise_scale <span class="cf">for</span> data <span class="kw">in</span> data_ti</span>
<span id="cb52-192"><a href="#cb52-192" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb52-193"><a href="#cb52-193" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> use_gae:</span>
<span id="cb52-194"><a href="#cb52-194" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("using autoencoder before neural ode")</span></span>
<span id="cb52-195"><a href="#cb52-195" aria-hidden="true" tabindex="-1"></a>            data_ti <span class="op">=</span> [autoencoder.encoder(data) <span class="cf">for</span> data <span class="kw">in</span> data_ti]</span>
<span id="cb52-196"><a href="#cb52-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-197"><a href="#cb52-197" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print("data ti shape", data_ti[0].shape)</span></span>
<span id="cb52-198"><a href="#cb52-198" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-199"><a href="#cb52-199" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prediction: run the first samples through the neural ODE. I believe it returns a list of tensors, one for each time.</span></span>
<span id="cb52-200"><a href="#cb52-200" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In our case, the only timestep is the first one, so it is a list of one tensor.f </span></span>
<span id="cb52-201"><a href="#cb52-201" aria-hidden="true" tabindex="-1"></a>        data_tp <span class="op">=</span> model(data_ti[<span class="dv">0</span>], time, return_whole_sequence<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb52-202"><a href="#cb52-202" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Why on earth does it use the autoencoder *after* the neural ODE?</span></span>
<span id="cb52-203"><a href="#cb52-203" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> use_emb:  <span class="co"># ???? What's use_emb?</span></span>
<span id="cb52-204"><a href="#cb52-204" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("using autoencoder after neural ode")</span></span>
<span id="cb52-205"><a href="#cb52-205" aria-hidden="true" tabindex="-1"></a>            data_tp <span class="op">=</span> [autoencoder.encoder(data) <span class="cf">for</span> data <span class="kw">in</span> data_tp]</span>
<span id="cb52-206"><a href="#cb52-206" aria-hidden="true" tabindex="-1"></a>            data_ti <span class="op">=</span> [autoencoder.encoder(data) <span class="cf">for</span> data <span class="kw">in</span> data_ti]</span>
<span id="cb52-207"><a href="#cb52-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-208"><a href="#cb52-208" aria-hidden="true" tabindex="-1"></a>        <span class="co">#ignoring one time point</span></span>
<span id="cb52-209"><a href="#cb52-209" aria-hidden="true" tabindex="-1"></a>        to_ignore <span class="op">=</span> <span class="va">None</span> <span class="co">#TODO: This assignment of `to_ingnore`, could be moved at the beginning of the function. </span></span>
<span id="cb52-210"><a href="#cb52-210" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="op">==</span> <span class="st">'random'</span>:</span>
<span id="cb52-211"><a href="#cb52-211" aria-hidden="true" tabindex="-1"></a>            to_ignore <span class="op">=</span> np.random.choice(groups)</span>
<span id="cb52-212"><a href="#cb52-212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb52-213"><a href="#cb52-213" aria-hidden="true" tabindex="-1"></a>            to_ignore <span class="op">=</span> hold_out</span>
<span id="cb52-214"><a href="#cb52-214" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> hold_one_out:</span>
<span id="cb52-215"><a href="#cb52-215" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'Unknown group to hold out'</span>)</span>
<span id="cb52-216"><a href="#cb52-216" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb52-217"><a href="#cb52-217" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb52-218"><a href="#cb52-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-219"><a href="#cb52-219" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is L_e; we replace this with the uniform density loss</span></span>
<span id="cb52-220"><a href="#cb52-220" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(data_tp[<span class="op">-</span><span class="dv">1</span>]) </span>
<span id="cb52-221"><a href="#cb52-221" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print('uniform density', loss, type(loss))</span></span>
<span id="cb52-222"><a href="#cb52-222" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss = sum([</span></span>
<span id="cb52-223"><a href="#cb52-223" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     criterion(data_tp[i], data_ti[i]) # criterion is an input parameter</span></span>
<span id="cb52-224"><a href="#cb52-224" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     for i in range(1, len(groups))</span></span>
<span id="cb52-225"><a href="#cb52-225" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     if groups[i] != to_ignore # nice double nested list comprehension</span></span>
<span id="cb52-226"><a href="#cb52-226" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ])</span></span>
<span id="cb52-227"><a href="#cb52-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-228"><a href="#cb52-228" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This second density loss </span></span>
<span id="cb52-229"><a href="#cb52-229" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_density_loss:                </span>
<span id="cb52-230"><a href="#cb52-230" aria-hidden="true" tabindex="-1"></a>            density_loss <span class="op">=</span> density_fn(data_tp[<span class="op">-</span><span class="dv">1</span>], data_ti[<span class="dv">0</span>], <span class="va">None</span>, to_ignore, top_k)</span>
<span id="cb52-231"><a href="#cb52-231" aria-hidden="true" tabindex="-1"></a>            density_loss <span class="op">=</span> density_loss.to(loss.device)</span>
<span id="cb52-232"><a href="#cb52-232" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> lambda_density <span class="op">*</span> density_loss</span>
<span id="cb52-233"><a href="#cb52-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-234"><a href="#cb52-234" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print('density loss', loss, type(loss))</span></span>
<span id="cb52-235"><a href="#cb52-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-236"><a href="#cb52-236" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is L_w, the wasserstein distance traversed by the model flows.</span></span>
<span id="cb52-237"><a href="#cb52-237" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_penalty:</span>
<span id="cb52-238"><a href="#cb52-238" aria-hidden="true" tabindex="-1"></a>            penalty <span class="op">=</span> <span class="bu">sum</span>([model.norm[<span class="op">-</span>(i<span class="op">+</span><span class="dv">1</span>)] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(groups))</span>
<span id="cb52-239"><a href="#cb52-239" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> groups[i] <span class="op">!=</span> to_ignore])</span>
<span id="cb52-240"><a href="#cb52-240" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> lambda_energy <span class="op">*</span> penalty</span>
<span id="cb52-241"><a href="#cb52-241" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-242"><a href="#cb52-242" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print('penalty loss', loss, type(loss))</span></span>
<span id="cb52-243"><a href="#cb52-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-244"><a href="#cb52-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-245"><a href="#cb52-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-246"><a href="#cb52-246" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is the radial distance preservation loss;</span></span>
<span id="cb52-247"><a href="#cb52-247" aria-hidden="true" tabindex="-1"></a>        <span class="co"># At the final timestep, we want the radial distances from our center point</span></span>
<span id="cb52-248"><a href="#cb52-248" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to match the supplied radial distances</span></span>
<span id="cb52-249"><a href="#cb52-249" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_radial_distance_loss:</span>
<span id="cb52-250"><a href="#cb52-250" aria-hidden="true" tabindex="-1"></a>            embedding_distances <span class="op">=</span> torch.cdist(data_tp[<span class="op">-</span><span class="dv">1</span>], data_tp[<span class="op">-</span><span class="dv">1</span>][<span class="va">None</span>,:]) <span class="co"># torch.sqrt(torch.sum((x_embedded - x_embedded[0])**2, axis=-1)) # </span><span class="al">TODO</span><span class="co">: How will this play with batching?</span></span>
<span id="cb52-251"><a href="#cb52-251" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(embedding_distances)</span></span>
<span id="cb52-252"><a href="#cb52-252" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> nn.MSELoss()(embedding_distances, radial_dists)</span>
<span id="cb52-253"><a href="#cb52-253" aria-hidden="true" tabindex="-1"></a>               </span>
<span id="cb52-254"><a href="#cb52-254" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb52-255"><a href="#cb52-255" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb52-256"><a href="#cb52-256" aria-hidden="true" tabindex="-1"></a>        model.norm<span class="op">=</span>[]</span>
<span id="cb52-257"><a href="#cb52-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-258"><a href="#cb52-258" aria-hidden="true" tabindex="-1"></a>        globe_losses.append(loss.item())</span>
<span id="cb52-259"><a href="#cb52-259" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-260"><a href="#cb52-260" aria-hidden="true" tabindex="-1"></a>    print_loss <span class="op">=</span> globe_losses</span>
<span id="cb52-261"><a href="#cb52-261" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> logger <span class="kw">is</span> <span class="va">None</span>:      </span>
<span id="cb52-262"><a href="#cb52-262" aria-hidden="true" tabindex="-1"></a>        tqdm.write(<span class="ss">f'Train loss: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(np.mean(print_loss), <span class="dv">5</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb52-263"><a href="#cb52-263" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb52-264"><a href="#cb52-264" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f'Train loss: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(np.mean(print_loss), <span class="dv">5</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb52-265"><a href="#cb52-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-266"><a href="#cb52-266" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for compatibility with the rest of the code, we need to invent the local_loss and batch_loss</span></span>
<span id="cb52-267"><a href="#cb52-267" aria-hidden="true" tabindex="-1"></a>    local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[<span class="dv">0</span>]<span class="op">*</span>n_batches <span class="cf">for</span> (t0, t1) <span class="kw">in</span> steps}</span>
<span id="cb52-268"><a href="#cb52-268" aria-hidden="true" tabindex="-1"></a>    batch_losses <span class="op">=</span> [<span class="dv">0</span>]<span class="op">*</span>n_batches <span class="co"># list of num batches containing 0s</span></span>
<span id="cb52-269"><a href="#cb52-269" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> local_losses, batch_losses, globe_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-71 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.plots import plot_comparision, plot_losses</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from MIOFlow.eval import generate_plot_data</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_regimen_neural_flattener(</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    n_local_epochs, n_epochs, n_post_local_epochs,</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    exp_dir, </span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">BEGIN</span><span class="co">: train params</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    model, df, groups, optimizer, n_batches<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span>UniformDensityLoss(), use_cuda<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>    hold_one_out<span class="op">=</span><span class="va">False</span>, hold_out<span class="op">=</span><span class="st">'random'</span>, </span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>    hinge_value<span class="op">=</span><span class="fl">0.01</span>, use_density_loss<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>    top_k <span class="op">=</span> <span class="dv">5</span>, lambda_density <span class="op">=</span> <span class="fl">1.0</span>, </span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span><span class="va">None</span>, use_emb<span class="op">=</span><span class="va">True</span>, use_gae<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>(<span class="dv">100</span>, ), </span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>    add_noise<span class="op">=</span><span class="va">False</span>, noise_scale<span class="op">=</span><span class="fl">0.1</span>, use_gaussian<span class="op">=</span><span class="va">True</span>,  </span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    use_penalty<span class="op">=</span><span class="va">False</span>, lambda_energy<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">END</span><span class="co">: train params</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>    steps<span class="op">=</span><span class="va">None</span>, plot_every<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    n_points<span class="op">=</span><span class="dv">100</span>, n_trajectories<span class="op">=</span><span class="dv">100</span>, n_bins<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    local_losses<span class="op">=</span><span class="va">None</span>, batch_losses<span class="op">=</span><span class="va">None</span>, globe_losses<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>    reverse_schema<span class="op">=</span><span class="va">True</span>, reverse_n<span class="op">=</span><span class="dv">4</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(model)</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    recon <span class="op">=</span> use_gae <span class="kw">and</span> <span class="kw">not</span> use_emb</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> steps <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>        steps <span class="op">=</span> generate_steps(groups)</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> local_losses <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>            groups_ho <span class="op">=</span> [g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out]</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups_ho) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> reverse_schema:</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>                local_losses <span class="op">=</span> {</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>local_losses, </span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>{<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups_ho[::<span class="op">-</span><span class="dv">1</span>]) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups)}</span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> reverse_schema:</span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a>                local_losses <span class="op">=</span> {</span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>local_losses, </span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>                    <span class="op">**</span>{<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups[::<span class="op">-</span><span class="dv">1</span>])}</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch_losses <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a>        batch_losses <span class="op">=</span> []</span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> globe_losses <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a>        globe_losses <span class="op">=</span> []</span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a>    reverse <span class="op">=</span> <span class="va">False</span></span>
<span id="cb53-59"><a href="#cb53-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_local_epochs), desc<span class="op">=</span><span class="st">'Pretraining Epoch'</span>):</span>
<span id="cb53-60"><a href="#cb53-60" aria-hidden="true" tabindex="-1"></a>        reverse <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> reverse_schema <span class="kw">and</span> epoch <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">False</span></span>
<span id="cb53-61"><a href="#cb53-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-62"><a href="#cb53-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># l_loss = None,</span></span>
<span id="cb53-63"><a href="#cb53-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># b_loss = None,</span></span>
<span id="cb53-64"><a href="#cb53-64" aria-hidden="true" tabindex="-1"></a>        l_loss, b_loss, g_loss <span class="op">=</span> train_neural_flattener(</span>
<span id="cb53-65"><a href="#cb53-65" aria-hidden="true" tabindex="-1"></a>            model, df, groups, optimizer, n_batches, </span>
<span id="cb53-66"><a href="#cb53-66" aria-hidden="true" tabindex="-1"></a>            use_cuda <span class="op">=</span> use_cuda, apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb53-67"><a href="#cb53-67" aria-hidden="true" tabindex="-1"></a>            criterion<span class="op">=</span>criterion,</span>
<span id="cb53-68"><a href="#cb53-68" aria-hidden="true" tabindex="-1"></a>            hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out, </span>
<span id="cb53-69"><a href="#cb53-69" aria-hidden="true" tabindex="-1"></a>            hinge_value<span class="op">=</span>hinge_value,</span>
<span id="cb53-70"><a href="#cb53-70" aria-hidden="true" tabindex="-1"></a>            use_density_loss <span class="op">=</span> use_density_loss,    </span>
<span id="cb53-71"><a href="#cb53-71" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> top_k, lambda_density <span class="op">=</span> lambda_density, </span>
<span id="cb53-72"><a href="#cb53-72" aria-hidden="true" tabindex="-1"></a>            autoencoder <span class="op">=</span> autoencoder, use_emb <span class="op">=</span> use_emb, use_gae <span class="op">=</span> use_gae, sample_size<span class="op">=</span>sample_size, </span>
<span id="cb53-73"><a href="#cb53-73" aria-hidden="true" tabindex="-1"></a>            sample_with_replacement<span class="op">=</span>sample_with_replacement, logger<span class="op">=</span>logger,</span>
<span id="cb53-74"><a href="#cb53-74" aria-hidden="true" tabindex="-1"></a>            add_noise<span class="op">=</span>add_noise, noise_scale<span class="op">=</span>noise_scale, use_gaussian<span class="op">=</span>use_gaussian, </span>
<span id="cb53-75"><a href="#cb53-75" aria-hidden="true" tabindex="-1"></a>            use_penalty<span class="op">=</span>use_penalty, lambda_energy<span class="op">=</span>lambda_energy, reverse<span class="op">=</span>reverse</span>
<span id="cb53-76"><a href="#cb53-76" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb53-77"><a href="#cb53-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> l_loss.items():  </span>
<span id="cb53-78"><a href="#cb53-78" aria-hidden="true" tabindex="-1"></a>            local_losses[k].extend(v)</span>
<span id="cb53-79"><a href="#cb53-79" aria-hidden="true" tabindex="-1"></a>        batch_losses.extend(b_loss)</span>
<span id="cb53-80"><a href="#cb53-80" aria-hidden="true" tabindex="-1"></a>        globe_losses.extend(g_loss)</span>
<span id="cb53-81"><a href="#cb53-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> epoch <span class="op">%</span> plot_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb53-82"><a href="#cb53-82" aria-hidden="true" tabindex="-1"></a>            generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb53-83"><a href="#cb53-83" aria-hidden="true" tabindex="-1"></a>                model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb53-84"><a href="#cb53-84" aria-hidden="true" tabindex="-1"></a>                sample_with_replacement<span class="op">=</span>sample_with_replacement, use_cuda<span class="op">=</span>use_cuda, </span>
<span id="cb53-85"><a href="#cb53-85" aria-hidden="true" tabindex="-1"></a>                samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span>logger,</span>
<span id="cb53-86"><a href="#cb53-86" aria-hidden="true" tabindex="-1"></a>                autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb53-87"><a href="#cb53-87" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-88"><a href="#cb53-88" aria-hidden="true" tabindex="-1"></a>            plot_comparision(</span>
<span id="cb53-89"><a href="#cb53-89" aria-hidden="true" tabindex="-1"></a>                df, generated, trajectories,</span>
<span id="cb53-90"><a href="#cb53-90" aria-hidden="true" tabindex="-1"></a>                palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb53-91"><a href="#cb53-91" aria-hidden="true" tabindex="-1"></a>                save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb53-92"><a href="#cb53-92" aria-hidden="true" tabindex="-1"></a>                <span class="bu">file</span><span class="op">=</span><span class="ss">f'2d_comparision_local_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png'</span>,</span>
<span id="cb53-93"><a href="#cb53-93" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb53-94"><a href="#cb53-94" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-95"><a href="#cb53-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-96"><a href="#cb53-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_epochs), desc<span class="op">=</span><span class="st">'Epoch'</span>):</span>
<span id="cb53-97"><a href="#cb53-97" aria-hidden="true" tabindex="-1"></a>        reverse <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> reverse_schema <span class="kw">and</span> epoch <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">False</span></span>
<span id="cb53-98"><a href="#cb53-98" aria-hidden="true" tabindex="-1"></a>        l_loss, b_loss, g_loss <span class="op">=</span> train_neural_flattener(</span>
<span id="cb53-99"><a href="#cb53-99" aria-hidden="true" tabindex="-1"></a>            model, df, groups, optimizer, n_batches, </span>
<span id="cb53-100"><a href="#cb53-100" aria-hidden="true" tabindex="-1"></a>            use_cuda <span class="op">=</span> use_cuda, apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb53-101"><a href="#cb53-101" aria-hidden="true" tabindex="-1"></a>            criterion<span class="op">=</span>criterion,</span>
<span id="cb53-102"><a href="#cb53-102" aria-hidden="true" tabindex="-1"></a>            hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out, </span>
<span id="cb53-103"><a href="#cb53-103" aria-hidden="true" tabindex="-1"></a>            hinge_value<span class="op">=</span>hinge_value,</span>
<span id="cb53-104"><a href="#cb53-104" aria-hidden="true" tabindex="-1"></a>            use_density_loss <span class="op">=</span> use_density_loss,    </span>
<span id="cb53-105"><a href="#cb53-105" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> top_k, lambda_density <span class="op">=</span> lambda_density, </span>
<span id="cb53-106"><a href="#cb53-106" aria-hidden="true" tabindex="-1"></a>            autoencoder <span class="op">=</span> autoencoder, use_emb <span class="op">=</span> use_emb, use_gae <span class="op">=</span> use_gae, sample_size<span class="op">=</span>sample_size, </span>
<span id="cb53-107"><a href="#cb53-107" aria-hidden="true" tabindex="-1"></a>            sample_with_replacement<span class="op">=</span>sample_with_replacement, logger<span class="op">=</span>logger,</span>
<span id="cb53-108"><a href="#cb53-108" aria-hidden="true" tabindex="-1"></a>            add_noise<span class="op">=</span>add_noise, noise_scale<span class="op">=</span>noise_scale, use_gaussian<span class="op">=</span>use_gaussian, </span>
<span id="cb53-109"><a href="#cb53-109" aria-hidden="true" tabindex="-1"></a>            use_penalty<span class="op">=</span>use_penalty, lambda_energy<span class="op">=</span>lambda_energy, reverse<span class="op">=</span>reverse</span>
<span id="cb53-110"><a href="#cb53-110" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb53-111"><a href="#cb53-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> l_loss.items():  </span>
<span id="cb53-112"><a href="#cb53-112" aria-hidden="true" tabindex="-1"></a>            local_losses[k].extend(v)</span>
<span id="cb53-113"><a href="#cb53-113" aria-hidden="true" tabindex="-1"></a>        batch_losses.extend(b_loss)</span>
<span id="cb53-114"><a href="#cb53-114" aria-hidden="true" tabindex="-1"></a>        globe_losses.extend(g_loss)</span>
<span id="cb53-115"><a href="#cb53-115" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> epoch <span class="op">%</span> plot_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb53-116"><a href="#cb53-116" aria-hidden="true" tabindex="-1"></a>            generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb53-117"><a href="#cb53-117" aria-hidden="true" tabindex="-1"></a>                model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb53-118"><a href="#cb53-118" aria-hidden="true" tabindex="-1"></a>                sample_with_replacement<span class="op">=</span>sample_with_replacement, use_cuda<span class="op">=</span>use_cuda, </span>
<span id="cb53-119"><a href="#cb53-119" aria-hidden="true" tabindex="-1"></a>                samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span>logger,</span>
<span id="cb53-120"><a href="#cb53-120" aria-hidden="true" tabindex="-1"></a>                autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb53-121"><a href="#cb53-121" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-122"><a href="#cb53-122" aria-hidden="true" tabindex="-1"></a>            plot_comparision(</span>
<span id="cb53-123"><a href="#cb53-123" aria-hidden="true" tabindex="-1"></a>                df, generated, trajectories,</span>
<span id="cb53-124"><a href="#cb53-124" aria-hidden="true" tabindex="-1"></a>                palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb53-125"><a href="#cb53-125" aria-hidden="true" tabindex="-1"></a>                save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb53-126"><a href="#cb53-126" aria-hidden="true" tabindex="-1"></a>                <span class="bu">file</span><span class="op">=</span><span class="ss">f'2d_comparision_local_</span><span class="sc">{</span>n_local_epochs<span class="sc">}</span><span class="ss">_global_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png'</span>,</span>
<span id="cb53-127"><a href="#cb53-127" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb53-128"><a href="#cb53-128" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-129"><a href="#cb53-129" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb53-130"><a href="#cb53-130" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_post_local_epochs), desc<span class="op">=</span><span class="st">'Posttraining Epoch'</span>):</span>
<span id="cb53-131"><a href="#cb53-131" aria-hidden="true" tabindex="-1"></a>        reverse <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> reverse_schema <span class="kw">and</span> epoch <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">False</span></span>
<span id="cb53-132"><a href="#cb53-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-133"><a href="#cb53-133" aria-hidden="true" tabindex="-1"></a>        l_loss, b_loss, g_loss <span class="op">=</span> train_neural_flattener(</span>
<span id="cb53-134"><a href="#cb53-134" aria-hidden="true" tabindex="-1"></a>            model, df, groups, optimizer, n_batches, </span>
<span id="cb53-135"><a href="#cb53-135" aria-hidden="true" tabindex="-1"></a>            use_cuda <span class="op">=</span> use_cuda, apply_losses_in_time<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb53-136"><a href="#cb53-136" aria-hidden="true" tabindex="-1"></a>            criterion<span class="op">=</span>criterion,</span>
<span id="cb53-137"><a href="#cb53-137" aria-hidden="true" tabindex="-1"></a>            hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out, </span>
<span id="cb53-138"><a href="#cb53-138" aria-hidden="true" tabindex="-1"></a>            hinge_value<span class="op">=</span>hinge_value,</span>
<span id="cb53-139"><a href="#cb53-139" aria-hidden="true" tabindex="-1"></a>            use_density_loss <span class="op">=</span> use_density_loss,    </span>
<span id="cb53-140"><a href="#cb53-140" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> top_k, lambda_density <span class="op">=</span> lambda_density, </span>
<span id="cb53-141"><a href="#cb53-141" aria-hidden="true" tabindex="-1"></a>            autoencoder <span class="op">=</span> autoencoder, use_emb <span class="op">=</span> use_emb, use_gae <span class="op">=</span> use_gae, sample_size<span class="op">=</span>sample_size, </span>
<span id="cb53-142"><a href="#cb53-142" aria-hidden="true" tabindex="-1"></a>            sample_with_replacement<span class="op">=</span>sample_with_replacement, logger<span class="op">=</span>logger,</span>
<span id="cb53-143"><a href="#cb53-143" aria-hidden="true" tabindex="-1"></a>            add_noise<span class="op">=</span>add_noise, noise_scale<span class="op">=</span>noise_scale, use_gaussian<span class="op">=</span>use_gaussian, </span>
<span id="cb53-144"><a href="#cb53-144" aria-hidden="true" tabindex="-1"></a>            use_penalty<span class="op">=</span>use_penalty, lambda_energy<span class="op">=</span>lambda_energy, reverse<span class="op">=</span>reverse</span>
<span id="cb53-145"><a href="#cb53-145" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb53-146"><a href="#cb53-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> l_loss.items():  </span>
<span id="cb53-147"><a href="#cb53-147" aria-hidden="true" tabindex="-1"></a>            local_losses[k].extend(v)</span>
<span id="cb53-148"><a href="#cb53-148" aria-hidden="true" tabindex="-1"></a>        batch_losses.extend(b_loss)</span>
<span id="cb53-149"><a href="#cb53-149" aria-hidden="true" tabindex="-1"></a>        globe_losses.extend(g_loss)</span>
<span id="cb53-150"><a href="#cb53-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> epoch <span class="op">%</span> plot_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb53-151"><a href="#cb53-151" aria-hidden="true" tabindex="-1"></a>            generated, trajectories <span class="op">=</span> generate_plot_data(</span>
<span id="cb53-152"><a href="#cb53-152" aria-hidden="true" tabindex="-1"></a>                model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb53-153"><a href="#cb53-153" aria-hidden="true" tabindex="-1"></a>                sample_with_replacement<span class="op">=</span>sample_with_replacement, use_cuda<span class="op">=</span>use_cuda, </span>
<span id="cb53-154"><a href="#cb53-154" aria-hidden="true" tabindex="-1"></a>                samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span>logger,</span>
<span id="cb53-155"><a href="#cb53-155" aria-hidden="true" tabindex="-1"></a>                autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon</span>
<span id="cb53-156"><a href="#cb53-156" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-157"><a href="#cb53-157" aria-hidden="true" tabindex="-1"></a>            plot_comparision(</span>
<span id="cb53-158"><a href="#cb53-158" aria-hidden="true" tabindex="-1"></a>                df, generated, trajectories,</span>
<span id="cb53-159"><a href="#cb53-159" aria-hidden="true" tabindex="-1"></a>                palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb53-160"><a href="#cb53-160" aria-hidden="true" tabindex="-1"></a>                save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb53-161"><a href="#cb53-161" aria-hidden="true" tabindex="-1"></a>                <span class="bu">file</span><span class="op">=</span><span class="ss">f'2d_comparision_local_</span><span class="sc">{</span>n_local_epochs<span class="sc">}</span><span class="ss">_global_</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">_post_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png'</span>,</span>
<span id="cb53-162"><a href="#cb53-162" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb53-163"><a href="#cb53-163" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-164"><a href="#cb53-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-165"><a href="#cb53-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> reverse_schema:</span>
<span id="cb53-166"><a href="#cb53-166" aria-hidden="true" tabindex="-1"></a>        _temp <span class="op">=</span> {}</span>
<span id="cb53-167"><a href="#cb53-167" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hold_one_out:</span>
<span id="cb53-168"><a href="#cb53-168" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps([g <span class="cf">for</span> g <span class="kw">in</span> groups <span class="cf">if</span> g <span class="op">!=</span> hold_out]):</span>
<span id="cb53-169"><a href="#cb53-169" aria-hidden="true" tabindex="-1"></a>                a <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb53-170"><a href="#cb53-170" aria-hidden="true" tabindex="-1"></a>                b <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb53-171"><a href="#cb53-171" aria-hidden="true" tabindex="-1"></a>                _temp[a] <span class="op">=</span> []</span>
<span id="cb53-172"><a href="#cb53-172" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i, value <span class="kw">in</span> <span class="bu">enumerate</span>(local_losses[a]):</span>
<span id="cb53-173"><a href="#cb53-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-174"><a href="#cb53-174" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb53-175"><a href="#cb53-175" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(local_losses[b].pop(<span class="dv">0</span>))</span>
<span id="cb53-176"><a href="#cb53-176" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb53-177"><a href="#cb53-177" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb53-178"><a href="#cb53-178" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb53-179"><a href="#cb53-179" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> _temp</span>
<span id="cb53-180"><a href="#cb53-180" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb53-181"><a href="#cb53-181" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups):</span>
<span id="cb53-182"><a href="#cb53-182" aria-hidden="true" tabindex="-1"></a>                a <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb53-183"><a href="#cb53-183" aria-hidden="true" tabindex="-1"></a>                b <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb53-184"><a href="#cb53-184" aria-hidden="true" tabindex="-1"></a>                _temp[a] <span class="op">=</span> []</span>
<span id="cb53-185"><a href="#cb53-185" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i, value <span class="kw">in</span> <span class="bu">enumerate</span>(local_losses[a]):</span>
<span id="cb53-186"><a href="#cb53-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-187"><a href="#cb53-187" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i <span class="op">%</span> reverse_n <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb53-188"><a href="#cb53-188" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(local_losses[b].pop(<span class="dv">0</span>))</span>
<span id="cb53-189"><a href="#cb53-189" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb53-190"><a href="#cb53-190" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb53-191"><a href="#cb53-191" aria-hidden="true" tabindex="-1"></a>                        _temp[a].append(value)</span>
<span id="cb53-192"><a href="#cb53-192" aria-hidden="true" tabindex="-1"></a>            local_losses <span class="op">=</span> _temp</span>
<span id="cb53-193"><a href="#cb53-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-194"><a href="#cb53-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-195"><a href="#cb53-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-196"><a href="#cb53-196" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> plot_every <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb53-197"><a href="#cb53-197" aria-hidden="true" tabindex="-1"></a>        plot_losses(</span>
<span id="cb53-198"><a href="#cb53-198" aria-hidden="true" tabindex="-1"></a>            local_losses, batch_losses, globe_losses, </span>
<span id="cb53-199"><a href="#cb53-199" aria-hidden="true" tabindex="-1"></a>            save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>exp_dir, </span>
<span id="cb53-200"><a href="#cb53-200" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span><span class="op">=</span><span class="ss">f'losses_l</span><span class="sc">{</span>n_local_epochs<span class="sc">}</span><span class="ss">_e</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">_ple</span><span class="sc">{</span>n_post_local_epochs<span class="sc">}</span><span class="ss">.png'</span></span>
<span id="cb53-201"><a href="#cb53-201" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb53-202"><a href="#cb53-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-203"><a href="#cb53-203" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> local_losses, batch_losses, globe_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="dataset-setup" class="level2">
<h2 class="anchored" data-anchor-id="dataset-setup">Dataset Setup</h2>
<p>In this test run, we’ll use something in which it’s very easy to see if the flattening has failed: the hemisphere. This corresponds to a neighborhood of high positive curvature.</p>
<div id="cell-73" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusion_curvature.datasets <span class="im">import</span> sphere</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusion_curvature.utils <span class="im">import</span> plot_3d</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>X_sphere, ks_sphere <span class="op">=</span> sphere(<span class="dv">4000</span>) <span class="co"># keep it relatively sparse</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>X_cap_of_sphere <span class="op">=</span> X_sphere[X_sphere[:,<span class="dv">2</span>] <span class="op">&gt;</span> <span class="dv">0</span>] <span class="co"># Just the itty bitty polar top</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'num points'</span>,<span class="bu">len</span>(X_cap_of_sphere))</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>plot_3d(X_cap_of_sphere,X_cap_of_sphere[:,<span class="dv">2</span>],use_plotly<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.scatter(X_cap_of_sphere[:,0],X_cap_of_sphere[:,1])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>num points 2025</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="14acb749-ce1c-4720-a034-44189e80e629" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("14acb749-ce1c-4720-a034-44189e80e629")) {                    Plotly.newPlot(                        "14acb749-ce1c-4720-a034-44189e80e629",                        [{"hovertemplate":"x=%{x}\u003cbr\u003ey=%{y}\u003cbr\u003ez=%{z}\u003cbr\u003ecolors=%{marker.color}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":[0.9346202192680949,0.04994960946202759,0.4726768774504602,0.8033570730468255,0.4661466188016612,0.9275872271325571,0.09098786217031038,0.8799273439986983,0.43442152513156,0.7060908501774382,0.7368468945135214,0.013630647139695112,0.8235574840707355,0.43237048181184906,0.4172304474775218,0.848242418986849,0.9547775544966279,0.9190171699678467,0.10157613477047435,0.7163469574494247,0.056844834137318515,0.29230857010211003,0.07089552745384557,0.5052273805574594,0.24684475089457597,0.21452737794743035,0.455691961623033,0.21535862401148365,0.1627450699737994,0.10568399836545424,0.5990603450552897,0.29624351221496087,0.9933398810149432,0.24666139785794336,0.9836608995921783,0.886234600665311,0.6789370108986329,0.11351799257880156,0.47655736932178433,0.6903759391465804,0.18315777694307842,0.6642470278510872,0.7910367203353494,0.20846774431123757,0.7577358027886072,0.8115150244127464,0.22554167316465965,0.8162048109035923,0.444787525517168,0.2974232604930851,0.677718334783137,0.8000427822612569,0.9373156113296677,0.6762784479326804,0.017788698899465938,0.10296382286698841,0.7062146262821583,0.7444881314407565,0.03770239925301037,0.12047654562448368,0.8740960613188656,0.40988326191994234,0.44852745858714416,0.7818642635055831,0.5962793523623149,0.23590273626534253,0.4596931996156608,0.3706422837864106,0.9703042039214936,0.5206176473497467,0.1705398491805054,0.7943539634311318,0.06899223361591361,0.734900698604532,0.6118877015899195,0.15814116205306605,0.16967453436479596,0.2986652042217763,0.33355096998623346,0.5388891964003059,0.4508847259098284,0.5135128813977317,0.5235605618947892,0.7150056586315127,0.32638253561551644,0.8910921368110236,0.8169108035882986,0.9839103846904549,0.7203289290578403,0.04536646655837627,0.47443536281427784,0.960961983531204,0.6995339232523222,0.7862219221894442,0.5719025722697608,0.5297431058599413,0.5885381923335098,0.5505904600000678,0.16328803997398614,0.9380695254226157,0.3278623837968352,0.27684890868385603,0.37344384547948567,0.979265093476249,0.879432141182319,0.05466224387798052,0.8888855693001637,0.869331764336331,0.5588569100611164,0.04726726292737592,0.9139916106101496,0.8025672754314088,0.6835850299932059,0.00787815823791106,0.5325713652720191,0.16434249893259872,0.4131052876155083,0.1650342760641867,0.8278716610751914,0.8479578423510717,0.43861384848987517,0.5238635806938797,0.7810744160890614,0.1823558507647524,0.9726258625520356,0.24488786406644705,0.17085942782370728,0.3544198274200179,0.8573720531054456,0.8504590210957537,0.25607795802840544,0.3229236443346939,0.33772402181547595,0.1384720419261373,0.003713787016145989,0.296486100646513,0.19288939341197825,0.6657176829784519,0.7015655787791681,0.4674897824753158,0.1388025950034986,0.8598947900790527,0.5277865753854644,0.8265026358059855,0.7203486969665877,0.19993251927291633,0.4792435706139613,0.1954510299093049,0.4519051033194799,0.6298957549906217,0.6977308871526327,0.46807820716269916,0.5923325688270968,0.15554144068402595,0.8179187960973315,0.43801453459259976,0.42529031791674243,0.3890783008146594,0.2829309592052101,0.9459796826838158,0.7207795310968896,0.7977308066946442,0.03298723791910067,0.336723422084868,0.9809368445589082,0.4867378936906416,0.11097267665547272,0.9567562049609413,0.47890739290350215,0.22605653812644824,0.11908832133298465,0.7654017142343327,0.6317573914443296,0.8671861389299501,0.9313688535768923,0.055990479586312904,0.2982017871568473,0.9002432267446425,0.4320543031638415,0.716833383113782,0.7636991376644539,0.1850509009217021,0.5730626207935854,0.5248815507784972,0.5171314369736559,0.1327229074784192,0.5403103264451642,0.9517892843452753,0.15251691249870833,0.571598602116153,0.5793894536730834,0.8429126130206978,0.4001661723572978,0.2896589570574256,0.837611774281643,0.219129026551366,0.9236456398735229,0.07032897366164408,0.48590705577385096,0.8421640531013878,0.1663878427157401,0.7952859461850905,0.8098088630666703,0.4193688825298263,0.8009217049510182,0.1474133060152359,0.8328548780423061,0.9694951418842784,0.6449061824519332,0.4646760807874488,0.5452253522064293,0.6512623568852058,0.05340036797435343,0.39977504026473926,0.3527306575392638,0.5098657321168104,0.4582419228689548,0.5654403377609536,0.971906050634983,0.5439744643158413,0.6662522366410257,0.7468099685508885,0.09177310350809137,0.7412460650763605,0.06685034370854367,0.5947388707862503,0.022119934141299678,0.25517673594354195,0.7348236966420744,0.8952258944964901,0.39195898225649434,0.02792152788268976,0.7168304894469799,0.14317973643903245,0.6287711619513665,0.6646185584051145,0.21398765940617878,0.6702351780659375,0.11172636029736702,0.4592725742108656,0.8057881392905838,0.33816434437422127,0.0636347944385767,0.29798717084932275,0.21260463984393727,0.3464888559397502,0.11646716566640519,0.08104513809205798,0.472907470211393,0.09173857594213351,0.3717231160341813,0.2143242526031688,0.5572789509545694,0.4912980671981739,0.8016046321819946,0.8983223443081612,0.5906635193222308,0.9657573179170523,0.5657902207854193,0.4170971134174033,0.6135328564298433,0.300909777285507,0.8874851739961145,0.6402881882556059,0.9278929384895911,0.22233427139124168,0.3183760602895478,0.031995036610984885,0.049848224295256614,0.6423931430658694,0.9667839255736773,0.19484537592101148,0.8463022393113806,0.8630453532083953,0.6558455079232999,0.36887223912001954,0.07387766076287057,0.03169466254280734,0.820754906896732,0.31982301222384524,0.8255074330690476,0.15550126960665775,0.2645323795982767,0.09335611672271166,0.23400709634002378,0.3330163394960996,0.3403761489665602,0.4275447001516656,0.3122220830826669,0.43916222528228077,0.24218256711939096,0.054799423547000826,0.03538863668220984,0.10390089417003875,0.06887492768289148,0.49048270556635565,0.8112679400214725,0.8980720149949541,0.3519312565530559,0.9339863212504118,0.2704747386359034,0.9471394021128868,0.020671799559357596,0.5854073636786006,0.6578615866006977,0.4722154740295404,0.9808044958990682,0.34790233805817256,0.2830742930977107,0.88447930741811,0.8153650496474407,0.5573080134795946,0.7822771670855092,0.43613670033604374,0.5613730434693505,0.5897029728153313,0.07054909666854377,0.6341136612275369,0.759655557941372,0.3521682649151306,0.08783153703349468,0.07597387727152541,0.14571172273934382,0.2232059586539772,0.29814848938648925,0.29972696839658186,0.4614611346005937,0.17316357888074635,0.4493978190610305,0.5525674254158166,0.41246434331133414,0.02361996000891793,0.20229626136037496,0.5457746004538455,0.5580576830003704,0.28791941760480744,0.025826157953491434,0.47666170321285295,0.46123844425373056,0.6343986314093926,0.04197074362869196,0.9624496864081096,0.5677861245352045,0.6331406517930805,0.5800554632635185,0.976519056910755,0.8637034031472737,0.024124499412264567,0.5612146488718116,0.9155643218051887,0.000375235557535062,0.09432997029157404,0.8200237986628126,0.6629262771726973,0.2419799765088399,0.9663810004741226,0.3105689842973354,0.23252056674024565,0.3244903965329572,0.6325309398885247,0.7891899493585266,0.6840204621969038,0.027024338324132438,0.7401925916472941,0.9637292975044544,0.174150174914813,0.736169875352299,0.9396094600825328,0.4235494220449482,0.46779317455864877,0.639307115995725,0.9855378317254082,0.5198856602165826,0.4884751806348223,0.9984446939807229,0.260669260244592,0.04299653203218412,0.9107348586696296,0.6927451156058538,0.9410282016886421,0.7487395779938145,0.8939540964503787,0.6039393361028217,0.4281007389606209,0.9975382590802151,0.155575782591845,0.26925337513760783,0.8340407392975798,0.3093800038753125,0.3113874585597365,0.7863442221883568,0.20469026082436986,0.9714320438810174,0.6470131444787443,0.12168570470099588,0.026558640278681676,0.8493815708874611,0.41322857872910423,0.439597834715384,0.1874649089731008,0.6318595414268866,0.3563595197437634,0.2988050032568553,0.6416577491959302,0.9055814113272381,0.16905612156464322,0.11196029781538676,0.1404369249672461,0.12879275335383525,0.24890137688321226,0.8273326362615835,0.663430380446236,0.9174668320152869,0.7528925645171528,0.7607680232185855,0.24515399950535735,0.90983229977387,0.870339228520544,0.43281800540949095,0.4002870597362607,0.9235313193059115,0.48298238742316085,0.6933027923825109,0.12435707876426223,0.902417871724092,0.6937043301136476,0.3137845448503222,0.4711846387771263,0.46521912378995306,0.5437572889590893,0.4161449839659482,0.8704362401488126,0.5046133036147749,0.30692842170758006,0.9902069538182373,0.6951484864935656,0.503096804827541,0.15707441484795706,0.20499587841624214,0.12428516155094804,0.4599900478804239,0.7219620948766807,0.3774178499260318,0.28212251191872006,0.6756129750741906,0.17344946889916263,0.05120065400004185,0.42169841651673473,0.811570041492294,0.7112578089686827,0.8504137741576874,0.8395634630901562,0.5824924673298988,0.9277094819701244,0.9148961649863444,0.7722898634858931,0.41610963752610264,0.010773414102738455,0.2928124940993888,0.6603843873310005,0.007019834008456664,0.1285148917951024,0.12635668006983425,0.9869067027034092,0.8535611985847276,0.7477198111594268,0.07315077782340872,0.48604101833268387,0.6385747800922043,0.8988559945836162,0.2862415669139225,0.779605223906302,0.5504507429464077,0.2634770084671074,0.5404463317325826,0.44404028318203803,0.8457225427229574,0.4814716171666061,0.11474374413849497,0.23704577502566632,0.9454656027328315,0.8653424883871473,0.31263638906420943,0.8617226700226606,0.5292732906746542,0.22998138700938112,0.17181493315004337,0.699238571845819,0.4702539999963204,0.27269784551115883,0.8602121805129992,0.7135549698503143,0.2605563825983071,0.6424144434154969,0.21127874219591264,0.27288092515478957,0.7209683045505175,0.13944715812352626,0.4582731795240766,0.6078884250547688,0.843883645263824,0.7893359049193773,0.7739315802850634,0.4311738064101914,0.3455335258419605,0.9530297543945724,0.2534828578637177,0.9827721006155629,0.23774912164636416,0.8801494949888127,0.52250826777117,0.44933468889950484,0.4095395525473987,0.9923406337358097,0.08791266989478731,0.999855359459251,0.27949067381038495,0.4833600756218752,0.5413681761746627,0.8988490501669901,0.8167018519882162,0.1581159495115337,0.19855312381567886,0.8138557927341274,0.761048077831785,0.9879932467584235,0.02330228159372472,0.009568920709584875,0.6589754070749417,0.14832284279626634,0.04370908829295707,0.9566473333960118,0.810862400904176,0.5828443746661124,0.3571559271431484,0.38513968425661227,0.37051754028517075,0.5669447852289009,0.3291138537283099,0.5126514498117084,0.5012871355178965,0.4951448730382164,0.8440780460855105,0.7935850456460957,0.09639997064048025,0.5256056199814604,0.7139448417159276,0.8661653091075535,0.9041641302997774,0.07285112499805639,0.02580551237613343,0.7645972093827831,0.19048375562407363,0.9860416534038086,0.7520677541671507,0.9127974146626091,0.9952424779268105,0.08831451618107689,0.3714084208667062,0.9594065550134978,0.4549008654348192,0.9231959091757128,0.9094763523955399,0.8552966769390227,0.8054864063392354,0.8334669767580787,0.7931238747357576,0.7917275872183177,0.46920719862729054,0.7717126266665345,0.44013769409207076,0.3667819114324596,0.04463505417474392,0.719083477127208,0.11770009031387607,0.7262078124636677,0.09318760871980494,0.8997108499014247,0.6757612252778338,0.5692197850297966,0.7659316314744437,0.6212196043588104,0.8011207191351899,0.6797334081428346,0.8848785928042909,0.560101071105651,0.6437991081418738,0.13503652916829648,0.1465111784170246,0.6899942195953522,0.9903374431723397,0.511712610026041,0.18413428127964648,0.8459603791040566,0.7388991663353738,0.8189113179568285,0.24753872186400996,0.7718594698552712,0.0654354987560755,0.95763192842824,0.5849023206269699,0.949511372179952,0.7466724748530961,0.6190614121685117,0.879564652817452,0.6370053590666183,0.446516034723368,0.33345061788559965,0.037634820893866744,0.30190741072198923,0.23437544599362198,0.8506837599935034,0.18550706206288464,0.6366720559550105,0.7356605011247268,0.9458180580595348,0.7341074981737721,0.653887784248441,0.5511636545265756,0.25246112238202106,0.3700923162382418,0.5376848214245574,0.03558154351592028,0.17339163359653448,0.8771377435674257,0.21515682287682503,0.4336665648550356,0.051478438060383755,0.6669195028157747,0.33762786506481157,0.7934452677966377,0.3181768242095094,0.04070726913231495,0.4866300082749424,0.1137307442684492,0.32086253094685085,0.9898324418159049,0.9820514354574489,0.7514017247916961,0.25614526036012697,0.13693539630573082,0.7819667569149724,0.34515788921276297,0.49019547620459775,0.9338887621622124,0.8033855418841028,0.45152142438875437,0.5478062757529754,0.4639087401836013,0.48849425748414066,0.13583194641139243,0.023291561365578767,0.6662128003631151,0.5259348778235629,0.7561348184111351,0.3157413880766247,0.14385164290967375,0.8434119270694918,0.8280970699792991,0.11370855851187187,0.9758110626367068,0.21095361657774042,0.5620436835483568,0.17875129584436317,0.09438748453421868,0.7616869699458462,0.11020383291648066,0.9007112734167138,0.8190219977829782,0.3461409282504398,0.8241811629103296,0.0843361238614389,0.029022567729915474,0.8562408743458471,0.20157704905466764,0.2473029080258638,0.6021636860685811,0.5542193087851012,0.790738512191287,0.12545114589522177,0.6477324199075766,0.5347875329617392,0.6571610689445181,0.45161765955758204,0.00188667858755511,0.585217879576291,0.5497610304753208,0.14328021603642319,0.38609214194658853,0.17870445568930293,0.7411221534941194,0.24054549456612528,0.3246619825367223,0.15246220379385378,0.1259135296701994,0.8879516697610933,0.0976738011236078,0.2801601948244157,0.3014638274580033,0.13974617710294027,0.3239619933445644,0.27511695183453794,0.8209555556475564,0.9979171079265089,0.6756132756344396,0.1383293560026593,0.17370282198788134,0.9894167808515562,0.6822957182478554,0.024245889306271747,0.6830383803227046,0.004768330810783014,0.6960920526723554,0.658209822093483,0.0952771627910967,0.5420247792165586,0.21283306424991102,0.6746050308923378,0.8718360435545753,0.12834442530719872,0.5464050263784608,0.22954861888062753,0.37719111574701375,0.2723853809557267,0.7967135861300341,0.5749739143010845,0.09946831709030324,0.4325914105803062,0.46406310794951827,0.8231132485353135,0.6258527484077547,0.15747030020467906,0.36463537403232626,0.4874462055092041,0.7058469704962086,0.3555828188175584,0.786857996390518,0.1865324616208747,0.8981816684424717,0.7428138130866234,0.6654986175050954,0.5458465181605444,0.2058989584711726,0.5398580536786733,0.5890394921145801,0.5462886248094649,0.3547307707995852,0.12170563870150837,0.08332877675698908,0.42279243799008936,0.2723462643609827,0.5764109883765828,0.7306759770605685,0.7582348523622138,0.1120477834058174,0.7140313709366235,0.8032066679120488,0.7913651527297754,0.14020902832069126,0.7771587854112687,0.9235019302651617,0.29363394514890767,0.4082195154555459,0.9851808802695916,0.19297399011409405,0.401592746407955,0.6622955888986535,0.7288768023042397,0.8940629394666196,0.8593283041704934,0.34843856571425014,0.18788733631131294,0.8780829530917188,0.14055625729509347,0.5024309072144847,0.45639886347313047,0.9046181538867104,0.9365708589142832,0.6089677701326707,0.4043816097888404,0.9250192022128683,0.2600142325494364,0.7159807215962307,0.23314170537056883,0.16885613958065762,0.9771679894505855,0.8997198473076627,0.8508014610003929,0.3360540154388692,0.6227151366910106,0.4212591321991153,0.5001507161732288,0.4478764215815967,0.726888640181689,0.8026391354683945,0.6918068483037216,0.24848036626771697,0.08673281941795018,0.1354837544420849,0.8074559361264894,0.1863830146927272,0.8468977055856555,0.008796474836741439,0.6964638964535811,0.4797031182001643,0.7026279669754824,0.2745667521417649,0.23848813638466546,0.6721994841540068,0.6487366650522729,0.18650745833907742,0.6212901723831574,0.7624926349538695,0.7344645960619285,0.05307978076112724,0.6687471109885034,0.6621039699257462,0.6998259823760261,0.31138200730168314,0.09320202552978028,0.9367313123591772,0.3273641674549485,0.4747812870294262,0.8314842383778226,0.6712245763957335,0.48246047392574665,0.010220502115814234,0.6431938319472277,0.6135314857628386,0.1770980726998197,0.6145122178633597,0.5438833917899443,0.9313823409855263,0.43800344709437933,0.9227803464267301,0.79747273847436,0.7004988576822321,0.9081912198963273,0.5547523521673725,0.7825436533668679,0.013333458598763939,0.46821646970055614,0.9991017533491585,0.15105901419592022,0.912307302105765,0.386806141948937,0.11159917308675237,0.005815951084687302,0.4230285268012233,0.12347785968632134,0.9579847632981905,0.32566988711834577,0.2708114988739426,0.49065949038789236,0.5744970355475965,0.8170546185760584,0.8523427137913339,0.967426370472529,0.6824741308683059,0.5489975935829192,0.4089853660944055,0.10407383514695577,0.9386758911793793,0.8067920795771141,0.9932177288080998,0.057284339359014586,0.9385458947916007,0.6989145490628468,0.8473226025099996,0.14684014053027786,0.43039259237220406,0.04839087834011032,0.5475992734013929,0.7657419360432312,0.2037177942655602,0.11254262390050006,0.37702775332066446,0.05322153838304602,0.46687811392627787,0.07709321987543569,0.61619550027305,0.8973234732845953,0.9130500653226973,0.07397990529260148,0.12009327167194556,0.0876140727932303,0.5437812903314426,0.5784359491593534,0.6812903721181913,0.8042606801658276,0.49128248829421267,0.9509807213073107,0.8008771841916187,0.5643713361998257,0.4866241101608958,0.9103474053839076,0.6443564236100292,0.44726352989135687,0.030687046363215623,0.4074089321476157,0.8698257307443067,0.8474558726569849,0.7320190158205728,0.444567482563759,0.9286662857949599,0.19810815334492698,0.7475857567701342,0.6732824582014261,0.5502861597554584,0.040289544309750513,0.03396705641183372,0.43580702103909263,0.14594501642998006,0.36674887032025516,0.558184102244458,0.15503621400365358,0.9812664895448301,0.5764812144008259,0.36903884371811524,0.0029119779219211675,0.639632951765198,0.571086826631676,0.7831242729018779,0.5331636114963588,0.4443961555158814,0.9144447695711804,0.23649588260431384,0.6560558003897169,0.40819313546692304,0.3357993472082328,0.3380758405728884,0.3720363709968631,0.8142394753579745,0.9805361213672517,0.3627518572377786,0.16037898566657846,0.482137669335013,0.4084952864046422,0.897055545668679,0.8841745016592882,0.9573991733393326,0.3361325475844496,0.9291033838351439,0.7842760974747525,0.018967975615163037,0.16841821314730765,0.32502356534810123,0.7459025761990267,0.2006796051749614,0.557926952016848,0.9327774675292111,0.013315624227738371,0.32491706146971033,0.9342188598127017,0.37373021289906716,0.22949285640523506,0.3079715918782311,0.8767683428211246,0.36118380008783507,0.5503312764564022,0.6188813771456003,0.8590602934532477,0.5327710969598419,0.5679226294885266,0.9887479036328458,0.9244452600119318,0.6109040931468848,0.6404587708810531,0.3814584234557586,0.9600280563396634,0.8891884554292041,0.40305498782520355,0.4770374283880066,0.4033978905055063,0.36259559063512287,0.11870164943047125,0.7450588253991299,0.931747155360736,0.5661895381327834,0.875792186914268,0.1793556000107659,0.9268074736582732,0.0021966430428288736,0.7387700327576552,0.9687074531587457,0.03490799064752778,0.15430355904083806,0.7960084891179144,0.31603911824420305,0.048035390166739106,0.28329272577988285,0.4133451774768451,0.08302349761944582,0.8716245699272658,0.729901141130779,0.12796325965932262,0.22465965935578752,0.9870747861646344,0.9542114254555287,0.8358417450001242,0.21452630384161145,0.4608155332704861,0.1487188835512796,0.4284904581452483,0.48409197973559714,0.19792878525914118,0.2074170633435191,0.3526546367035481,0.019666727125508168,0.011961210318828042,0.23481880172162634,0.8357791511792984,0.08433430197183929,0.760320311847257,0.6816544855120592,0.9834015166668426,0.11184359140323863,0.4481769511016175,0.750615009047616,0.36471607086804303,0.10731649105256785,0.31662998084334787,0.2808858987209352,0.7528287057047369,0.8909649369729825,0.521169277979326,0.41626176646204116,0.9373816510379013,0.8827108382481922,0.23434559805718708,0.6615281063278577,0.230229015049792,0.019542100522723992,0.44638051873993556,0.4311445471126048,0.38421550354477885,0.7336487944813213,0.39963387284722757,0.6139499611341035,0.24365845773499836,0.011343522154717938,0.9801420724699825,0.0386168038530329,0.6905773460140517,0.9553884757284952,0.9741035670288772,0.40019225873504793,0.4750744287978551,0.7791015234406755,0.31669754119295174,0.725008740685477,0.2416314646834531,0.7617775352047803,0.3640797640306048,0.9369509803531563,0.986016457912794,0.3890748886476791,0.5413340626737267,0.8627992095544421,0.9173545529532369,0.25289030200744245,0.06560683985371532,0.7287077887359844,0.1734482346579357,0.7626591207116836,0.3818189862432234,0.3963497362213005,0.5493442173026933,0.9842496656266898,0.23246617581161144,0.7095790555707658,0.6763102586241274,0.8422358329016594,0.6780589192133201,0.5101412962576116,0.3461020517717381,0.4159772774571288,0.17439874821899568,0.5984191644329442,0.52421404440881,0.1268914075274888,0.8724845685264219,0.8031124193776542,0.36054055204146124,0.5220697907546715,0.5444401323034965,0.10575369319586254,0.4474059882607781,0.27995000416153415,0.9025987508106694,0.49909991523106817,0.05850870657493328,0.9062728032510706,0.012649503518274648,0.7017436199452065,0.2954784030888601,0.7234460382734547,0.3504350420613718,0.4569263032816939,0.5206142018481905,0.5137798709276635,0.3186676218978857,0.40196625753663934,0.758037980742,0.1406853095686104,0.8394530419193642,0.0147334429791474,0.541481796537657,0.727655077973122,0.9302312286624576,0.27391860245189603,0.24867254779452558,0.8775959300587851,0.8346754824560169,0.07836943019871521,0.8445396178910552,0.7090865278016885,0.5233535715838412,0.769472188144823,0.9427637772645899,0.8661476246266594,0.9332344912600804,0.9895382285945458,0.0901521222849927,0.229939762049471,0.8729525677180204,0.8040999422828274,0.8473405830361046,0.8136217822847707,0.09450138041475825,0.8001268511695544,0.059159627803093,0.02230530565209657,0.3347829893664334,0.35753059803751813,0.32516727100631326,0.5662904413881304,0.2088787974989328,0.781356145485475,0.9577467043263577,0.4446506005657439,0.49155870384684003,0.23482665879623849,0.05544229180117354,0.056966513428383936,0.5197883466595136,0.5888516319515785,0.15597842533188028,0.29735014808557614,0.9494804012295168,0.7937699205608102,0.867605267662978,0.6823390198833066,0.6275719095645016,0.5761083997715561,0.9467037289823897,0.7278532371260239,0.7831404852681272,0.5131606767455507,0.2927854846411969,0.1721955689624628,0.882689090212164,0.2943006744984326,0.35439207151688984,0.020788769394078997,0.21356592440899333,0.030765515419466476,0.4856601938813277,0.9729079660798164,0.6378954041239407,0.637468386953218,0.7333017755762533,0.3068444655808629,0.3650842823549642,0.18243860089639274,0.7841165231480762,0.01879035071504376,0.40446709291754773,0.2939932714813514,0.9269145515286904,0.7706430830711144,0.07201996545798198,0.6443656723788721,0.280446708190384,0.9683900377166689,0.11559918600696477,0.4653969323390606,0.6102221034135832,0.3730983510227006,0.8314882693381791,0.37485428281453526,0.13934622771519187,0.5531136301013706,0.08270406913999179,0.007199084231564665,0.586714568514514,0.2720081990428275,0.9980576632767665,0.805080245417152,0.9332972161779324,0.8355318610252638,0.25682984776177814,0.46533100511344533,0.7487528184564854,0.10464751151398101,0.7231117252691344,0.15674450636590423,0.9973034644508808,0.14274585552147884,0.7482996347153376,0.4770186723286265,0.20223783835148812,0.634252943968349,0.26505600689175385,0.9478280919945329,0.06852405873478568,0.7317894992725923,0.00037550834479097267,0.7864093245899108,0.23581984902044098,0.5215087342779278,0.6613986179703831,0.10941348151106219,0.5151775750419267,0.5244692659461113,0.00481647713819121,0.3839914693624292,0.6053263471109039,0.7752300388071899,0.02445448167585997,0.10887988246351261,0.24464945938409974,0.22350610724133416,0.21945495360426243,0.3308219499123326,0.2908145011013515,0.5314162600651432,0.7756639155382716,0.8131524078748863,0.6273635320236274,0.7274693780854158,0.2752493382699566,0.5023016565438957,0.08972013045389857,0.7691315796375365,0.3831097702835412,0.9549116810879753,0.6294126131605869,0.05521045111886029,0.10710783801577663,0.2610288003109698,0.021417795523354027,0.6732131970953188,0.9278249092624957,0.8799298197023565,0.781625155352813,0.09474885486095769,0.2644269313661691,0.07590521336110316,0.3066179386345476,0.28110953943002304,0.08733576997763595,0.6994163094343685,0.1708483047726999,0.15223622719756325,0.7087154644228649,0.027305463588860428,0.40198250338108477,0.8595969696270133,0.5545628070257479,0.34899769339160863,0.08728283357859488,0.04813296995043161,0.1119498367285223,0.8002787505048364,0.4296432604708535,0.3178216289488704,0.8354800670443281,0.3938770110182501,0.31986550593689816,0.5202112686632411,0.11608729419238097,0.9414162244877617,0.36295363585843027,0.43945834614028734,0.25666062018238106,0.8876616016469423,0.8861948720469474,0.30469637810280115,0.9980281878821342,0.5618224676395966,0.2792654716405067,0.32163712719751775,0.9587904095694464,0.18860028819579647,0.526238960654926,0.8692875646548744,0.5245549451666744,0.4475701952867215,0.793420176648357,0.3101459771834816,0.2931263241663878,0.9160937775365531,0.5165938710471926,0.08529801364422826,0.966275570892064,0.8482187428238127,0.6438071179145217,0.11642974691832388,0.8555109472733627,0.20005239918025267,0.32251158852679773,0.10698757820470035,0.6981757633165863,0.43025104680002435,0.9306820748305118,0.7737084815446443,0.7359578767107039,0.26459883248216187,0.20163173084146505,0.44843312734544405,0.543539289741935,0.21146420124198703,0.1641017866358102,0.20652350003225822,0.12971343043887593,0.3748264055940571,0.9318201338004632,0.7599879181392637,0.5315419919547264,0.3761899298120659,0.035128539600133124,0.2482462072883503,0.8631453696406454,0.38063483805372345,0.28708409193597223,0.8227130987800062,0.06095698608454978,0.770777015264822,0.9224597328281215,0.9014880857867903,0.9112475678755234,0.5314874144792852,0.9686879065606541,0.8500514167066893,0.8983557079234983,0.434693252590737,0.2289916725729282,0.24655772761827396,0.3815821340417064,0.8573699738531626,0.5261050456283888,0.30299393734335517,0.013961600222015442,0.0002779236664379826,0.694294599318416,0.7656943526979538,0.8635577093241493,0.8943159368899894,0.8946250187481487,0.20152205883424665,0.3425031031910697,0.7810820867606219,0.1920380658763415,0.4518424781336632,0.9041098350886121,0.3746027323854511,0.4711511361753661,0.31944499607206855,0.5652764412737414,0.8807009165809708,0.3625545680399915,0.6728995063500193,0.6634307781924766,0.9400707976540963,0.1981304210709389,0.6755222830320168,0.14187427026974758,0.9387873094519436,0.5787422371374072,0.956109391816003,0.5268088454260305,0.5346762854552467,0.07195364340175785,0.08176511947108463,0.5878834657157056,0.8146046132329656,0.4764033152031824,0.635004437030976,0.18005896648113506,0.6968021571577392,0.04723602502884747,0.6926882446846477,0.5479584745698008,0.02154548821890093,0.6795258818966743,0.356490597019287,0.22250610874659302,0.99618814412091,0.7738331560245221,0.47938066241232646,0.216437857818584,0.2637205510119708,0.997916721396489,0.457836378496878,0.8187465259726012,0.5040344840811445,0.8355952894525387,0.6671395881873555,0.43864955339690836,0.8578241572911114,0.5746733751388753,0.47867783672972936,0.18434818668549818,0.05436900861505848,0.39408138917862673,0.5797070249197647,0.10578884582363567,0.6258089661088424,0.6165302280898801,0.5520266498542117,0.4119357055884079,0.23030775515841037,0.237698680271146,0.6855953960037523,0.6137853656656709,0.7070552722292835,0.9155069009186131,0.026950257554114412,0.5592648531023954,0.8809816639375492,0.8328874564221828,0.8645509811505415,0.74560864836514,0.4111807019727429,0.7694534736583308,0.25575910190005624,0.3484182688116166,0.30967929146548023,0.7877299163782978,0.9905200304927237,0.8373308287945536,0.9143713639243543,0.10339668863854168,0.3854533525954334,0.07918087995544862,0.7920933290224683,0.7125224676115572,0.8131396657879911,0.3809334323576511,0.44631501612150104,0.1499692994922285,0.1386869334015369,0.48415255025786347,0.3084559170599875,0.3147949691491633,0.15201686133908124,0.618921796107321,0.9078837832700505,0.9985923094387259,0.6965107150190116,0.8524795672517325,0.13991315503780932,0.7927704345891255,0.21293889681682432,0.3258492567952172,0.695652632995822,0.712823424250032,0.5448610072006759,0.9881780977896075,0.5073795629749795,0.6296623211104988,0.13117638143531127,0.40684951582565776,0.4167732048666277,0.3580410431793869,0.3067989292071132,0.6184416462604336,0.5377593858806915,0.4959956998291612,0.6316917084219504,0.6420073939671145,0.356472813673019,0.9477706110603573,0.9019354111059699,0.09198213218749392,0.7108286726709285,0.4980171286578221,0.299091818893498,0.16748888036656048,0.5879231762264568,0.07715000188208782,0.598601681341353,0.42994232812535627,0.9758590586443289,0.8546282614494883,0.5558141747167735,0.02383254626675311,0.22134013496222943,0.19674547963894984,0.2400517868112408,0.8586642291610505,0.8682904052914261,0.8707806268445584,0.8756896699617369,0.9917645959744081,0.13557291722219858,0.9906627920357036,0.25123449674755954,0.730922529794128,0.40433687119431105,0.9371147791724936,0.36814584347888596,0.22783855959042176,0.013960955276441887,0.2516023642078181,0.42582218746316847,0.8190741304225607,0.10781941159815629,0.48665398537261384,0.7484371813080526,0.8188816177610272,0.33144685283897385,0.4966112879765934,0.9746012415441534,0.7308462152788409,0.03042784432138869,0.019102138659183008,0.9993291653699896,0.0483995426715746,0.20193360359070858,0.36496588556554305,0.2878378565913969,0.8316307753012359,0.8025669944854137,0.46740967848117504,0.362132009334824,0.4223001758821382,0.8789121681028846,0.14738457262486898,0.7775317117668294,0.3433289965700221,0.4558518499135022,0.22796865565465957,0.5680226184484533,0.9334558443454276,0.10451470524759486,0.06766392776192044,0.08769611936768829,0.8365808229826385,0.9395222824764428,0.012409970986991878,0.19977750306212516,0.9559252498377202,0.30083400009330125,0.24938120848146558,0.7074009877154273,0.4347557632015424,0.42265891329526756,0.6468905917185251,0.389485199759286,0.03763008719085161,0.711894463329498,0.3107934612656511,0.21247745424622388,0.7539681113162909,0.9272352547012264,0.7209485141583679,0.4512349760006892,0.46703397026532834,0.16351452321461152,0.981163437363938,0.5827238541135753,0.4527694635113058,0.53966401876695,0.2128666824770497,0.7254526146362663,0.22327976989338427,0.39020099961056715,0.4455829967493666,0.7476972704668743,0.8865374628364687,0.33341061731783606,0.7169261444596577,0.574554192542806,0.5639562349984605,0.807453793421946,0.44445647208512945,0.028914844798533297,0.10387657608921433,0.03926629593653048,0.8959797771116903,0.6180660902447194,0.10360633527283403,0.780526928391168,0.4044642391900959,0.535384974323806,0.20650451218400168,0.848919033622146,0.20698353174659223,0.6701236791948981,0.5367599997323634,0.4922896015206132,0.7588771022629283,0.6188305664796401,0.361936618067231,0.06547743392120048,0.29202053235733005,0.8105231970165734,0.06298654234766951,0.5987534577576721,0.3070079029967278,0.4420639754861468,0.7648808987852705,0.13029911564180338,0.7214677372986789,0.05390863083683341,0.4815861912590396,0.7338033996547187,0.9682609718863633,0.5800669327552529,0.5641109021870732,0.8970873736732408,0.7909200104024493,0.2927426767221122,0.99834644564813,0.6980904823438273,0.028628917068793364,0.4649266800613516,0.35884764665341873,0.8720973426904652,0.6569829253786464,0.4371232330649943,0.8624226018111136,0.39094227303556295,0.48932727574965906,0.11786965303605691,0.47226635874531714,0.486205137376524,0.8904944461937692,0.0775517109321741,0.8461043632697797,0.339541111605939,0.49097395508755803,0.278043813684981,0.5471053902820477,0.805700382583553,0.1402717003576785,0.4953904998399821,0.03984366817506048,0.8764636355842647,0.4022148056470241,0.38191510964320075,0.6691503269017022,0.5117394961094586,0.6998475590508115,0.11476362871793236,0.0210664115703712,0.6088532752829484,0.0691342832373811,0.09811950747682506,0.48183703798923744,0.4299403070682153,0.2039055921753414,0.19269027321467422,0.9449580365294802,0.5292213041826561,0.27296610060693605,0.12142252894541998,0.9058010677655981,0.13356607658529046,0.516004280514627,0.9271850455638933,0.6636332650556914,0.7817311387933409,0.40997764619102184,0.8756520273654805,0.22124659692598575,0.4797675268545993,0.20944471364562162,0.6689048347729393,0.16207831757248506,0.24640767772945152,0.5435188372267455,0.4550798822451604,0.021238641538182255,0.5284913456598718,0.9815483632824497,0.6373423113807525,0.6651170640151207,0.5550415479660652,0.5515077101774158,0.5009744118862386,0.6495653933868744,0.2138332255208208,0.810989923997336,0.667200682772182,0.4765184912439491,0.30279168026421494,0.8526726188001943,0.2866755068166612,0.1546769083890607,0.6778779753571286,0.4204626700440394,0.7171812323727774,0.24486826131710315,0.9527878474608347,0.010561988693713097,0.5473508350846686,0.1583496625893797,0.011630602392797089,0.6110132636267223,0.8958943873744325,0.8093689013952824,0.4387112383950959,0.09377315813263076,0.15138223725341837,0.9156737977544596,0.36302860357275873,0.46442069387270546,0.1778073860719071,0.06278117271331216,0.4194066052937284,0.5499894768319239,0.07409075675139527,0.5141638339896174,0.4306612884578134,0.5048268446818623,0.6139446257908904,0.6597963575645857,0.3014999742338043,0.6682334024732667,0.4039194199785094,0.7034222540599417,0.9123911897254599,0.4420746155673202,0.7716669868485843,0.25252025658169025,0.04564913601645763,0.25292802133701936,0.9652806709990837,0.8376062443263335,0.5467165192865665,0.24734512008505322,0.23577975302621473,0.16328313315263523,0.15824059131003365,0.9015513775736077,0.4900152524157921,0.42280797434790024,0.3854009748803435,0.21242098477096666,0.026584758225125817,0.584698525162613,0.8623994089191466,0.4926224194432707,0.6307056102041775,0.7029927834077949,0.8786151132549509,0.9191132095362687,0.6219792985016532,0.3777965946574692,0.21893437906689034,0.2681098403002806,0.17508886700998127,0.8300119392455136,0.40109533988690654,0.8564418587400114,0.28014597623062704,0.47992326416957665,0.6019292161436743,0.2911465779475347,0.6503936761550227,0.2219624810692417,0.7382521843973605,0.8147732888696188,0.10888288290535125,0.12398487852560311,0.24793741031779098,0.6166239400237872,0.8381283819343823,0.036314209982841454,0.4690074563539003,0.7998530050883423,0.20144153222373365,0.5365327097504786,0.9821731564122277,0.11659235399759399,0.9618050071825293,0.5266087742089628,0.6756171691841045,0.9559364421337343,0.31574124089139044,0.9878008658911717,0.3051666398189257,0.3160036473946601,0.31095314861849005,0.9755571676477309,0.03889775892566315,0.4969646414885529,0.5048636400549713,0.7843909720867932,0.39391700100856564,0.6680964593179967,0.5893283986614826,0.7515471760478323,0.3953863244948736,0.0994689892727555,0.8496521892263297,0.7662640913085856,0.9415771084336315,0.1370498421477762,0.6220866775988004,0.9061929938277892,0.4756759115226604,0.16903235486140147,0.44973491709345526,0.5310964645069857,0.4551204388175795,0.961299612099518,0.33058901531046203,0.3942546674497719,0.9675170166872382,0.7622025265782394,0.6119851345703333,0.7441625803028945,0.08077668239563547,0.23790045151443526,0.05166557861952966,0.543603323992737,0.8290409874671063,0.48964559648208256,0.2843274799035865,0.2957670161180451,0.19069679346343715,0.5164926073691527,0.3456557424223925,0.7197758964002453,0.27332310360077805,0.8043429404938599,0.78209135226154,0.9092031721516551,0.9613181572111301,0.891686578759014,0.23580010974850554,0.7191273440875074,0.6061300215937476,0.6115944625768734,0.07407594109497652,0.956894950630473,0.7487748784162295,0.6855350445984028,0.2670794457273647,0.8938739057318279,0.45655260649402657,0.4991632645761965,0.32504559937609506,0.3796429513110378,0.8832907369965957,0.996656913475875,0.6131995280909369,0.581917374375152,0.7201425779457749,0.0062848131005610085,0.3240910069801741,0.9552453137940339,0.4295737927220252,0.9470328153268428,0.562677037309813,0.3487648535639496,0.7838552325224842,0.7302186041451468,0.9282894621343127,0.1347004981617596,0.854048663169588,0.10430699035609443,0.34559379756344866,0.633516670537387,0.3632331237107822,0.11229530304792809,0.2472458325308414,0.5087979957133905,0.3800912481170157,0.5651174402282788,0.3942624444279188,0.0839103022766655,0.949624566331361,0.9865909380661434,0.025871433481403708,0.7199788867272519,0.5293324251789048,0.23069820347844686,0.9884188632412643,0.8627461576559049,0.8192265546022562,0.23257730995363607,0.8505408921824598,0.9758349428708383,0.22531048326949787,0.3702027430951089,0.1091707173394688,0.13673613629362574,0.60831962422357,0.8338810397497296,0.05346458580198034,0.5943035909838711,0.5064551931615942,0.6334060838416439,0.4562384928926538,0.3970038605733465,0.27764038432074417,0.07161018606317877,0.3367815787226647,0.2846559692479154,0.9054133125958278,0.31755286815678296,0.3766184557773529,0.6135226306900563,0.010443829824400976,0.45442810223105157,0.4214353638026131,0.8582699121822298,0.6721048837601029,0.9218747614350988,0.7621170186619237,0.6167104505066726,0.8675102113519312,0.06328496022943557,0.7822869437013704,0.3358647308791141,0.7332666014431359,0.43975879087412006,0.5211884174975115,0.06036272488433513,0.14568473982438856,0.13549675901714212,0.7941961940188086,0.24432428263268413,0.49676135168969804,0.9806607743494837,0.7717504431067522,0.04446628731633621,0.23542939745985184,0.4790294695942475,0.056603996175308764,0.32350752220964296,0.480001584679442,0.44243194393973195,0.2791219168060768,0.6747560678561723,0.01751904009824027,0.3186297929946505,0.8403860377067625,0.8442537193435358,0.3368582128038128,0.99811952588984,0.3887069715887558,0.8978360687058378,0.35048390713082406,0.010481622851807667,0.5545349766864639,0.38914134976472337,0.06628261376184746,0.23572909227467603,0.6339774904639246,0.3664770991420921,0.47673619913387355,0.8128528826548194,0.0262637057445198,0.7792470397606593,0.46058667193605146,0.7329986594452302,0.8473463842915817,0.7400480033601247,0.19252414639750456,0.8187880210710793,0.40189077404285994,0.11131818068519816,0.8050368893206289,0.22917118432051967,0.4903815887082826,0.8857123452298041,0.9911180298091821,0.47999334963541596,0.5353118246981926,0.46096503764713354,0.7078266549876614,0.9965938504308355,0.1515725380166611,0.6363132293648193,0.24230492015931743,0.08637992245015767,0.2357339123461478,0.572623663123219,0.2303265544742275,0.2538729345658235,0.5272191602107601,0.8544088718210924,0.6763528593361752,0.10051579081216855,0.17932074390224975,0.8915444603704357,0.27650821294601224,0.5758873068813816,0.9966052714933042,0.15608703713777314,0.8548286997653514,0.7793353078508866,0.2783237354114593,0.6219558029813116,0.45215075278836425,0.35294014748801866,0.9159157500951463,0.23773241844412904,0.6728788369918219,0.4905629242637403,0.025766753265320914,0.5292437006225472,0.3182970443474633,0.3619533249915715,0.8748325854280947,0.5576168311840786,0.7090533398124804,0.3114260300217663,0.5365567197759181],"coloraxis":"coloraxis","symbol":"circle"},"mode":"markers","name":"","scene":"scene","showlegend":false,"x":[-0.3529580087309874,-0.998346580017809,0.5352466319749282,0.09205031392261587,0.5082656637871851,0.292195942213566,-0.12333345456793607,0.4022419172645254,-0.11179356235991962,0.7078266541483921,0.35644810606161065,0.04731966575963832,-0.5381231339525427,-0.9008727494644494,0.8598031963697106,-0.375252698226276,0.2889611155006437,0.3611750917542397,-0.3435409975701145,-0.1320290892640705,-0.7388767358563832,-0.4873321792050252,-0.7039975747476789,0.8181156574746361,0.03018435357474002,0.8811080915928551,0.4276778495611921,-0.9742235208015356,-0.9768038004207132,-0.4986590559236755,-0.24599785084564857,-0.0846684284735851,-0.11360305445949476,0.5311033409919601,-0.17410562974156024,0.43581004175778937,-0.7163824253540179,-0.6418831507037968,-0.8465086003552592,-0.6757233782406192,0.38928935273088316,0.6699710067942007,-0.026547924721101047,0.7868312058054264,-0.6050453309518308,-0.3245392706574704,0.8943856826601142,-0.2127442954457689,-0.8695180293002276,-0.802064615688032,0.4360189183441429,0.5947574142155433,-0.08663037899804113,0.4378241623668614,0.5497801337070887,-0.9012999724110011,-0.5832195755074115,-0.6671120601608079,0.9886591965376755,0.7257282635405206,-0.4295579463916339,0.8977955169995313,0.4871667481702551,-0.23314655318612346,0.5757839766942068,0.7344848610288377,-0.5813137802901803,0.8151568481837278,0.1515596440581531,0.8487074949086185,-0.026903508033207523,0.15769599612581442,-0.9969681259217383,-0.274772131723223,-0.3765064484734786,-0.9207576020670206,-0.9031307388552964,-0.1358503509301454,0.20290351443261834,0.3546521756199258,-0.8485025398756683,0.5972979316589696,-0.15517987297328031,-0.6315354417720209,-0.925447293053337,0.17246707690932617,-0.42892597400773924,0.07174122722719305,-0.02965466449883335,0.7304965262385709,0.8563644912959,-0.2232489264753058,0.17760793196469074,0.21441071697252154,0.43257471090811844,0.06878114263247921,0.732265596104156,-0.5349924291588521,0.4021827999651534,0.17989598709065713,-0.3260337614738683,0.7224983758002109,0.36057379016621993,-0.18801557248959255,-0.2500750583462777,-0.9937669807922378,0.40901581581906077,0.3798978484751246,-0.1949256568721354,0.9983741462353298,-0.40338713753116395,0.1674112622233676,-0.5907499496082143,0.8975291499619638,0.06817807725877909,0.9606761193241085,-0.8417891599395062,0.28028240916841046,-0.4936273603702271,0.46067106647365547,0.8384381456483613,0.35217629375673376,-0.5119552882335838,-0.9641327322339015,0.232363226953799,-0.050891398857254855,0.9691156302179873,0.49948178546823924,0.2628178802366615,0.5082628097789028,0.8088713334603649,0.3192565326550036,0.3450225187220427,0.7350199824916861,0.1401044432775034,0.16822986841849435,-0.8958579222065046,0.3053217027640748,-0.7123288762021822,-0.2733231060403734,0.22592119783976275,0.19721684253128477,0.014682613156189418,-0.17354196662660043,0.6922243340512388,0.6278795860584034,-0.8340062079649507,-0.8217450401063493,-0.8281900512674958,0.3568433252369756,0.6791353555680462,0.3621187694618875,0.7249828665081478,-0.9875343180668087,-0.5595662313369338,0.7919457015302281,-0.5373701737721361,-0.48127407575532055,-0.05447008425839609,0.22642775572067675,0.49246871410790455,-0.09664864406456398,-0.9235210759661269,-0.9154114228207918,0.11484896180248173,0.05803209234075524,-0.993232912046789,-0.2835890937027216,-0.847445747460122,-0.816906168639392,0.3941010085369425,-0.6428237062607266,-0.12808261666856585,0.4850348835934441,0.24469219432019546,0.07226224665607024,0.8655877655504399,-0.39343022462561095,-0.7863342644580844,0.17312685187967733,-0.6162832912503624,-0.7875483467688936,-0.8148829792152579,0.7173422298256894,0.1952399508834434,0.3952769781141411,-0.828301226115352,-0.16840063455125237,0.5684651112341055,-0.656992548856304,0.8105262346512604,-0.2412615036961166,0.30794911903652344,0.6657538023114592,0.3918263617991542,-0.7478210833545683,-0.1490624410939114,0.723416674744311,0.28035569070150174,0.5391401881142915,-0.9681380434093505,0.6024480690174769,0.20424013904270505,-0.9078084874527426,0.5936502401487201,-0.7112674572287848,0.3602438329881329,-0.09344301258145817,-0.3942636315067982,0.8708256873455438,0.8208908202906922,-0.7181602282424435,-0.7491980205047509,0.7406523999299489,0.9354802775385432,0.4819459404061966,0.3736369131979466,0.7142459063299147,0.2339237808040033,-0.29044808725243076,-0.349484696389117,-0.5512078198342688,-0.4571931465067322,0.6697619798945701,-0.07291779600796541,0.1867243138556899,0.03929725649504994,0.45120401867980703,-0.20048013801098352,-0.03807629009623578,-0.1881704267679866,0.19096804504971496,-0.6305280992374469,-0.4794873424465099,-0.09536856413401035,0.6365624182303109,0.9755095039088215,0.5539889812804477,0.9290614339598797,0.3529200116594151,-0.4862505701467149,0.22016284799015154,0.9686405177941354,0.8936096921392699,0.803906001367679,-0.43035036066303156,0.6454024461660389,0.8805599812479209,0.4652149593835467,0.986734314249109,-0.8389092492478097,0.6811848978547193,0.06975810908532316,0.33097253677410093,0.10677346284473482,-0.128887364813215,-0.7665664955124021,0.059986015176781066,0.5115262437559308,-0.9063664133242318,-0.7890249830342696,0.4306476106686732,0.46042188427068625,0.6206112506244071,-0.13135777601097817,0.7723285699210369,0.9101091855694349,0.9271903439022684,-0.9716695138096189,-0.7662671782654381,-0.2529676936428737,0.31403306537194714,-0.28134628511410664,-0.26162939311726646,-0.4010622012363133,0.8567175022248118,-0.5041001547933465,0.646669884498152,0.3998290093744583,-0.28220685613392416,0.04977307392106575,-0.7069187554173627,-0.5058605395040984,0.7191696759758772,-0.28164953176553376,-0.8353135240885986,0.8223656106726122,0.02323506544149289,-0.629648329894493,-0.30491445256635913,-0.8581998212609221,-0.6091367771817043,0.6422673517313129,0.3631830072127156,0.10808529021857023,-0.20183135005428496,0.5181140693115566,0.068554505766499,-0.36405426486535214,0.23485304952829464,-0.9223729035751596,0.21731396013455812,-0.4962467561166972,-0.28862869303337585,-0.7019818344572396,-0.08779528724053925,0.0451451512946406,-0.8418580908612925,-0.7290644346846922,-0.03925431554396362,0.5020024303713843,0.5982544697811897,0.5687238460709126,-0.8981975651370961,-0.8179664505307216,-0.6786725631292991,0.14347165656330332,0.0097285107360117,-0.4851122400220069,0.9183062062801887,-0.2929120954148437,-0.978204185663253,-0.9541765510075976,0.32607515320517333,-0.9370068991462468,0.8988901702648199,-0.21615115210452332,0.4722462996156844,0.698781629686664,-0.2287057646791624,0.8884485958081882,-0.36331882813378713,0.015363704233731287,0.6265708809231104,0.7696632788315849,0.5288898140312399,0.25579362663990834,-0.4128394530782888,-0.815581771314701,0.17319500304194174,-0.8413453960673781,0.2667140372672119,-0.2852329859357871,0.586715086887197,-0.07463797742304415,0.20395637006626827,0.37744738687556695,0.07301736842576097,-0.4520171771843859,-0.40138601529307305,-0.16393785830280988,-0.5699633796039291,-0.12735231753647858,-0.5893204112805089,0.9549484963457622,0.15712250693901147,0.33417580077630915,0.6142312458434904,-0.5226223904328239,0.7274954066394318,-0.1868108104333902,-0.4644329753068083,-0.9937974913979954,-0.6721908055511179,0.23643898075071904,-0.535343562398656,-0.37008874919088725,0.3218438671665137,-0.888395169399196,-0.8120776595186517,0.6368545892997017,0.0985208613744113,-0.8420868537720375,-0.7839785893087017,0.052311074694074554,0.9463908790265649,-0.9307699341820023,-0.4129057518969408,0.1876370764429893,-0.31218215501226804,0.49497272445236995,0.41315318704171655,-0.45573559742974096,0.40101425863989854,0.06933100016157627,-0.9381245917436786,-0.5560821320344294,-0.547613010698354,0.43495081537878977,0.9115828191428375,-0.3299547575843123,-0.7097956221166921,0.214620952661623,0.7175929877673259,0.9872728480073282,-0.7337811555829954,0.4585153546175361,0.1412974723439327,-0.8955637498639882,-0.5466187534195759,-0.554504664274689,0.6085686442639318,0.8378701183379171,-0.3885690991884463,0.41993672215244576,-0.07617607139923675,-0.09400744037613042,-0.6052929570103532,0.987988412709329,0.08349147579327022,-0.13007160164904066,0.5529931844506033,0.20166752909129626,0.6237803021877392,0.5171242109017002,0.5450310510827359,0.2086290988199369,-0.4823802854572454,0.269007184747826,-0.43491950766403314,-0.35649246774881077,0.2845916467972401,0.08985178901986839,-0.3034993488250139,-0.42395211488255274,0.23924195776844542,-0.9494739892055899,0.7966843409424661,0.6724832867998558,-0.8372196028752785,0.19689365632298944,-0.07956368153941162,0.0889262912100302,-0.7422679492413452,0.011404635110285138,0.5035666572131987,0.6059585061914888,-0.04708233367324643,-0.39518585623381586,0.992238783515612,-0.6510983789560894,0.669334128897315,-0.8745004929105756,0.5151761581611838,0.7157553222252035,-0.6801391343829359,0.44407418708889773,0.8260979819538963,-0.5743524832593844,-0.6179024211854964,-0.4276480459754839,0.49382934857180594,0.5191503290202332,0.0004565056544428246,0.39111175141417004,-0.6348619424315258,-0.13810979634939488,-0.8428720379503992,0.10768087872439225,-0.6470297635574576,0.6031978657883605,0.9566540352151349,-0.6599124716155058,0.0001237442054191125,0.30893798813336676,0.19539603211271203,-0.6200962907622054,0.2454385326776805,-0.7694009810854054,-0.016666958337143982,0.6964698602359403,-0.27498160990930803,0.22972872658751556,0.010102195705310037,-0.8206572686393233,-0.4511239310030796,0.48318101023461985,-0.006796828989137288,-0.4595031386608338,-0.07488738290362885,0.3254977552377713,0.228848884154647,0.9305014253959826,0.20608471829105876,0.11979821878855738,0.24885316470888896,-0.5042592181939899,-0.44191260163509244,0.2602005914485141,-0.042742161086249175,0.41239688628002996,0.5863393100205001,-0.8813841828063198,-0.10717991469128023,-0.4083676277482796,-0.04475049565570391,0.6312423364947671,-0.9882311034687293,-0.3482967786215266,-0.05764442897416476,-0.3145078657473813,-0.06303235270889583,0.6147634349011795,0.38688710730955456,0.591708095083844,-0.20414206975280855,-0.9567791400353775,-0.1445339713995281,-0.9708228370781232,0.24414458867224528,-0.3815042822564681,-0.883845392211964,0.8762340748384461,-0.07376993464186339,-0.4121587164421731,0.009319155132523183,0.9599722102894078,-0.2013250305140945,-0.71930834934578,0.40603772194298093,0.019230804307560505,0.9686478448191647,0.9539695709155203,0.24199307028892983,-0.5628362014111777,0.008818419821763228,0.298047079852873,0.7247768528848493,-0.5365833747528033,-0.5141145266743835,0.950988423527398,0.037396949996359036,0.5262464099300893,0.8050002945118663,0.6092683198951264,-0.9207662440751968,0.8412208172580166,-0.667541967787625,0.24824650150159228,0.5974096138968039,0.22705820149167602,0.39298374544087616,0.5074868698991031,0.3389876457176909,0.7319313526819555,0.8492719209418375,0.3402228090456585,-0.3972736050344993,-0.0657292858791921,-0.46037791776138043,-0.6277056101991483,0.5061276754104015,-0.7788950173812267,0.0764362553874373,-0.18734600162716514,-0.3934934710573193,-0.04478443078977941,-0.9469537554228903,-0.8536952070534808,0.24913025834587782,-0.24094618307139415,0.342123599111284,0.32927876609759077,-0.04780548224795877,-0.5920513281687401,-0.551745959166936,0.5034463467511215,-0.5070429298009778,-0.8430791770201487,0.3795894804598266,-0.5321390407244341,-0.29212885911819697,0.8155256632431944,0.6679033076685513,-0.04818691243073022,0.04714055317888901,0.1563070803733903,0.4119532372723226,-0.1633002342622409,-0.5016345042996364,0.18256607961761784,0.44800223577319254,0.5213159522570935,0.7216873338934673,-0.4496517800508163,0.3189626264940012,0.6876949214415603,-0.8358628447691878,0.9882845674815995,0.3683038110349327,0.13709695711110173,0.21210298014577156,0.3755639297244414,0.4789363855142389,-0.07697738528405813,0.44532615246335616,-0.04601883859064774,-0.16252330296731202,0.7290986169046841,0.14645399175566992,-0.6898773501072836,0.3054578008407086,0.5027270029015708,-0.77425646769819,-0.4630505223516282,0.5171072067848258,-0.72434252097702,0.8529914947147387,0.9795509684038547,-0.3022125448057805,0.7288370220683028,-0.4748265983766806,-0.9759780271777486,-0.25233321215653615,-0.5565512102618143,-0.27285535386750487,0.3189338988613805,-0.6286828688077862,-0.15057926606627806,-0.5186849376400726,0.0877034477802448,0.622155477111434,-0.6653434042848785,-0.7814416166679808,-0.4800861094090666,0.09815501278632434,-0.901062672893296,0.05133276052781051,-0.672479225514505,0.13656229909597434,-0.5755195418374016,0.6093227401249117,-0.9989670343487671,-0.4319939800544511,-0.9931557581358265,0.7571464042651088,-0.032956719136728284,0.10082672001075092,0.5896592258647441,0.7254287685292741,0.5523411803140369,0.1162015999311318,0.2631919296296406,0.4949178845274348,0.2221001677702385,0.1310412926053964,-0.13897839554437,-0.336732892500289,-0.30769785295198476,-0.7302264536056822,-0.9903500847834632,0.8693448371788856,0.7243078341444802,0.754215494149364,0.406274166740649,-0.6955176627857439,0.926914809313182,-0.489996522360936,-0.3960135796941391,-0.968549121941417,-0.1866531268385198,0.9541341725620393,-0.3706148005229247,0.9017061498989029,0.7097552571582707,0.34863679584528984,0.06684380282311982,-0.3119041572572384,0.4423189923716685,0.14483803490722108,0.49790065506324327,0.7027317497082436,0.9955956415651349,-0.5163193985533383,0.44873716488816673,0.9337422741834388,-0.08871319213637403,-0.4991795360511398,-0.5794737040852656,0.9829172840759639,0.7251105293338524,-0.678668994938874,0.7076166635575114,-0.8789043522973173,-0.46588726983871037,0.737122915592786,-0.8162322909127107,0.9811824550156959,-0.6998024657709803,0.3862064013965334,-0.207787725137142,0.765734374098827,-0.0926592623994296,0.1466140253223267,-0.9890788916613248,0.4026685225249743,-0.03831716010709991,-0.34091250777605886,0.9376523494573074,-0.795453956452247,-0.6440970913903518,0.3946779945656868,0.5374488386140408,0.06030330198235179,0.7372094661244523,0.12813874769166791,0.3915140806445881,0.026695474117314388,-0.694120607905409,-0.20014950473326046,-0.10541352619116452,-0.7170394945585744,0.3610227477828007,0.6954228687195337,0.9865387183311692,-0.5160050834955111,0.790051264900573,-0.718377354058615,-0.34458547877972595,-0.9138186545779243,0.15278553178939255,0.415308144268994,-0.7110436700102227,-0.7483301200058766,-0.563749188511183,-0.42849569133768234,-0.405177965241568,0.6666513096032156,0.8266576619630741,-0.44134651368869093,-0.6286204429538013,0.9842629150496286,0.28024092427662883,0.8639594688851598,-0.19215042682031552,0.7728731456933025,-0.1942293601109603,0.552533612567686,-0.3794937727602721,0.39926894719441475,0.5425052127157499,0.1892486213767116,0.6032205149866984,-0.6534762848126731,-0.6315626347353122,0.36425969296823135,-0.9345035780678805,0.6502356163889526,0.606298002279375,-0.3529749122932374,0.35551955197135054,-0.788961132615802,-0.48573975866973396,-0.6172760745009994,-0.029963240103306425,-0.4364199880614526,-0.45618860268022443,0.3813457211388789,-0.800540382985958,0.4621121457683872,-0.009056671930198665,0.057525384149094694,-0.9110365558171796,0.15727030592293784,0.4327974482279971,-0.8987940661293702,0.14156659743761588,0.552198379073504,0.3123357331537268,-0.2687641948544692,0.9323738391480876,0.47267460082531715,0.16956287825909017,0.5355675179147578,-0.7902128508991231,0.0735825403117222,0.37901780319741857,0.34131596450309376,0.5739953496655288,-0.32750407589315483,-0.033540417372250345,-0.8643491638902947,0.6862514316076604,-0.897850693717334,0.6646997678530425,-0.21244326607526987,-0.3290017795121539,-0.5252587429164361,0.7470897689144278,-0.5954334579869458,-0.3881230463823811,-0.5494004902335163,-0.2343644134226654,0.6754166061072104,-0.18696011755139028,-0.696140194758802,0.7151339877416056,0.0393553635314338,-0.9124549903806201,0.5253130870594305,-0.8856120171033638,0.49794362852675295,0.039341359780414725,0.7156842276134623,0.34249437238029456,0.6990768072581357,0.4943114129870377,-0.4704641804531455,-0.4306060480256981,-0.7107162320461214,-0.8382620631778347,0.3345017642271409,-0.020021471757612153,0.6454125976062192,0.6371693427931676,0.6823635210007603,-0.5118179765486836,-0.2581491489317703,0.4170596802185537,0.8555556998368945,0.3336727408737794,0.14002114181636852,-0.05029561260848156,-0.02529151331019743,-0.21173332877355586,-0.8088525913219419,-0.4490081215003929,-0.7649646624064766,0.6729519997957965,-0.8196171053120983,-0.7716842279703533,0.6945535612849622,0.29861637976188005,0.5544599578787445,-0.3791703687340766,-0.2572159260089484,0.36860914526841887,-0.3446692999841341,-0.8318160589023126,-0.47469003016324396,-0.7263644580000598,0.43143717055407493,-0.0072385284379766915,-0.9735272921155086,0.3365308024278501,-0.13843273150159452,0.993478599559216,-0.935253814667192,0.5129501401552871,0.3959154023559952,0.24826800613884797,0.6684351262696057,0.92138983569273,-0.8568120244031184,-0.18666450850601787,0.13585962789053185,0.2815177510739716,-0.0008372904680372987,0.1115928224658226,-0.04695044149870826,-0.7597169832472355,0.9909115759214766,-0.3441781721515999,-0.24657260821852842,-0.11552611534686232,-0.9438657269434827,0.12955126280548343,-0.3169930932417681,0.5300482007914263,-0.004820071636043091,0.7052144423853994,-0.6380003526756323,0.8294910141201439,0.1977072701008624,0.04723066736274273,0.585461006317827,0.907987407004313,0.8437692526052474,0.3996616652302444,0.9777484432596967,0.6497312864442099,-0.1124384375056117,0.11689118655080646,-0.5073688661337943,0.4559545326803361,-0.1513171653003318,0.7524578732652871,0.8010385010676805,-0.5143745820246108,0.4332115648208375,0.39940189868496306,-0.07373060175720474,-0.16183708280413683,-0.8249721144058275,-0.7882968373225774,-0.4138377856684207,-0.6005273901123339,-0.8477894010520771,0.4230548425633882,-0.3660693171825644,0.4845044867210189,-0.5032031512840094,-0.04324938412443962,-0.049556825229392906,-0.13882729785266115,0.8331315470994322,-0.4707068382534562,-0.6964842590589967,-0.1378843207985569,-0.9330806206708049,0.47338381736300644,0.7453362770802766,-0.9824060730153593,-0.5207585337911989,-0.59503573160301,-0.7939181766896665,-0.1801117486927003,-0.8082521786441055,-0.2510179002905552,-0.8567794230982512,0.5869275570472015,0.3944568516014985,0.13619414859697546,-0.03167447964838698,0.8650552919203355,-0.1824974689809614,-0.8061532623748612,-0.7318727157261736,0.14566177549558837,-0.07208958888606232,0.9242192188069942,-0.8341957489732621,-0.1847834765298523,0.11399805997989046,-0.8802733159261065,-0.5669691352750705,-0.8109625031922968,0.0355151553804304,-0.2781631845850836,0.32197351958521997,0.16820564363623616,-0.14647953757750748,-0.003549783040192102,0.6188140047705286,-0.2404970970589643,-0.9787241459653446,0.5370612470257884,0.500609947503005,-0.8014347133432974,-0.8252579774614491,0.1034523636101178,-0.26361055768895747,-0.5725641097032776,0.05075087667790404,0.20518610416382344,0.4086547354807655,-0.8991335982044982,0.23817584535512268,-0.3095414588111379,0.8277870995278014,0.675278779372948,0.2850260094625921,0.5426992834309536,0.6452554838399498,-0.051573886245491314,-0.10157479620641459,-0.6442064432478716,0.767600153030007,-0.5124571781655832,0.27978986515166887,-0.43294079416201753,0.4394258674231476,0.774787885398532,0.044647557108582835,-0.22301060570176115,0.9516002762931793,0.009253371730026985,0.3268133864119179,-0.6817500957001421,-0.4643071478616771,0.9695729011828615,0.3746448028553827,-0.9276759537749407,-0.31153244692947507,-0.19247040899729004,-0.9958091243412457,-0.9878828554382237,0.517707480684874,0.5690031915497132,-0.8989279620817863,-0.05673987755387928,-0.8963326853301601,-0.5583639326419376,-0.04206174141124058,-0.4882628546850238,-0.9507037868246418,-0.2088904584616325,0.1551592237335043,-0.2739828072301051,0.35670789741136366,-0.483870480172083,0.7676878771101816,-0.7350853599053674,-0.3262729515327614,-0.49042199156237776,0.0755336086396498,-0.9145594383611817,0.7023993049175408,0.6030883888353433,-0.9952260874482642,0.8844744814706283,0.07268157727297064,-0.6233793219664139,0.472371541561798,-0.27865006939780085,-0.060693639361586546,-0.5063854743196067,-0.02837018572545769,0.48155880641784987,0.8265466207806984,0.72477401284178,0.18405672773570037,-0.9546500645328329,-0.6545944709432804,0.41138747191962544,0.45536804514036694,-0.31524592575695054,-0.11438448260403171,-0.1259679356033278,-0.3151525333662902,0.5857755140081305,-0.7335424608575811,-0.9666660382436105,0.8615467985310516,0.6524482517599786,-0.4400831942429225,-0.6114936596359203,-0.6360007198085664,-0.23773161176534915,0.21613970463988214,-0.09958877398345202,-0.1710793220791865,0.982603945903759,0.5313541636587142,-0.0381069412184387,-0.12229519099895778,0.48879929660146804,0.2895533191013733,0.6268086093757734,-0.6431545894298984,-0.07006403423507984,0.2669832391386198,-0.3419435468681426,0.7560521144198966,-0.24710858359095525,0.16013713661431458,-0.2480056285340126,0.3185916112407446,-0.12683079096355845,0.3929393264156698,-0.858613011891548,0.9002004592856653,-0.5465662549000581,-0.728631423846322,0.5033199863987633,-0.9131293591802141,0.8333355637110599,-0.20665499818389374,0.14784995342807028,-0.6065107479907456,0.5443164132135434,-0.7194474928860636,-0.538233273185388,-0.7348018773129524,-0.7047668656115262,0.3726323877491109,0.5047560274497099,-0.9788513069144422,0.31879932849391834,-0.5013556093502729,0.3803026885394827,0.17983821883753534,-0.592946680713145,0.38447560737156905,0.0016984299264001513,-0.6482914776039911,0.08445047319835941,0.5645937107374214,0.520740328305822,0.18634036726316378,0.01454399744998931,0.2497142078602634,0.4121270654833975,0.34093201387408045,-0.6510620229448908,0.9497009248301477,-0.5198743933531996,-0.6560364112756781,-0.7962456198662794,0.69748701944403,0.6120018650792669,0.7331672812228186,0.7077041264620075,-0.010238573439535941,-0.39949834757055813,0.5400814559545376,-0.8277418255708924,0.34794387828360224,0.30519957395033875,-0.3652623036670866,0.031115092314994187,-0.7993718745853368,-0.39805633351128245,-0.07449770674553255,-0.45265854977048403,0.159466882698406,0.6588169726205276,0.42205194453406847,0.07300226229203732,0.25365530369426387,0.15210019103716776,-0.2929095267670218,-0.020248331416817766,-0.3409576782602771,-0.8845799816602151,-0.043054110192114296,0.14246878796940038,0.5145364265781127,0.5532468413141437,0.9756197499380149,0.21391448385030234,-0.7407458963041661,-0.9260640449890825,0.9418026663226122,-0.8721187746491611,0.07975987463935552,0.6326522810739729,0.6558698973521293,-0.3032550701248814,0.059474523449310465,0.48329516819109064,0.8518068870854583,0.9232280359621662,0.7890370508928494,0.2467959066070116,-0.738186835381118,-0.8070232458347594,-0.7686210414075686,-0.3551728479700325,0.21896008617897209,-0.05488633515291067,-0.2012914276671968,-0.7298932708876337,0.771971513009868,-0.02765671231772605,0.3053700485597443,0.6835681620977619,0.22579648098313027,0.7711589710189264,-0.871275679802068,0.3571501664534237,-0.405480948418953,0.694641148130873,-0.9281230609057839,-0.7843893171195864,-0.33037797856441936,0.5410604942980118,-0.4930215476650766,0.018761696291874463,-0.7697564275637522,-0.6045310304061112,-0.19099561403593476,-0.17867611445485948,0.5322982449083464,-0.3698790801675986,-0.576691386900812,0.4148367889939825,0.7407446319320284,0.827123972597861,0.3146544728060517,-0.6358964835099383,0.7857642831443722,0.6120747640244025,-0.9319405363922447,0.24815152950075348,-0.1631559265288155,0.4245201620287518,0.029502706179142864,0.7877709583220277,-0.5408651959476278,-0.3561082888244174,-0.9230028290456721,-0.1982115325987794,-0.01932846765023057,0.6130257721511075,-0.7559926164354418,0.5569078888187603,-0.025778097686718814,0.44963990579483987,-0.32858013479940046,0.5414080954442433,-0.9433168636222898,0.5967827544748688,0.6256401077880477,0.6640655103983839,0.48677307460440405,-0.9866177230723614,0.010593034957855865,0.7289170800948647,-0.6166941106866907,-0.09329806144745105,-0.5398395774414229,0.13095893813567536,0.8342181591926126,-0.22550194657510808,0.9943145363605878,-0.6783789467309485,-0.9985091535248259,-0.35273401239374846,0.9602699346552399,0.8277210849119085,-0.6659110893848447,0.9862274558321591,-0.7084433521104375,0.8495979613166585,-0.9990847131286049,-0.578430347432142,-0.74828579122929,0.18694955366842092,0.8987340189786879,-0.7534952585141256,-0.15801501184237166,-0.9500910274373531,-0.3621616617761194,-0.7933398452398286,0.0360586697460046,-0.6381082362448037,0.06471235247587626,-0.5082586952232325,0.38974722438228543,0.4896914112652407,-0.3417170480064685,0.22401492326828548,-0.3264994805475791,-0.513597680653887,0.07366178649256315,0.2878370785602199,-0.7686155565524354,-0.15281214408137833,0.9414414139631232,-0.9029818137046561,0.6841536143686408,-0.5083682853964748,-0.33452489316337697,0.4076528671911298,0.426136938539647,-0.9824574271172825,0.7804523976272097,0.7633984031122931,-0.5080889569699518,-0.0793281190916122,-0.9145393027477132,-0.3478285605365046,-0.8884510398447696,0.34134444215434856,0.59492165401993,0.2321568261949182,0.01347529926278638,0.33687522574719936,0.013230263067453988,0.5817856908965502,0.05181616345989559,-0.7453716598250746,0.44338954973456673,-0.5322629994734835,-0.8853489409612362,-0.8925072622460335,0.52544086379259,0.7693885176092986,-0.0850942840633463,-0.7017574298165126,-0.24889873250077305,-0.3365364436204702,-0.7943004403896127,0.017743652492831902,-0.8529184634046735,-0.4602814803952073,0.05588248143962819,0.6548166839857922,0.0331002903940247,0.5722172931263338,-0.3320369383995382,-0.8282723708224874,0.02993628492774391,-0.3336603247354907,-0.11997081009252407,-0.07752216458957237,-0.6704642553323836,0.431336835178288,0.27684740800243635,-0.9498352593196318,-0.9108445525111845,0.3013910010255818,-0.2723672733183446,-0.6512959045465235,0.08328174806721217,-0.1266892044564501,-0.7651091627240876,-0.4720262586005393,0.4392284033128215,0.39434033230332916,0.7133611635921827,0.43849259764084064,0.6880733330286039,0.5867514919976826,0.16766455553826054,-0.5865957472382252,-0.6675636028935561,-0.40094466647877747,0.32967862057134245,0.653090986401282,0.811922876166326,0.42541837135363725,-0.49391310403821964,-0.6304663074523933,-0.9883648917926349,-0.2054448022719297,0.30073593200379894,0.5876371755525363,-0.7737963051654384,0.011054863140263027,0.6982797251972563,-0.9558662496554398,-0.4537929031030465,-0.8556315368094597,-0.6413697458535681,0.4194526209279605,0.6225775297969142,0.5791465200228259,-0.37183468326049346,0.43009967871304544,-0.4034949399238652,0.6690550119288321,-0.1681592844366192,0.25344185868745156,-0.4055793787959932,0.8598723631560631,0.8795922526931563,-0.4079948305566367,0.0948849650963338,-0.23461546747215575,-0.8414661531097034,-0.8192819196864495,-0.1771257667162001,0.6157438672279475,-0.5840031006860456,0.38790416831502694,-0.12589257383032,-0.03994931158357278,-0.30904521383322536,-0.7501588866021305,-0.8298840475007043,-0.13922708186106575,-0.6449346042379536,0.7166596971251593,-0.2345794174193357,0.05463765879426721,0.5110300042891932,0.9459028603776377,0.690845468769276,0.1574736080630861,-0.5449634886904645,0.7048995657565787,0.7019459974425797,-0.15082678656027046,0.5974679756038324,0.7336402350525583,0.449451532184029,0.30969664278003645,-0.7342170252876878,0.11735556878410411,-0.28952311391033364,0.8329817398238172,-0.9060410580110846,0.047902701130982675,0.7035598684711846,-0.5239343202592782,0.7340357952700306,-0.6702963056423723,-0.572200504295133,-0.56454587051701,0.9685084569953242,-0.6046016892482466,0.24556051690051872,-0.5012538514791766,0.2838575610045737,-0.8170859555670902,-0.759678827488728,-0.07327032281827833,-0.5968881670637666,0.7700516705686917,-0.36463336714251,-0.8043064709826436,-0.05612156695518464,-0.21063192934384808,0.4690054901139034,0.5917085307034612,-0.5480937741035662,0.20109575639050742,-0.8953195841124298,-0.20720006866321555,0.7551156947375295,-0.8765318059764026,-0.18479311848957464,0.4070888919101712,0.07557328153405173,-0.3170843073025786,0.9642672185242834,-0.6127758376355641,0.08753097576264962,0.7513544745141714,0.8978802114786513,-0.6408274331232342,-0.16946020874895698,-0.23018109915953788,-0.7727715335554406,0.5062111301130375,0.15173585863425648,-0.38626855397895626,0.2999848031849631,0.005854929611049915,-0.07603855418381632,-0.08275689175005695,0.4723305957531791,0.8813864393194234,-0.3654114234825963,-0.5804014259591319,0.9008388109746265,0.8906446816915077,-0.555045983198654,0.0598751142461522,-0.08329287595612218,-0.07018006874484889,0.27018698634985244,-0.8538182253756115,-0.08063448011533483,0.5055043395192048,0.5762224694022944,-0.14483507696615078,-0.9245794860433031,-0.6619329115704667,-0.9596222916117326,0.4885518636443931,0.6059777973125955,0.9390825816317944,0.2337127744762741,0.7874488068397422,-0.37041122981107694,0.10931550373164504,-0.029998145015219458,0.534107675290442,-0.4978516406640343,0.8163178641192189,0.5063898208911273,-0.4012482230392412,0.00039595561747636824,0.041895104624384664,-0.6635936120984481,0.40340459062213263,0.15328354174928188,0.7119790097137917,-0.16664820717282244,-0.9630454340871123,0.7758912997015089,-0.10883917122270645,-0.9104929220770279,0.9514136333202673,-0.7738945570726696,-0.8201966270785942,-0.30108736722899104,-0.7667810660190981,0.766173646387396,-0.9325789816538421,-0.28023513011256607,-0.158509909394307,0.20324066573109445,0.7025461940215145,0.8665670315809931,-0.9197016811481439,0.9255588079351841,0.16085329036512186,-0.9391616336642735,-0.656884855115428,-0.48929151833213697,0.17268351958638722,0.2858032427468566,0.8303528425259001,-0.3717263804861986,-0.8569635204359715,-0.9572388827271748,-0.5060357855865046,0.1264377459104549,0.48121403372769156,-0.18322648687496618,0.43673646131866634,-0.10878214698003424,0.7684846027122262,0.10918781053656657,-0.07589304219950743,0.21995707703046896,0.8018090977413364,0.19726770618702896,-0.3072979189874754,-0.9735974076255459,0.9828753614280951,0.8493220923903216,-0.2807421773716972,0.03370241985359127,-0.5101746945205048,-0.681446764944566,-0.5067202817456753,-0.4389220147540638,-0.18813444799022472,-0.31402428736002025,0.18914669806816659,0.5697429402735191,-0.4401330938890927,-0.750742183490473,0.03101706169463913,-0.14249457005143462,-0.6806056729587573,-0.3935958560753442,-0.9179376915336301,0.008588298998042566,0.5274442890618039,-0.5648654188273109,0.30305808498008785,-0.7785231094267687,0.2292013932849344,0.7164711636926817,0.23233938667085863,0.0404559730522071,-0.2970439064212262,0.29151849530375895,0.13441373143696542,0.11449539658191528,0.7302669038979331,0.9120929071919248,-0.9864633950299391,0.5342896858850663,-0.33879227346558555,-0.5250871725329561,-0.854606237834357,0.11583870916927869,0.8339680328189903,0.36204115593020075,-0.7029568562419817,-0.43974841350831345,0.6551067423822179,-0.6328587550398617,0.8684112595910994,-0.20205934316655624,-0.24553495636943357,0.2844170529762003,-0.11206952663209309,0.16346793082337288,-0.3123848794307789,0.005645667295599459,-0.01597935321135434,0.8766117003624918,0.6530213235249956,-0.19191365748760494,0.7132364222305749,0.8854744434528034,0.8242671498491108,-0.07133836796468392,-0.6661494202149815,0.9629857657257068,0.9116792810179845,0.8677231537700505,-0.5990971754522922,-0.029386867752230067,-0.31294939406440525,-0.6598214383448706,-0.3916048988522409,-0.8258040343826398,-0.29180453491414315,-0.7855224854957499,0.05630278406662702,0.9945496536725515,-0.7601374921507797,0.22683792245441672,0.6426810410300511,0.7842503148294173,0.5588835936499964,0.7799667805164474,0.8288866999658496,-0.922843176687217,0.4983728845543815,-0.4596428256802628,0.7414098212780205,0.21175519738583737,-0.31834242507843347,0.5375000884297588,-0.4745835623739485,0.7074803584824348,0.7007334638182977,0.8993849746236848,-0.514378997496074,0.13389816420246523,0.2167901369095051,0.3130381443064154,0.860575111420258,-0.24680078219592572,0.11244672337868344,0.35181460879595194,0.20600836099432912,0.8670612483055536,0.6721843310621479,-0.2498003181192018,-0.1308393192598704,0.44810911324306907,0.21190536003138233,0.22755843901625156,-0.12970990787544912,0.052193918248170004,0.5305420206780718,0.9555698548167938,-0.8344489671053261,0.6181994274703279,-0.33624801711928,0.4210769917266015,0.17201912852950865,0.2102615899674392,-0.20329865001269748,-0.4571885043822467,0.7383635289169151,0.3720865696427871,-0.8700411060142587,-0.021753779561333553,-0.6923209667504853,0.02102599851536317,-0.6671042714819939,-0.23354167687613872,-0.8015254636513276,-0.8361141965237776,-0.16610411302223257,-0.9250389894718821,-0.38990110548691637,-0.7024998539501373,-0.33987643933264744,-0.7704190203730126,0.8761352678197672,0.6904522979678499,-0.606979443607695,0.16821008237073048,-0.9836409629698374,0.7497601702623738,-0.7605205635825583,-0.6697257715016699,-0.6074849788830737,0.16798579577747608,-0.9010553606422287,-0.7285226890401506,-0.9085679507599332,0.1791839810831401,-0.36743090268433487,0.7813478363479456,0.615268440702919,-0.21241035656528887,-0.1941547239250286,0.3328892590193776,0.34759118112337895,-0.6036403326390593,-0.5518007714221264,0.5302395310488347,0.295859402983785,0.953918004791305,0.14046216010949644,0.8622762153329259,0.06015156465847047,0.7319663548005888,0.5789542191968623,0.4862634589688051,-0.2430354618799262,0.2773666406359746,0.7418657094052381,-0.057730415626183175,0.42919010511936573,0.6823953353663668,-0.03796142491105089,-0.16487174230110083,-0.630114985130007,-0.7602270646463489,0.9179112195762889,-0.5324538461957604,-0.20642151798489106,-0.6694716029428365,-0.4603956880700163,-0.3986921991595179,0.4040145078485591,-0.7260254948331268,-0.7059547099119173,-0.22970165909538362,0.22902112811786904,-0.964581754612984,0.1216410499172058,-0.8346462380887821,0.34122780710172523,0.9402668312587528,0.25299265540973787,0.7419870276681296,0.3404276761293198,-0.5764101344000528,0.8022731618093397,-0.8931730827307552,0.3629704893745881,0.31817549223957553,0.8127852786316383,-0.7470943425331927,0.2433462269194889,-0.8312876979020014,-0.4560786051823657,-0.8102275541295393,0.8909262203868723,-0.14385248092625066,-0.1770886001213744,0.4936889157941846,0.6578542000457275,-0.569106857765797,-0.43714086264815905,0.15929809528485717,0.8329795857759752,0.10387231956076272,0.37896288417987095,-0.4411376949633209,-0.6311577222110852,0.37512923886830907,0.991055232050986,-0.124952045828875,0.21033631277878528,0.5428875602368534,0.8092556557176757,0.8734584011788644,-0.14293703107962447,0.9634639047723083,0.8912135096028175,0.35509482974291845,-0.16050127771620715,0.6261034021335886,0.8142965772697451,0.047605724556237225,-0.9311848107788655,-0.7266623254679222,0.4956631454466165,-0.14290695379871915,-0.4445253965058227,0.1562853234637413,-0.23124273644156834,0.23113368402943654,0.011938525462110485,-0.8303297326960645,0.8430284735393366,0.8670079704287096,-0.9243836414938766,-0.4930505563669166,0.8439091144029381,0.5161850910941711,-0.6360006003653427,-0.19882305531423172,-0.6955876544352152,0.9252103634413706,-0.43751164613128785,0.23356534006050825,-0.5707691011906291,-0.33150522580329567,0.7641795713386447,0.9175153897646907,-0.6343455690767641,0.20573440289641134,0.23363216610688353,0.34333195373353287,-0.8718069065156787,0.062303612618330007,-0.05471181733028425,-0.30625881731228655,-0.012525227514447868,0.39247246898563465,-0.19161369270210002,0.7869825499439628,-0.40769756122748074,-0.0987094805715371,0.4730622395930567,0.01581275846175531,-0.6179501400502327,0.6239281354800151,-0.34364098769679113,-0.00578711556489744,-0.9075822409454206,0.6406123001840234,-0.809306461767119,0.32180490636118986,-0.9187415493622031,-0.6740175795363974,0.23295391759056408,0.6363165628680859,-0.6378727634258954,-0.42510764756928465,0.5091295363494425,-0.33195344013227135,-0.3255717664372763,-0.272486041573805,-0.26731617020240783,-0.37191025946998935,0.7851239677415995,-0.2441257724118446,0.757893512256702,-0.6853817567166202,0.6986220658333965,-0.009272988242941408,-0.429936555443669,0.6932982772074802,0.2296083702555366,0.4609539128399268,0.6704196752127373,0.5493975673961702,-0.32241036557715697,0.05965134874449921,0.7788876019386864,0.525298000349284,0.43423490477613463,0.40853912430876943,0.7267565453941982,-0.7410291187545841,-0.09314509193140626,0.8072092085280439,0.11157099304193148,0.5842018035601331,-0.8066874521286659,-0.10691145468940337,-0.5646556429915257,0.16620148295613443,-0.20677171824599958,0.36116485013931315,-0.9560860927772995,-0.3524013048998806,-0.6219152987383756,-0.4974667303152858,0.33952559318393655,-0.08825756055063748,0.08918777879636197,-0.3862106915749516,0.6583098659022392,0.12285008962962603,-0.5330556104971942,-0.4446953160883531,-0.3632369540632622,0.9226665333476995,0.4618259863734783,0.04291166100307546,-0.788580905050463,0.48665019679522037,0.5929791488627276,0.9513244376319223,-0.5349244084174148,0.2541554619162269,0.76430909858961,-0.012138292225924522,0.05929764129119851,0.6154470248509074,0.040521230335455294,0.598379644787328,0.2177164544309229,-0.7535555925027011,-0.15345795753776567,-0.8002560642340499,-0.9257822091055457,-0.7540492412073305,-0.4010797058642453,0.23666912697142453,0.9547502186332119,-0.42701301091809546,-0.328903186514498,0.2693800559176691,-0.8845850097198916,0.9656181810283229,0.015797053071207684,-0.040739866641141764,0.9818798808527597,-0.28358276055974335,-0.7185855634853989,0.4412275354178963,0.040432056108398014,0.27377772073785384,0.379177118221466,0.9207282833231095,-0.1854122405413831,0.11848880918147306,-0.7710096177684183,0.8517669351333683,0.9845335066560005,-0.9703018496530219,0.12015822892986863,0.5059576504639987,0.5452679087598323,0.4126484930427185,0.7583636221423489,0.41518842109111154,-0.8686364019313615,-0.8423751073646604,-0.708410660049272,-0.06578362606769106,-0.9056998234023157,-0.7809876318764062,0.3785325780863601,-0.24418076555999174,0.8311696086124365,0.36316740514352647,-0.9951905496708321,0.05287807311924409,0.1924327325875421,0.41533703403258604,-0.7404556604276706,-0.37670294102741564,0.6423682264147341,0.5572221849537357,0.47279513128183637,-0.9882825748060959,-0.49057428734066433,0.8272749878144605,-0.4322835705286332,0.6045547187592349,-0.020033862631536725,0.9027479933711096,0.9682774991544115,0.9241530045756903,0.21355481587095856,0.9651454807048718,-0.6209881991922944,-0.036647680197839164,-0.23773064441360453,0.8799817387349521,0.7412637336077814,0.3438719420627252,-0.966856604977994,-0.16227973964861403,0.8685617450550436,-0.8967834682357975,0.8163338852430263,-0.14953214759319094,-0.1945410907015106,-0.25317627329459735,-0.43380057506981184,-0.019640137894919085,-0.6650867625227093,0.030807056204074295,0.028575616551504666,-0.15967229236319921,0.592977374129038,0.6217063361173385,0.8196550959124839,0.10293843332511474,-0.6073407885814335,-0.8484739202878754,0.01262170933876262,0.07885923586815573,0.8171368395322056,0.5650678480241488,0.08429970934670437,0.6236746383070993,0.5659496514120345,-0.6742409373084765,-0.39286629541581697,0.39791816724702594,0.7119664271131492,0.5403633617654806,0.8597415122470211,0.484932477190786,0.5496270892898838,0.56902633644967,-0.4655320400089878,0.4515769778603378,-0.08675460682231069,0.26530567437544716,0.8423459217139757,-0.38511609734403895,0.6957164124187908,0.07285946866328877,-0.36594437696972537,0.7374951771797699,0.43114141921345933,-0.7956778246464029,0.5110569652778159,0.5920606843821031,-0.7304568849548392,-0.14470629595110926,-0.6394812910979567,0.3619906170054978,-0.250982981402809,-0.6993526745448699,-0.012782359427559262,0.3806238883582241,0.9181822504798176,0.3367243158538875,0.032894938658432,0.23272285197569906,-0.45077303166687877,-0.5889351899241365,0.29113676877786915,0.06424416736395543,-0.8753594053924744,-0.4592114579381183,-0.36648814880012137,0.21338903748762666,-0.7156924318240677,-0.35608614561675117,0.35262418389577765,0.8252380945926211,-0.16645359356101413,-0.906919975515641,0.4264040294272854,0.09325652667838479,-0.48051791448697095,0.8504116736938804,0.5762972949896316],"y":[-0.04365420721894323,0.028445433400378303,-0.7000625775480207,0.5883401676686586,0.7241362750164853,0.2328163813275603,0.9881852396803017,-0.2528424594065548,-0.8937450072124868,-0.020423980336724348,0.5744574847026281,-0.9987867914079307,-0.17937826829405507,0.038523443903068486,0.2943759793361813,0.37372477989188546,-0.07000925051778863,-0.15798732354403142,0.9336283370986663,0.6851389319992348,0.6714386301383896,0.8228384087754662,-0.7066549645593863,-0.2746489849424026,-0.9685848304382377,0.42145763137069553,0.7806641359168914,-0.06715053671371642,0.13917031897238255,-0.8603313538601272,0.7619788450879453,0.9513522158960673,0.01924127861373918,-0.8106092745546052,-0.04580899808772526,0.15702815061851325,-0.16075060147920026,0.7583532726925137,0.23731047863438384,0.2584162896308247,-0.9027220106963382,0.3315339138703037,0.6111923713317249,-0.5808940119785736,0.24445163256909205,0.48591936260401963,0.3862708949929924,-0.5371681034956509,-0.21471481985685853,0.5179109541020273,-0.5921024924324971,-0.07870936912025066,-0.3375420302604509,0.592416630176366,-0.8351199714846245,0.4207811912532975,0.4013923621222832,-0.026437120339656454,0.14536685382866582,0.6773506399589896,-0.22679516370206879,0.16111772482694833,0.7493274840619266,0.5782135921874392,0.5593958760382339,0.6363032987042997,-0.6713839818403894,0.4451332500822252,-0.1885190339070365,-0.09302071464975947,-0.9849834318895916,-0.5866291448497416,0.03598093379533523,0.6200171278431319,-0.6955834492718279,0.35664661935912995,0.39441782531064223,-0.9446394962838051,-0.9206377758120372,0.7640813231136369,-0.27703141295845635,0.6160679357623766,0.8377371574977496,0.29988313375502684,0.19241036413207357,0.4197736426884336,-0.38558948092788037,0.16362625466048342,0.6929984378307406,-0.6814078872932047,0.20384048802629706,-0.16343800976332726,0.692176070608079,0.5795542541609583,0.6969982548858993,-0.8453646528049499,-0.34265127013777374,-0.6408067150862454,0.9008806865582386,-0.2960793800690394,-0.8866838465147866,0.6335225163532983,-0.8547082754476294,0.07543222918861771,-0.4050451508765255,0.09715568424597786,0.2063698308856993,0.31613273832340266,-0.8060291200508254,0.03185702409564246,0.04356779784059132,-0.5725899385139281,-0.4286327143453708,-0.44088474638088027,-0.8436346902976393,0.2238145142857371,0.34746946851838456,-0.9456243751272073,-0.26638419976140754,-0.26220157533725774,-0.323480088776169,0.7755893287935037,0.3575255786385081,-0.19285854486398876,0.0025024497050158494,-0.9682148519596903,-0.17782562017591438,0.7905090334189891,-0.4425380485129619,-0.1356037227826264,-0.529293156311371,0.8909520673339867,0.8757293798106112,0.6637553155664575,-0.9901297656165577,0.9401035599843891,-0.4002902261222515,0.6808807710539692,-0.019832064965275342,0.8406829265458081,-0.964206125258983,-0.47083592366863025,0.8492501114005161,-0.5355152461182879,0.04385460210579953,-0.7521929361196588,0.273421369137645,0.5352886921727437,0.33148607297344046,-0.6898508382821702,-0.2279183580378374,0.8060848520985626,-0.3514853783250758,0.024141890362202337,-0.13377023487335626,0.42540013318453873,-0.7282591858853414,0.7854892359803787,0.9575923361452423,-0.23206230066239636,0.48780265802540973,0.5952181109902432,-0.3821265031123018,0.22054311140606792,0.15675657230146545,0.8716183219185745,0.034255619436460764,0.06476720004451526,-0.2290925885621916,-0.5306248705151264,0.9113190258039479,-0.030625128838392267,-0.7645112436470698,0.11282447493899224,-0.2695881833939188,-0.995812850842924,-0.40228536422029954,-0.18648000174666346,-0.4415965394528932,-0.6754087606863581,-0.19224601961078544,0.5878126959922259,0.0869767948044355,0.458169709781286,0.8333405297191494,-0.9089228462325214,0.14826270587212703,0.2563949775019866,-0.8084466641105382,0.491564674082828,-0.08576178586326116,-0.48092745154015487,0.863153737514667,0.6876551194488694,0.38062923926571424,0.6266945803284487,-0.35307098521084174,0.6868203186901094,0.8278254766800013,0.009357629183157358,-0.18714650597838225,0.06765048364645163,0.5499959735329735,-0.0036729381220546487,0.0781269153987341,-0.6872902745543233,0.4202108196074366,0.22660002926408532,-0.6547153615933762,0.16043304585769705,0.169904609925245,0.24515959919017738,-0.6601899171995633,0.540012906937726,-0.021394708886279003,-0.7125763437969125,-0.8064798802333961,0.41246819237819726,0.02604406868118559,-0.7872303924415633,-0.6587627829201019,-0.3720817251983398,0.8846197625306405,0.04442028023907179,0.9950949836932101,-0.7819333131354731,0.9989826996227447,-0.8551606673370755,-0.64795204229663,-0.44398287574575035,-0.9005332013415202,-0.9811990593404513,-0.2976379771991731,-0.8657894960709749,-0.7717199381079549,0.39124220058716325,0.050896850643535496,0.49384310737598924,0.3526500706693749,-0.8151786110712445,-0.33803233219710443,0.9149716916707414,0.24017943339151512,0.3356566163864135,-0.5554587366149344,-0.8335130711553129,0.7549113072437896,-0.46695375040393483,0.7482870880778483,-0.13393814530390588,-0.39755904785618307,0.7000373202056791,-0.8273898579507818,-0.805657116314785,0.5882426721145899,0.42000596770545406,-0.25197701264145317,0.2524172753059358,0.6467011118070963,0.06730470099953978,-0.031890597812015926,-0.8508795104827411,-0.019538536836648232,0.452628668533891,-0.3489410113222597,-0.5950428991666498,-0.26521303621173314,-0.37322162826751487,0.23102707734719918,-0.012870946160898625,0.03655389480418696,-0.9292031604204207,0.45234587163920764,0.4320911697344521,0.6395434156304886,0.36051129328810555,0.8604795902155555,-0.7621112181627064,0.40804184352536216,-0.9044736210653536,-0.5621922438634315,0.6899892958516709,-0.8210528209053883,0.688534394658108,-0.9305451198726294,0.43743734877500445,-0.4559154303132937,0.9036955577550173,-0.7113791896707379,0.845082076783831,0.45259327322794274,-0.7911696467026543,0.7656633028256092,0.9259064247874051,-0.9917528998571388,0.8477563457005629,0.2709282943413917,0.43447317019813064,-0.8623275960412812,-0.2692834878791922,-0.2758108817834594,-0.23601185499012345,-0.8679353856979405,0.757622396783989,0.2728362823512691,0.8771000704739498,-0.18969569351663915,-0.4126001914990306,0.6231645004853225,-0.4649252127604563,-0.2883979814677106,-0.5757598175439612,-0.25415668546583436,0.05500829575418458,0.1256630091639195,0.43778300093570105,0.9871366211027903,-0.7731786473535517,0.4331160905226377,0.18080714779552795,0.9520966996382347,0.1932473573512029,0.26137177231594483,0.9186153136561472,-0.18200425607355974,0.3196251026078483,0.8604256508827594,0.8642903490427882,0.5565481418849939,-0.8014754603678489,-0.20132624792413045,-0.9313654098218748,0.9792038496818898,0.5563623070254513,-0.31014522350420326,-0.7983595515681573,0.9663863772507167,-0.7761167480941521,0.3494073150945367,0.7533537465143394,-0.5388658285654057,0.050539325851811645,0.772179422491592,-0.5048745605256637,0.8111503139766032,0.06937096366291128,-0.3339908704330467,-0.9970388520194609,-0.6933387263249822,-0.02512447739862245,-0.9864705965275214,0.8162374670494233,-0.5579806061546396,-0.4617685609503095,0.1718111180795612,-0.20351972812441071,-0.8898727100928728,0.7540916142434873,0.7883982620326894,0.26581016421925524,0.5850478142312011,0.5625104610081484,-0.10787136426871557,-0.016566478292062547,-0.1237838822833246,0.826485926544098,-0.566646479161483,0.1164069915886286,-0.17708729506535456,-0.3488544406296848,0.4309322957555361,0.1378724849708064,-0.14355706532762244,0.38310516755757235,0.01928067753197008,-0.1907769400605404,0.3630683514932645,0.008417675120370883,0.6963451244520636,0.13041558847981574,-0.44089799998848106,-0.17363904364185972,0.6538825150920005,-0.8098872277477049,-0.010518273993823766,0.3093839463137855,-0.7863048279192046,0.06705248471034368,0.8456369205536366,-0.26842990611201495,0.5222955314555792,-0.6740119227043608,0.10127996247909181,0.25774851072099314,0.10239586348540207,0.678866595391047,-0.2613706498761938,0.899598335942625,-0.06869726081218792,-0.8161279595221181,-0.5415515646782112,-0.7089796174127677,0.4568251687736521,-0.6612786009335946,0.05979512393521012,-0.9826582487862701,0.9892560299876966,-0.7835163727066633,-0.08538924426201883,0.964923457096729,-0.5464449536969956,0.5040423278377953,-0.3429064301847436,0.20988311246347935,0.39219200061625403,0.801773452966719,-0.35871867168265964,0.09909029972934127,-0.8604090357192903,-0.8066072722589193,-0.14143204268992848,-0.8280915456712855,0.7150230024863131,-0.9446816724294018,-0.07685433676488908,-0.6793655775977296,0.006197034379396785,0.3785222544019534,-0.5756191413034936,-0.058235275093557665,-0.8877253181135171,-0.4858089875820472,-0.85875347367368,0.5956788022727867,-0.13914065871630998,0.5130196911121733,0.6162044252896101,0.9864638270388089,-0.8954355526028097,0.003923659734660292,0.6037218372493395,0.17539258095639937,0.30463856363955594,-0.8093209587864105,0.17675244444616883,-0.7122682357226305,-0.8945259132027997,0.37380819910614393,0.10711345725899653,0.3351252408789435,-0.3064531962571579,0.226419446897146,-0.6254482083930775,0.3733027038012329,-0.09998302455427091,-0.02277456495765544,0.8987649602149742,0.5380061906613267,0.950087191607401,0.38111014948468597,0.7975606915080888,0.26133690802007353,-0.7406412891596333,-0.16129211030442975,0.4195123356454622,0.6346144298976499,-0.7811079015646846,0.8387634083429933,0.01563267523518212,-0.43792706413384197,-0.6580239350917201,0.562672914817916,0.8026385810377086,-0.9646127780882624,0.18557858159346077,-0.774154652371181,-0.2264718350675587,0.876435328463353,0.8807330689499073,0.9686078568875675,-0.012083268771961551,0.4458817847962763,-0.1908548809146279,0.4636411638928449,0.8399512905838719,0.940840403067179,-0.8462873445899048,0.5619418761394487,0.8433011488770212,0.9611498284446859,0.29993968174219166,0.38346514643002544,-0.3940460554124613,-0.758825506146128,0.888029939643566,0.9610064483785917,-0.2858982624119623,-0.06287906033211599,0.817725532761917,-0.79192725833417,0.4346782668126711,-0.6107174074128632,0.15197311652383702,-0.8151119646179856,-0.728346148978267,-0.2237415978213721,0.14254865121215965,-0.11519083888554009,0.03127897319232183,-0.40709984806309873,-0.7625218637722779,-0.13006021688919628,-0.25395117836168607,0.0990861412092879,0.9068608244558315,-0.014227210135092332,0.01839344244795299,0.8519573166440406,0.4353343501112317,0.16493560371628127,-0.576739335510161,0.19162645756067648,-0.2247636865472129,-0.5282784328034243,-0.3225232295658432,-0.1542451938687101,-0.9542667037383743,0.6889172296283403,0.5270955272142875,0.8447997323432392,0.30612176322090034,0.28883792624458043,-0.2560603109339817,-0.11075631248771713,-0.7079772299154033,0.06210432659945709,0.3937820322878907,-0.48266067971607224,-0.9110750494752453,-0.6166767745177574,-0.834958550408119,0.7748518119764767,0.1731742734833252,-0.5052822492208305,0.6745246775483292,0.04975878358306276,-0.6119895450024456,0.3032018140471937,0.42209819527813497,0.8847287078105995,0.7780229703906693,-0.3990311812223751,0.5975269790913602,0.14791672189117006,-0.6318983848818513,-0.1093790108774044,0.0865260936881914,-0.3089969762346991,0.3650483233261236,-0.1321861436522314,0.8573273234243609,0.1751021308047688,-0.25382722200455937,-0.5159285127651942,-0.02582390397161498,-0.030152863822631486,0.34277732606910843,0.34069765918817474,-0.2627967009521985,0.510266056256613,0.7232612609397754,0.8832506773935993,0.5769970576446345,-0.1918961295285884,0.9918793828941594,0.6858571143924388,-0.9833026829040137,0.1442737566936883,-0.7188044239575171,0.6514227969811082,-0.61645629405494,-0.6429464984759649,-0.29399876070248343,0.13088119021565264,-0.1216682074078119,-0.7645584562650202,0.335556855658194,-0.5320699582992263,-0.042756148991254454,0.623105352008908,0.020884755247542637,0.8325638297173493,0.9083205938145203,-0.23445889963206112,-0.6694045892759619,0.36203435093150194,-0.9677845047698895,0.6146699397134487,0.6812733682819403,0.247976446368684,-0.4265659587146377,-0.07157992739939445,0.4355981816426195,0.13149102703777787,-0.10931713161256705,0.5716854985143527,0.5253107109507433,0.4015173662197289,-0.1976449355671854,0.9041678456532052,-0.6433232046008436,-0.22555895450490537,-0.11425419200636326,0.7286814415152785,0.38607561104144567,0.17600612749086145,-0.5994725592403075,-0.420937781718719,-0.8206975755781483,-0.8168410599075712,-0.9248458156411107,0.5690497299093338,0.7456890156999991,-0.5994033209267843,-0.012112240168967444,-0.9716342599118596,-0.004401141354498816,-0.9973539582873563,0.3209206568847996,0.9313206554121941,-0.19804510590870308,-0.7262845908475687,-0.020193625831394134,-0.7593236702770724,-0.026588679730218764,0.5690136718417208,0.13836759661317366,-0.15940185271143398,0.2961375446184676,-0.6388604756792703,0.8222943011889896,-0.6123929941249538,-0.9008862523604279,-0.7174710326441315,0.2802200838333377,0.5808613007628356,0.8813701883373769,-0.7658454435158859,-0.8307290244538243,-0.4776428444554432,-0.02750257994225169,-0.4936568213238528,-0.17759128928319723,0.3931303761778587,-0.5130316148402077,0.6454165760255175,-0.34660588729578,0.22036272223566994,-0.39677258900979245,-0.22132094818986864,0.11381291788379949,-0.2124301118065053,-0.739426512519498,0.3936673639869811,0.6980963240839329,0.5461550552704288,-0.9916587221593224,-0.3023822061988665,-0.3654543420659975,0.9269349499474807,-0.26985245670804964,-0.7064386074982161,0.0891459985578146,0.0163047164028788,0.8706329020560517,-0.2587791279927558,0.7934285504828233,0.6660786354159195,0.19733938180508173,-0.1346685659621672,-0.23378929070822554,-0.5033991357733496,0.2596497389165529,0.15352077738392625,-0.8848420719236512,0.33789027913247294,0.1775602901611207,0.12942862767536376,-0.6010069607150581,0.9049360933442371,0.6384059953372699,0.5964802874927349,-0.9412804354637988,0.977373830217029,-0.07660763092536627,-0.22226086730695624,0.9944805799086239,0.8973789206783187,0.17299604704028015,-0.5896812775979753,-0.6929556722696664,0.8766640995380616,-0.19282303162182313,-0.022911950541537454,-0.008300894108634847,0.9820618364481437,-0.9036282721841262,0.14262463122124305,-0.2294975697946903,-0.9794653197570491,-0.7227354699349416,0.6970162309906872,-0.6205791084058043,-0.2883519789451075,0.1329045502756507,-0.6632856794187791,-0.5749096325432353,0.16982941285446154,-0.3480844164470045,0.3853091940740019,0.8234671386425728,0.8802423398552472,-0.5934170216128929,-0.6048208294475267,0.21779420132042868,0.6969909901701247,-0.9088106900658275,-0.6069931654446024,0.3182491818653707,0.35734833837655006,0.461675942638369,0.08018490264043326,-0.8879786418404928,-0.12637338666962833,0.6818051537749586,-0.5255739335506459,-0.5857725234143254,0.8123498308884904,-0.22193279818466005,-0.5374122690206974,0.5126399167788688,-0.8162331394393543,0.7705391808335698,0.5305864934821662,-0.504143942873053,-0.7542437367870373,-0.0294812283709624,-0.7499209162888689,0.7908596888173166,0.8346587719960642,0.8941103737541064,-0.21281612650436466,0.47975983928842175,0.20988129147660714,0.9932509745660232,0.547445700813156,-0.3830809410637499,0.477824900998313,-0.5826461392526886,0.42717278353310184,0.3834868465672183,0.9541854832446296,0.05804499260065749,0.06844475163325713,-0.880594917057088,0.17576260331239235,-0.7357468663985746,0.4047413460610875,-0.32108853929661174,0.435109955316691,-0.09627870997916538,0.8609744889301114,0.44745810731980185,0.8327132593455092,0.35089433416558635,-0.8867275157463586,0.19496538287091633,0.07961431157703155,-0.5474372964111872,0.8539417977461676,0.3784369378641097,-0.43045687560297596,-0.12818181976777177,-0.3735091391227328,0.7277786907721528,-0.0032525516710038365,0.2868135726190662,0.015496030118792654,-0.5735194642649881,-0.5076266892565876,-0.819695824318819,-0.6693342680922427,-0.8628325635442337,-0.12427917351444069,-0.5664065083133479,-0.19181270521331184,-0.6533306109132269,0.9954539534288473,-0.38609589846536746,0.26844193371786373,-0.4253853864393578,-0.18659158363485567,-0.9991871093249125,0.05228888296777112,-0.8078258000812223,0.13268593586990254,-0.8247844115943759,0.8495804045023163,0.602267619009006,-0.272072006871377,-0.5123784552659658,0.7085951534049868,-0.6466870358293602,-0.20977210487283804,0.7688938583958606,-0.295224197436415,0.5474127253637585,0.6660349925471128,0.853874972501696,0.509252223278159,0.1058156436621413,0.9344660409618794,-0.8786655113517425,-0.5549723422691732,-0.7103707240088356,-0.3361389245700709,-0.8934692206020246,0.03362677228618113,-0.4131763811600103,0.5448523408464225,0.16394568124368025,-0.4709418293605294,0.2082190976789343,0.7076207567917593,-0.06860221368606438,-0.5457812737695975,0.6110880856393567,0.23746953857635686,-0.018217352085082714,0.4028582949857852,0.6871803933728569,0.7711259983721481,0.04175272638630744,0.17153945824912176,0.23332896421526308,-0.9117112412379186,-0.023364434172728307,-0.3539300451550364,-0.7469464634276394,0.9099473678980426,0.1436251733379317,0.6686804966448926,0.2787506103307624,0.15851315191656423,-0.7969375868982654,-0.560324827018202,-0.4407489694566259,-0.25315117352882466,0.722340710931941,0.8345042230469778,-0.5055304893736332,-0.08522256475030555,-0.020711280331786663,-0.5369250312734056,-0.01312478015599544,-0.3253244441563715,0.3199188548293741,0.6411192026022098,-0.033065270503247,0.9891484924107561,0.5634135307943747,0.7685140681075667,0.10990765789607937,-0.6120058191994817,0.9778897301639101,0.8028509001594928,-0.182781131267853,-0.5340607795093403,-0.7888570086410018,-0.19510155599706921,-0.445143079086711,0.42681164705730207,-0.3907378004761799,-0.8585474985642936,0.8818634079194944,-0.9845947845352699,-0.3716302695510046,0.15410766537509332,0.5208283961402868,0.4068076922142472,0.7740281907154889,-0.300332259450009,0.5765453099893806,-0.030099922405243066,0.3765435853620908,-0.0024266569279306105,0.4734676895812018,-0.28497099202003656,0.9055842894889292,0.836666729961118,0.09305160118092878,-0.16913051894034592,0.6799100317320936,0.8943734536087848,0.3439562632061268,0.5163767856962294,-0.46856227835354036,-0.24819429558570724,-0.8235126328475543,-0.3574035645427985,0.8802011136879702,-0.5045255934847557,0.1165262197181104,0.7709123267936876,-0.5782412871087805,0.5879266119756522,0.0683800737000433,0.1199909378245129,-0.8948744859257736,-0.5156748399330826,-0.4963725715576397,0.7199052914602,-0.6067680999194419,-0.8454190030482894,-0.23279046132375705,0.3612276805831109,0.5423897261873374,-0.18426370980364887,-0.9011997621599526,0.9391708521822657,-0.1775487583981495,-0.40707049887018676,-0.5503354827469442,-0.15985417421790346,0.30582671456071253,0.8079756064399457,-0.3314861780290177,-0.9120692269359236,-0.34338985239309755,0.33847969407551676,-0.2347204386884155,0.9303540484812581,0.3698030572973308,0.04449978010769149,0.9704645084737894,0.11719364993323948,-0.778411779787224,0.43933930768905005,0.5634093505748448,-0.08756018986633247,0.34528800230137097,0.9645372818229176,0.7527278681207316,-0.3530714807042332,0.9045575662303038,-0.8833653468571834,0.310985966203975,-0.417791263331429,0.8796194335229204,0.109105471949014,-0.40122862703320017,-0.4251771232578843,-0.649332307842245,0.5109885003493818,-0.14041978962806403,-0.3675371029122054,0.4602110901035625,0.02454725788841907,0.7693355001046129,0.007985136135278017,-0.14800729536677484,0.8027774186090685,-0.4148964021977396,-0.9139347556472198,0.9048705473140896,-0.283490445308533,0.6669345708587122,-0.15824111014256342,-0.46329927036617735,-0.13193527877136746,0.16661319886679837,-0.025868484003136648,0.3733798328071517,0.59763397929618,-0.15671953246201412,0.08453176957613914,-0.016669601031582583,0.3136071582332517,0.759180244570199,0.43545507262978994,-0.9573535490169631,0.16041658725162888,0.8254312918515165,-0.4883660706975484,0.4783761165765053,-0.28246719082868155,0.951784016372323,-0.04011211548220048,0.12005822284391339,0.41728653613495786,0.848438461750818,-0.4453137855879833,-0.6614621737696401,-0.8425804937076193,0.7246662855050325,-0.9773018315401404,0.3472163526368404,0.6182798101691857,0.7974318874337548,0.09686054052156011,-0.40319352919569856,0.5442339559225654,0.777358312784791,0.4458454329866131,0.6765362526915247,0.17099046510960583,-0.8550232525844734,-0.8934945736058846,-0.45241377538030725,0.4287223709192054,-0.6805775496278051,0.930520594187129,0.09872368604944969,-0.06895664201088549,0.19220257290761908,0.7218189019106861,0.8528459110973943,0.32898606419857085,0.45271807478717924,0.9196526634463843,0.46823884047224135,-0.6394607562221523,0.25529370696036036,0.2418295813798666,-0.6232380430224188,-0.811606572780666,-0.2963696181817109,-0.6601483561110422,-0.7526945768310559,-0.9454703506997914,-0.9949640197522713,-0.1002665613810178,-0.1816541437857643,0.4906787971114474,-0.292883460571192,0.1901739386986197,0.7751912046008073,-0.8309411305861385,0.010572766186183025,-0.6971763345794213,0.6851666636931224,0.9329170302304378,0.5502450341552972,0.5439035996430088,-0.24710363884599368,0.046126372088813795,0.8871944202029481,0.7781110575208298,-0.4893786616269531,-0.06371271405274449,0.4459037911490649,0.43050537240118875,-0.4126139692739893,-0.6625797748782613,-0.40622660780262815,-0.14286019441162995,-0.3853034192057934,-0.8096385876684153,0.09691742353708596,-0.7603316313788552,-0.44745637351773526,0.1581130578406584,0.03072043965492063,0.017386867821002332,0.4930106722868264,0.8610217612579772,-0.756428620156947,-0.10693547386875722,0.7350248239285552,0.6883619604667115,-0.9161159510593262,-0.454344464837645,-0.058520728572314246,0.8498170495288392,-0.8529010780372824,0.5322622496508514,0.9907998152765314,0.6935862047821781,-0.8065094578775265,0.38806283326704744,-0.8664223835719592,0.9665503327026739,-0.0939195825346853,-0.9400028467917024,0.28926481663884374,-0.10373363333470614,-0.45426472991206995,-0.668439607126907,0.3964987594092698,-0.49241518106274407,-0.6012353627066959,-0.6007634180759398,0.581014627346537,-0.6521300417604715,-0.905874557518853,-0.060254554569368006,-0.560915676246035,0.7653316415667898,-0.6143055490230097,-0.03540212904540961,-0.9612494214619999,0.5469612144341718,0.26716388022300247,-0.545680245818439,0.8882333419378103,0.5111977573648805,-0.2513095554834082,0.740252115988498,-0.6344944612617691,0.21646119094556734,0.47608174113281504,0.20803219330588776,-0.1428429180071704,-0.9357459358623329,0.4057658954060161,0.485901387227985,-0.5771706223252215,0.1314009210936544,0.17871073825508055,-0.19807976330405477,-0.5603905920307302,-0.6691753548560624,-0.3767065939404636,-0.03046453246798331,-0.3340369955130428,-0.9422869033706353,0.528267193041869,-0.7253994249396033,-0.5454529643843129,0.28139657320670747,-0.7541297128609034,0.18109408545338027,0.3041487036500996,0.6118387733693808,0.9673916460409594,0.430000314821954,0.04435353684084745,-0.6203969902703685,0.8862477856098384,0.2248187010493849,0.6057365792375159,-0.4546898510756032,0.04085676269162965,0.10106177035269229,0.8169052686673232,-0.10247527981643925,0.05444476991829613,-0.5794022173841219,0.3768022840417493,0.39389763869419664,0.918037278454413,0.2375819237435556,-0.6563998692208812,-0.11399054110252414,0.6199203386369484,-0.9193693421095899,0.8404207425872862,-0.7218475805993143,-0.23043022434282848,0.02375911660488026,0.4776780180324273,-0.6525252342674194,-0.9348375902078973,0.7637879583005055,0.9109915603107077,0.22932143903392546,-0.909701797963029,-0.5363802392060241,0.47899257852040417,-0.20450422222416928,0.04177200946018792,0.6143188226890349,0.45842923500164784,0.22986187264118582,0.025328901625324245,0.979804557977279,-0.776651998903289,-0.7916808541537489,-0.4901168612571248,0.12685463241653394,0.8559621214085036,0.35867590718534714,0.8091832305114381,0.9963866956588598,0.7900301107329805,-0.2902433099796147,-0.7847707582631432,0.05671322997479985,0.386871753370372,-0.14488409605519015,-0.09361508104518376,0.21021827728451195,0.6536951886326678,-0.21896043565089426,-0.7403147278240957,-0.4900626558088986,0.04490688414506487,0.07261946987311499,-0.6695547110411648,-0.24440955490578553,-0.8739271468377569,-0.8171126528005691,0.7619533841317713,0.4835549359523563,-0.2253237229340048,-0.0815049455955969,0.0654685679282649,0.054583232962316715,0.5070888390594369,-0.14923354651417253,-0.2071387111673885,-0.34512938035055307,0.12403263855314506,-0.48238996986302113,-0.055814811694788725,0.042503382697269804,0.7197005520537095,0.2713823651929595,-0.6033806852346049,-0.4378117648674835,0.6483749429083938,-0.9566493077693221,0.21765123387856078,-0.905913049954636,-0.5110466978770599,0.9560997323947542,0.5571486672109356,-0.6278198798770829,0.2836481276110456,0.6741751254482351,-0.4806148413033821,-0.8985918210643227,-0.8351708567630713,0.9409295337024428,-0.3803341105281192,0.9207610141203939,0.07275642602309917,-0.11432361357108706,-0.9867118397528755,0.3197123318089268,-0.3413031050647875,-0.7290233946734803,-0.5369782841603075,-0.16502737229342526,0.24401363131888665,-0.4554881185407015,-0.1606239658476501,0.5665443080697314,0.6414524742412382,-0.8048795260861634,0.9563913824176937,-0.3949561077032989,-0.6243653726563564,0.425987800946274,-0.927527952645627,0.3791973313013606,0.9722950270927544,-0.9155481873092505,0.38419803758716575,0.8320366898177561,0.7346604793204548,-0.9948350577692708,-0.6649092463888316,-0.8893103739654875,-0.2761340632408612,0.17766238057279318,0.3200315593974702,0.1608880238815238,-0.5028938348819711,0.9436339443510279,-0.48674094306222454,0.9615472744938753,-0.02187954264819893,0.48718730342037186,0.8980877044052307,-0.45459368762776076,0.014065552487602889,0.45993020886417574,-0.6916467505489245,-0.053330169402832965,0.5974302338369943,0.9009785057856078,-0.4588185241903641,-0.28253277573120267,0.9236345159151803,-0.8418310762941498,0.4881899670555967,-0.5247092450326937,0.783345042053235,-0.5420700471061123,0.04027968458306759,0.2905841689449061,0.26445350302010756,0.8117554070171671,-0.7540145181555026,0.24367123657930026,-0.5142711442387006,0.010971054563832499,0.8738623033545455,0.2741886737607939,0.8969251584726146,-0.6221753173698943,0.8923440479568859,-0.1977515913771871,-0.6860078158198902,0.32514530967758265,-0.23933368945451292,0.11280443164719633,-0.8770580552456597,0.9223104966637059,-0.6102293780051771,0.21294666859247577,0.8799443737577433,-0.8538855012718425,-0.7482381165790541,-0.07943089219889268,-0.9040451310022565,-0.20314806778774278,-0.27767051372262325,0.3445321304336027,0.926476619623476,0.7149625242514507,-0.15714176191131948,0.22147702355147986,-0.3507303711925982,0.7114967134576434,0.3836699830485369,0.7801804055726218,-0.265503486012713,0.10395676810693616,-0.04830628898397861,-0.0825814960919193,0.519506033904357,0.18266415833682798,-0.4617139950445517,-0.1687080008082792,0.2676962742222856,0.41698954770405405,0.8790617186470893,0.9194519663249652,0.45811822748920283,0.12307800020762498,-0.4867975041103407,-0.9840891913258875,0.7879463260461383,-0.42058933384759634,-0.5130716465078623,-0.4882818269413736,0.4456490295384689,-0.3227028534684049,0.6298019566935771,0.44043625192340374,0.6087089562493101,0.7397166602902141,0.5312600619945987,-0.3571524926683945,-0.9255741564730702,0.7189331968944325,0.056768593552412976,0.45077165308152833,0.44673029704401224,-0.7560218126401202,0.2243275652925585,-0.2590973161213116,-0.3058074162833427,0.777030471972696,0.0737675446198315,-0.8819665592599616,-0.15088531095754548,-0.354940534077031,0.26848184548790666,-0.7991550581035926,0.14234707897201881,0.4170279060199291,-0.9954997722060186,-0.39924496517163505,-0.24882152670944394,-0.4839745164011333,0.384021129097294,0.8001033379971245,0.44244176324405976,-0.2444588445280924,-0.3932426643103064,0.7996508880046885,0.8650320042141221,-0.6765127204222267,-0.45308387242523607,-0.6110475518613923,-0.04733541282214249,0.21191215787770418,-0.42096865104124587,0.9056473713699046,0.5324871563797463,-0.03182117030809406,0.8637245167677897,0.3311917517855991,-0.6291504220373739,-0.03706382378596491,0.7172762833350447,0.07739128897646284,0.4703252563000986,-0.3155167182061065,0.05059171607578366,-0.9653327143657086,-0.9117689646975076,-0.9159631749263825,-0.7505979665047855,0.24289390971933547,0.4825649290487391,0.7824505135364657,0.36157023035403346,0.15530647216530585,-0.7323240668382957,0.9564426668902404,0.6906342118431181,-0.16152919804490923,0.4937845013388482,-0.3725900476058394,0.9219925638663108,0.772807829892034,0.473114180304371,0.5481939649542137,-0.4956842723543942,-0.47007611277170436,-0.23256907556472473,-0.5238471565862995,-0.7731244832107458,-0.25902537829358996,-0.3329426187980013,-0.26721814193216653,0.12363268130516222,-0.5403141493290584,0.3987477483147517,0.9572398430831058,-0.34988591139024416,0.9935937141840273,-0.3421308533958258,0.4003490962955446,-0.5637611944807863,-0.006518749889377223,0.6022023970097669,0.2379795509106969,0.8614424014600558,-0.6311792275122668,-0.1515877703063361,-0.9199361208502358,0.5973401463783269,0.6926264008346298,-0.4047186142484144,0.04374369469395243,0.479167836002736,-0.15944381865230145,0.5604011543235811,0.3392408986829583,0.8908742277843069,-0.9454216546415433,0.7171556416946687,0.22699401715153913,0.7351130655193489,0.0028640666383574973,0.48543987145916173,-0.7587843807168299,-0.23523658056837782,-0.48214744894060263,-0.9024711244758772,0.2069039638253734,-0.026201438313510562,-0.13644465802654607,-0.1951725795959028,0.814453597847106,0.11407182958808394,-0.028362822021011605,0.056776545241180934,-0.15231264117291807,0.40173016169677106,-0.9747987069910504,-0.03393292464215735,0.03225708201812836,-0.254347993041535,-0.3395414083836567,0.7927626112743388,0.3346988244118093,0.4584520849721247,0.7587775725299084,0.13371424652991185,-0.4335055261091909,-0.03980904536747366,0.9280363720183993,-0.4654267604004826,0.21209652905427148,0.8284340187091425,0.4966983369897089,0.12043598225769102,0.45625558015180034,0.2059826819835922,-0.06759756408431647,-0.6253410265687794,0.08164100995366871,-0.9649463579856159,0.6460426763794822,-0.44001575582246966,0.28792593275388867,-0.8775173086128767,-0.01405982320580271,-0.1837419811185254,0.4640562829040918,0.8601507394106459,0.5726968794821028,-0.8532858580531714,0.5466243729175797,-0.42787421247749985,-0.36983829041530625,0.9245260478875394,-0.8091761091395743,0.11989973557312236,0.37584144478899506,0.8974168518328416,-0.6603190760735318,-0.01947206010546315,-0.9886115424043153,0.7042717782768438,-0.8437311209465227,0.2730197113301178,0.5552625457314634,-0.27872700138080986,-0.6800406245770698,-0.8814852267300485,-0.46428905817215926,-0.41830625392997806,-0.6818701191202317,-0.5843482237487791,-0.9383434948666831,0.839025630471713,0.929003368634818,0.8119625938010656,0.33992792003440175,0.6751199341289925,0.40436137492348656,-0.1385617584828314,0.12110763053799922,0.050176450355473745,-0.8509579624525806,0.4793090104774466,-0.2697930877185098,-0.4625972596372433,-0.8981843988118742,0.07372584920926835,0.7858808810373018,-0.6262544204214625,-0.42546722378516144,0.3068601691061735,0.9786500898569972,0.6579656968881177,0.9069257766823311,-0.9707181633396494,0.636247061062344,-0.20651990108018528,-0.692965631451971,-0.8922621009011893,0.11589304295679412,0.7394769446842554,-0.022078434045737234,0.38952113659049326,0.10456970356481171,0.17130794648214218,0.9744735054106655,-0.17309954900186425,-0.15101178552030814,0.12877914608136326,-0.22025512802023806,0.2864111836180132,-0.4617225779525947,0.8893256079830723,-0.22506126474965898,-0.7187023605265328,-0.0010304386041944205,0.5127070166206524,-0.4305959465696583,0.997994954023628,-0.008980162570321026,-0.6485747073623424,-0.3817915608593908,-0.45271998806120345,-0.6117329245522863,-0.2800479294783557,0.47755680343513573,0.16220286029543776,-0.3251405199133036,-0.17595721722903807,0.8636470867121424,0.03529492165718693,0.8167303343622045,0.8101290320867768,0.36769443645372535,0.6259546088295384,0.6070201206414998,0.7104120764262847,0.3253162708855728,0.2801185356826051,0.9889913938484475,-0.7710358833167166,0.8987509486541571,0.2529622880616677,-0.5950181380268302,-0.9850775983973861,-0.5964149437051992,-0.9770641814751618,-0.12759126958936817,0.09849261764066185,0.008395915031779963,0.8039921803472511,-0.6935251348453036,0.3877245960550062,-0.5680340605786562,0.9473526613803939,0.024086705039267322,-0.48082724834995666,-0.2933711602609902,0.295868388754162,0.6993267007391196,-0.35550456516841217,0.6253539819969097,-0.882798220736061,0.46045338490417675,-0.8976824594792984,-0.7426556999499443,-0.6640226231526447,-0.7990720061977536,-0.08144346649155532,-0.4544737774314921,-0.717410211196611,-0.5326023975183294,-0.6630865135861846,0.8392871145130024,-0.5293850855379624,-0.039859030290837365,0.5685563447379807,0.35302509547566857,0.7762508490226424,-0.7105676866466638,-0.34102126251155424,-0.4946491515870762,-0.29415615157362707,0.2747971328785297,0.6080284887727965,-0.6942036893300504,-0.13885159520604623,0.6613742460904295,0.22562393832502364,-0.739383420065494,-0.788247780005714,0.8600080471941912,0.057013765144848905,0.6539703365087222,0.37064098189535094,-0.2737652463727759,-0.7648001980594561,-0.5612353040871788,-0.7789103384469211,0.3666146015347849,-0.9718354008592238,-0.7892555503258193,-0.13967198032510197,-0.4418248955445224,0.29053801007581215,0.7421350074863612,0.3817076638434208,-0.20270763055111252,-0.8660793853644109,-0.4610993823429274,-0.740910326076324,0.661782346706839,-0.7772356582338013,-0.6842040061643746,0.8566423203674577,-0.9605293679882602,-0.41271317736568686,0.18229100265556808,0.6399926810452349,0.3032423081071136,-0.8309559616791715,0.8177142252678884,0.593278808107831,0.01093663080437269,0.33423695881714177,-0.2424628731293447,0.7157048314921085,0.5698577895894198,0.8344776862021596,0.33762987942089173,-0.8686710719480054,0.6700462930749751,-0.20520574572584693,-0.8777518390230039,-0.6581796128777333,0.09808961854995807,0.2782063491475147,-0.5506851202266844,0.7641797216575846,0.30136965738516824,-0.9673982868481369,-0.27589498444958355,-0.2854509484940998,-0.1125759228925384,-0.40483357462824937,-0.4398277379828061,-0.9194214714086906,-0.24557249895708727,0.4556208114514776,0.4755664648161087,0.9535025680627842,-0.5522855128188067,0.7849142662296936,-0.20259043882525965,0.44806364457649106,-0.8455424386442466,0.884969198521004,0.7081109456225889,-0.4362337079418321,-0.4906996545632027,-0.8473521297195391,-0.726696798298725,0.3781456225380762,0.7031412900138191,0.15469146494048588,-0.7810042050805195,-0.07854929090285609,0.891914555415484,-0.12540208692568508,0.9593823024559706,-0.15489306544253081,0.0607361210945473,0.2149565798670424,-0.4193933869056267,0.9612371784367476,0.21231091029138235,0.4250863389516057,0.24720998257549742,-0.8568105929850265,-0.6551549028012925,-0.4340359118875521,-0.9760179405207715,0.3635768127986714,0.36067921955418614,-0.1028849150355524,0.8584292366979521,0.6360877338216145,-0.6938126866427241,0.41780650976618966,0.3190738600432811,0.7829426695775252,0.4096611625064085,-0.49129495261454786,0.42001701483064685,0.338908205842941,-0.260732294846986,0.3562863103581944,-0.007764942373037556,0.7190420490741225,0.8544840859748839,0.392223193794015,-0.24335047468646298,-0.6209441404160652,0.9466677817059191,-0.35945284163373514,0.4756561499756842,-0.6357468840339721,0.3778799537444914,-0.7322108573065436,0.7599001724268775,-0.4929065087379752,0.938511826083634,-0.14136733581703698,-0.5969534572358661,0.9779713319621247,-0.7863448277863141,-0.18756068217766983,-0.9123440053387865,-0.19548739327046175,0.3214679844243154,0.6142671562706424,-0.2764813864367634,-0.8225081071160129,0.15491741675641843,-0.7245763909716654,0.7147414753529441,-0.886125843737232,-0.21966957446259056,0.4180687028121685,-0.5853563239285016,0.3002261745624979,0.5302569237177404,0.027264661962989423,-0.3151942631499696,-0.7735790268681678,0.17400595965159907,-0.6608993812085514,0.8996607183550728,-0.13739822618763023,-0.5501329438920907,-0.08622977312200632,-0.952349042061107,-0.7359009652816086,0.20123870611389244,0.3966267546129156,-0.9548982512573417,0.47258430827328934,0.4981850990828809,0.5520802435345529,0.27534899212883024,-0.8401579977892801,0.6032418719014867,-0.10578666612207172,0.45449840342437897,-0.41953742878625133,0.37997942972759546,-0.9431472227333115,0.9694560803678474,-0.6250318164158618,0.6546429843610112,-0.3523224213325437,0.7702875915881868,-0.6252861807991332,0.6028247816193628,-0.9772198958329968,-0.2857423318315704,0.9317049002994789,0.37500788215491004,0.5239750333901413,0.5844675995586808,0.26362306719166756,0.38174161262894746,-0.1819693851992526,-0.27286442839786224,0.1740623205648764,-0.5988899592500355,0.49581020372509477,0.6152065227017055,0.9376753844076262,-0.276699577996706,-0.6567965602575367,0.617157195810487,-0.7037731808798481,0.43115808716724774,0.7123281783145842,0.7436949046125795,0.8731576452904015,-0.06751072329625578,0.08071079386410829,-0.0695239970851745,-0.04611393432407539,-0.6515702228919784,-0.36023658398842373,0.3081271093049066,0.7802697587846499,-0.15136509390439726,-0.4809344637469354,-0.3209073208201272,0.8245473554754461,-0.7068154189890621,0.619620048367726,0.3297310887139771,0.301460146554095,-0.6434358902042042,0.49704279112223015,0.5905169628549902,-0.153271746872441,-0.17339656855276245,-0.8409499196641607,-0.965079006740877,-0.1652922209783791,-0.7475188880989558,-0.8644960017290181,-0.7797927059312822,0.24913146226958463,0.2460499738658628,0.31299143140705926,-0.1580461457670803,0.18773004156550924,-0.6334123621940939,-0.4510454207777647,0.8672350320996415,-0.1462648270353956,0.42510566577917785,-0.4302238548148868,-0.3133164904431455,0.49214072355667715,0.1835934812868482,0.5956335748070815,-0.3707330808254979,-0.13702382547289046,0.19954335266155993,0.7845439661388848,0.22056578946622651,-0.8365551062193701,0.6903075133107339,0.4103507693800378,0.6530048299547732,-0.19317670367510234,0.36440652187115913,-0.6488991861011881,0.995261018925881,-0.25746067296207337,-0.5559040367074277,0.19219734832079063,0.916261932885972,-0.4090423211440736,-0.7012128189590309,-0.09739967281498328,0.8892125782319326,0.8862064531278538,0.3014430394016787,-0.0006634504054911566,0.09078446150236891,0.08087466574057528,-0.5560320645711375,0.15456648095987377,-0.13889768228229096,0.3838801978672099,-0.4503453975429132,-0.5248342651342891,0.6641730180268349,-0.8532064684556139,-0.4259136085040873,0.20301389413924087,0.3571860193646647,0.5688995922167285,-0.09380749432207558,-0.6063017531983026,0.1922534607969066,-0.58981810269881,-0.4729216518465945,0.628570661142837,0.8076403003331767,-0.2489664496019749,-0.9322060765589095,0.1232841179234448,-0.0057780689407897975,-0.5056579311773198,0.7227339660811887,0.9807379095674353,-0.9134422968402797,0.3249128632366355,0.5355837211473804,-0.6664729122655194,0.052993747984830135,0.9209181963545945,-0.41035978455780403,-0.7249405938512087,-0.7831802903635583,-0.14372363541199445,-0.9154084819619382,-0.7916715111952558,0.4738394260154819,0.773248494365448,-0.9270791534290307,-0.3240524369831311,-0.14131000774576483,0.9960942609836814,0.06167654786673995,0.6837842566931976,0.09006732875400866,-0.35729564642496703,-0.5422085252902806,0.6753060489271877,-0.19389072647244956,0.31516398566212506,0.8674380411387003,-0.2232166426404975,0.78974020836209,0.7367535661135352,0.10766556819171118,0.10079032286024157,-0.8361933289915451,0.062406718469440944,0.799497858429573,-0.12231148752306645,-0.03862894134641558,-0.9182104762435611,0.22628817417597147,-0.869140611354222,-0.5995292389562604,-0.8265895600665472,-0.5670681497318837,-0.6429482230522948,0.9563517245276694,-0.5595566418534866,-0.3727576598185302,0.6924986301170114,-0.7076739449805548,-0.9837076202275798,-0.24550749641070985,0.2836980138760584,0.7449634520469047,0.07547089400832174,0.9599358890081401,-0.25704390282465395,0.21399023343598003,0.9153006501539227,-0.7804124973996662,-0.17118880846553405,-0.8152043235836545,0.1636605496651701,0.9476014013818309,0.18713207570666626,-0.7953305440114908,0.9354102091371468,0.19718821614004142,-0.9332631958618607,-0.21560600301998809,0.22988595250986962,0.8248434335145735,-0.5160871003598706,0.4240445885399966,0.6164319218299298],"z":[0.9346202192680949,0.04994960946202759,0.4726768774504602,0.8033570730468255,0.4661466188016612,0.9275872271325571,0.09098786217031038,0.8799273439986983,0.43442152513156,0.7060908501774382,0.7368468945135214,0.013630647139695112,0.8235574840707355,0.43237048181184906,0.4172304474775218,0.848242418986849,0.9547775544966279,0.9190171699678467,0.10157613477047435,0.7163469574494247,0.056844834137318515,0.29230857010211003,0.07089552745384557,0.5052273805574594,0.24684475089457597,0.21452737794743035,0.455691961623033,0.21535862401148365,0.1627450699737994,0.10568399836545424,0.5990603450552897,0.29624351221496087,0.9933398810149432,0.24666139785794336,0.9836608995921783,0.886234600665311,0.6789370108986329,0.11351799257880156,0.47655736932178433,0.6903759391465804,0.18315777694307842,0.6642470278510872,0.7910367203353494,0.20846774431123757,0.7577358027886072,0.8115150244127464,0.22554167316465965,0.8162048109035923,0.444787525517168,0.2974232604930851,0.677718334783137,0.8000427822612569,0.9373156113296677,0.6762784479326804,0.017788698899465938,0.10296382286698841,0.7062146262821583,0.7444881314407565,0.03770239925301037,0.12047654562448368,0.8740960613188656,0.40988326191994234,0.44852745858714416,0.7818642635055831,0.5962793523623149,0.23590273626534253,0.4596931996156608,0.3706422837864106,0.9703042039214936,0.5206176473497467,0.1705398491805054,0.7943539634311318,0.06899223361591361,0.734900698604532,0.6118877015899195,0.15814116205306605,0.16967453436479596,0.2986652042217763,0.33355096998623346,0.5388891964003059,0.4508847259098284,0.5135128813977317,0.5235605618947892,0.7150056586315127,0.32638253561551644,0.8910921368110236,0.8169108035882986,0.9839103846904549,0.7203289290578403,0.04536646655837627,0.47443536281427784,0.960961983531204,0.6995339232523222,0.7862219221894442,0.5719025722697608,0.5297431058599413,0.5885381923335098,0.5505904600000678,0.16328803997398614,0.9380695254226157,0.3278623837968352,0.27684890868385603,0.37344384547948567,0.979265093476249,0.879432141182319,0.05466224387798052,0.8888855693001637,0.869331764336331,0.5588569100611164,0.04726726292737592,0.9139916106101496,0.8025672754314088,0.6835850299932059,0.00787815823791106,0.5325713652720191,0.16434249893259872,0.4131052876155083,0.1650342760641867,0.8278716610751914,0.8479578423510717,0.43861384848987517,0.5238635806938797,0.7810744160890614,0.1823558507647524,0.9726258625520356,0.24488786406644705,0.17085942782370728,0.3544198274200179,0.8573720531054456,0.8504590210957537,0.25607795802840544,0.3229236443346939,0.33772402181547595,0.1384720419261373,0.003713787016145989,0.296486100646513,0.19288939341197825,0.6657176829784519,0.7015655787791681,0.4674897824753158,0.1388025950034986,0.8598947900790527,0.5277865753854644,0.8265026358059855,0.7203486969665877,0.19993251927291633,0.4792435706139613,0.1954510299093049,0.4519051033194799,0.6298957549906217,0.6977308871526327,0.46807820716269916,0.5923325688270968,0.15554144068402595,0.8179187960973315,0.43801453459259976,0.42529031791674243,0.3890783008146594,0.2829309592052101,0.9459796826838158,0.7207795310968896,0.7977308066946442,0.03298723791910067,0.336723422084868,0.9809368445589082,0.4867378936906416,0.11097267665547272,0.9567562049609413,0.47890739290350215,0.22605653812644824,0.11908832133298465,0.7654017142343327,0.6317573914443296,0.8671861389299501,0.9313688535768923,0.055990479586312904,0.2982017871568473,0.9002432267446425,0.4320543031638415,0.716833383113782,0.7636991376644539,0.1850509009217021,0.5730626207935854,0.5248815507784972,0.5171314369736559,0.1327229074784192,0.5403103264451642,0.9517892843452753,0.15251691249870833,0.571598602116153,0.5793894536730834,0.8429126130206978,0.4001661723572978,0.2896589570574256,0.837611774281643,0.219129026551366,0.9236456398735229,0.07032897366164408,0.48590705577385096,0.8421640531013878,0.1663878427157401,0.7952859461850905,0.8098088630666703,0.4193688825298263,0.8009217049510182,0.1474133060152359,0.8328548780423061,0.9694951418842784,0.6449061824519332,0.4646760807874488,0.5452253522064293,0.6512623568852058,0.05340036797435343,0.39977504026473926,0.3527306575392638,0.5098657321168104,0.4582419228689548,0.5654403377609536,0.971906050634983,0.5439744643158413,0.6662522366410257,0.7468099685508885,0.09177310350809137,0.7412460650763605,0.06685034370854367,0.5947388707862503,0.022119934141299678,0.25517673594354195,0.7348236966420744,0.8952258944964901,0.39195898225649434,0.02792152788268976,0.7168304894469799,0.14317973643903245,0.6287711619513665,0.6646185584051145,0.21398765940617878,0.6702351780659375,0.11172636029736702,0.4592725742108656,0.8057881392905838,0.33816434437422127,0.0636347944385767,0.29798717084932275,0.21260463984393727,0.3464888559397502,0.11646716566640519,0.08104513809205798,0.472907470211393,0.09173857594213351,0.3717231160341813,0.2143242526031688,0.5572789509545694,0.4912980671981739,0.8016046321819946,0.8983223443081612,0.5906635193222308,0.9657573179170523,0.5657902207854193,0.4170971134174033,0.6135328564298433,0.300909777285507,0.8874851739961145,0.6402881882556059,0.9278929384895911,0.22233427139124168,0.3183760602895478,0.031995036610984885,0.049848224295256614,0.6423931430658694,0.9667839255736773,0.19484537592101148,0.8463022393113806,0.8630453532083953,0.6558455079232999,0.36887223912001954,0.07387766076287057,0.03169466254280734,0.820754906896732,0.31982301222384524,0.8255074330690476,0.15550126960665775,0.2645323795982767,0.09335611672271166,0.23400709634002378,0.3330163394960996,0.3403761489665602,0.4275447001516656,0.3122220830826669,0.43916222528228077,0.24218256711939096,0.054799423547000826,0.03538863668220984,0.10390089417003875,0.06887492768289148,0.49048270556635565,0.8112679400214725,0.8980720149949541,0.3519312565530559,0.9339863212504118,0.2704747386359034,0.9471394021128868,0.020671799559357596,0.5854073636786006,0.6578615866006977,0.4722154740295404,0.9808044958990682,0.34790233805817256,0.2830742930977107,0.88447930741811,0.8153650496474407,0.5573080134795946,0.7822771670855092,0.43613670033604374,0.5613730434693505,0.5897029728153313,0.07054909666854377,0.6341136612275369,0.759655557941372,0.3521682649151306,0.08783153703349468,0.07597387727152541,0.14571172273934382,0.2232059586539772,0.29814848938648925,0.29972696839658186,0.4614611346005937,0.17316357888074635,0.4493978190610305,0.5525674254158166,0.41246434331133414,0.02361996000891793,0.20229626136037496,0.5457746004538455,0.5580576830003704,0.28791941760480744,0.025826157953491434,0.47666170321285295,0.46123844425373056,0.6343986314093926,0.04197074362869196,0.9624496864081096,0.5677861245352045,0.6331406517930805,0.5800554632635185,0.976519056910755,0.8637034031472737,0.024124499412264567,0.5612146488718116,0.9155643218051887,0.000375235557535062,0.09432997029157404,0.8200237986628126,0.6629262771726973,0.2419799765088399,0.9663810004741226,0.3105689842973354,0.23252056674024565,0.3244903965329572,0.6325309398885247,0.7891899493585266,0.6840204621969038,0.027024338324132438,0.7401925916472941,0.9637292975044544,0.174150174914813,0.736169875352299,0.9396094600825328,0.4235494220449482,0.46779317455864877,0.639307115995725,0.9855378317254082,0.5198856602165826,0.4884751806348223,0.9984446939807229,0.260669260244592,0.04299653203218412,0.9107348586696296,0.6927451156058538,0.9410282016886421,0.7487395779938145,0.8939540964503787,0.6039393361028217,0.4281007389606209,0.9975382590802151,0.155575782591845,0.26925337513760783,0.8340407392975798,0.3093800038753125,0.3113874585597365,0.7863442221883568,0.20469026082436986,0.9714320438810174,0.6470131444787443,0.12168570470099588,0.026558640278681676,0.8493815708874611,0.41322857872910423,0.439597834715384,0.1874649089731008,0.6318595414268866,0.3563595197437634,0.2988050032568553,0.6416577491959302,0.9055814113272381,0.16905612156464322,0.11196029781538676,0.1404369249672461,0.12879275335383525,0.24890137688321226,0.8273326362615835,0.663430380446236,0.9174668320152869,0.7528925645171528,0.7607680232185855,0.24515399950535735,0.90983229977387,0.870339228520544,0.43281800540949095,0.4002870597362607,0.9235313193059115,0.48298238742316085,0.6933027923825109,0.12435707876426223,0.902417871724092,0.6937043301136476,0.3137845448503222,0.4711846387771263,0.46521912378995306,0.5437572889590893,0.4161449839659482,0.8704362401488126,0.5046133036147749,0.30692842170758006,0.9902069538182373,0.6951484864935656,0.503096804827541,0.15707441484795706,0.20499587841624214,0.12428516155094804,0.4599900478804239,0.7219620948766807,0.3774178499260318,0.28212251191872006,0.6756129750741906,0.17344946889916263,0.05120065400004185,0.42169841651673473,0.811570041492294,0.7112578089686827,0.8504137741576874,0.8395634630901562,0.5824924673298988,0.9277094819701244,0.9148961649863444,0.7722898634858931,0.41610963752610264,0.010773414102738455,0.2928124940993888,0.6603843873310005,0.007019834008456664,0.1285148917951024,0.12635668006983425,0.9869067027034092,0.8535611985847276,0.7477198111594268,0.07315077782340872,0.48604101833268387,0.6385747800922043,0.8988559945836162,0.2862415669139225,0.779605223906302,0.5504507429464077,0.2634770084671074,0.5404463317325826,0.44404028318203803,0.8457225427229574,0.4814716171666061,0.11474374413849497,0.23704577502566632,0.9454656027328315,0.8653424883871473,0.31263638906420943,0.8617226700226606,0.5292732906746542,0.22998138700938112,0.17181493315004337,0.699238571845819,0.4702539999963204,0.27269784551115883,0.8602121805129992,0.7135549698503143,0.2605563825983071,0.6424144434154969,0.21127874219591264,0.27288092515478957,0.7209683045505175,0.13944715812352626,0.4582731795240766,0.6078884250547688,0.843883645263824,0.7893359049193773,0.7739315802850634,0.4311738064101914,0.3455335258419605,0.9530297543945724,0.2534828578637177,0.9827721006155629,0.23774912164636416,0.8801494949888127,0.52250826777117,0.44933468889950484,0.4095395525473987,0.9923406337358097,0.08791266989478731,0.999855359459251,0.27949067381038495,0.4833600756218752,0.5413681761746627,0.8988490501669901,0.8167018519882162,0.1581159495115337,0.19855312381567886,0.8138557927341274,0.761048077831785,0.9879932467584235,0.02330228159372472,0.009568920709584875,0.6589754070749417,0.14832284279626634,0.04370908829295707,0.9566473333960118,0.810862400904176,0.5828443746661124,0.3571559271431484,0.38513968425661227,0.37051754028517075,0.5669447852289009,0.3291138537283099,0.5126514498117084,0.5012871355178965,0.4951448730382164,0.8440780460855105,0.7935850456460957,0.09639997064048025,0.5256056199814604,0.7139448417159276,0.8661653091075535,0.9041641302997774,0.07285112499805639,0.02580551237613343,0.7645972093827831,0.19048375562407363,0.9860416534038086,0.7520677541671507,0.9127974146626091,0.9952424779268105,0.08831451618107689,0.3714084208667062,0.9594065550134978,0.4549008654348192,0.9231959091757128,0.9094763523955399,0.8552966769390227,0.8054864063392354,0.8334669767580787,0.7931238747357576,0.7917275872183177,0.46920719862729054,0.7717126266665345,0.44013769409207076,0.3667819114324596,0.04463505417474392,0.719083477127208,0.11770009031387607,0.7262078124636677,0.09318760871980494,0.8997108499014247,0.6757612252778338,0.5692197850297966,0.7659316314744437,0.6212196043588104,0.8011207191351899,0.6797334081428346,0.8848785928042909,0.560101071105651,0.6437991081418738,0.13503652916829648,0.1465111784170246,0.6899942195953522,0.9903374431723397,0.511712610026041,0.18413428127964648,0.8459603791040566,0.7388991663353738,0.8189113179568285,0.24753872186400996,0.7718594698552712,0.0654354987560755,0.95763192842824,0.5849023206269699,0.949511372179952,0.7466724748530961,0.6190614121685117,0.879564652817452,0.6370053590666183,0.446516034723368,0.33345061788559965,0.037634820893866744,0.30190741072198923,0.23437544599362198,0.8506837599935034,0.18550706206288464,0.6366720559550105,0.7356605011247268,0.9458180580595348,0.7341074981737721,0.653887784248441,0.5511636545265756,0.25246112238202106,0.3700923162382418,0.5376848214245574,0.03558154351592028,0.17339163359653448,0.8771377435674257,0.21515682287682503,0.4336665648550356,0.051478438060383755,0.6669195028157747,0.33762786506481157,0.7934452677966377,0.3181768242095094,0.04070726913231495,0.4866300082749424,0.1137307442684492,0.32086253094685085,0.9898324418159049,0.9820514354574489,0.7514017247916961,0.25614526036012697,0.13693539630573082,0.7819667569149724,0.34515788921276297,0.49019547620459775,0.9338887621622124,0.8033855418841028,0.45152142438875437,0.5478062757529754,0.4639087401836013,0.48849425748414066,0.13583194641139243,0.023291561365578767,0.6662128003631151,0.5259348778235629,0.7561348184111351,0.3157413880766247,0.14385164290967375,0.8434119270694918,0.8280970699792991,0.11370855851187187,0.9758110626367068,0.21095361657774042,0.5620436835483568,0.17875129584436317,0.09438748453421868,0.7616869699458462,0.11020383291648066,0.9007112734167138,0.8190219977829782,0.3461409282504398,0.8241811629103296,0.0843361238614389,0.029022567729915474,0.8562408743458471,0.20157704905466764,0.2473029080258638,0.6021636860685811,0.5542193087851012,0.790738512191287,0.12545114589522177,0.6477324199075766,0.5347875329617392,0.6571610689445181,0.45161765955758204,0.00188667858755511,0.585217879576291,0.5497610304753208,0.14328021603642319,0.38609214194658853,0.17870445568930293,0.7411221534941194,0.24054549456612528,0.3246619825367223,0.15246220379385378,0.1259135296701994,0.8879516697610933,0.0976738011236078,0.2801601948244157,0.3014638274580033,0.13974617710294027,0.3239619933445644,0.27511695183453794,0.8209555556475564,0.9979171079265089,0.6756132756344396,0.1383293560026593,0.17370282198788134,0.9894167808515562,0.6822957182478554,0.024245889306271747,0.6830383803227046,0.004768330810783014,0.6960920526723554,0.658209822093483,0.0952771627910967,0.5420247792165586,0.21283306424991102,0.6746050308923378,0.8718360435545753,0.12834442530719872,0.5464050263784608,0.22954861888062753,0.37719111574701375,0.2723853809557267,0.7967135861300341,0.5749739143010845,0.09946831709030324,0.4325914105803062,0.46406310794951827,0.8231132485353135,0.6258527484077547,0.15747030020467906,0.36463537403232626,0.4874462055092041,0.7058469704962086,0.3555828188175584,0.786857996390518,0.1865324616208747,0.8981816684424717,0.7428138130866234,0.6654986175050954,0.5458465181605444,0.2058989584711726,0.5398580536786733,0.5890394921145801,0.5462886248094649,0.3547307707995852,0.12170563870150837,0.08332877675698908,0.42279243799008936,0.2723462643609827,0.5764109883765828,0.7306759770605685,0.7582348523622138,0.1120477834058174,0.7140313709366235,0.8032066679120488,0.7913651527297754,0.14020902832069126,0.7771587854112687,0.9235019302651617,0.29363394514890767,0.4082195154555459,0.9851808802695916,0.19297399011409405,0.401592746407955,0.6622955888986535,0.7288768023042397,0.8940629394666196,0.8593283041704934,0.34843856571425014,0.18788733631131294,0.8780829530917188,0.14055625729509347,0.5024309072144847,0.45639886347313047,0.9046181538867104,0.9365708589142832,0.6089677701326707,0.4043816097888404,0.9250192022128683,0.2600142325494364,0.7159807215962307,0.23314170537056883,0.16885613958065762,0.9771679894505855,0.8997198473076627,0.8508014610003929,0.3360540154388692,0.6227151366910106,0.4212591321991153,0.5001507161732288,0.4478764215815967,0.726888640181689,0.8026391354683945,0.6918068483037216,0.24848036626771697,0.08673281941795018,0.1354837544420849,0.8074559361264894,0.1863830146927272,0.8468977055856555,0.008796474836741439,0.6964638964535811,0.4797031182001643,0.7026279669754824,0.2745667521417649,0.23848813638466546,0.6721994841540068,0.6487366650522729,0.18650745833907742,0.6212901723831574,0.7624926349538695,0.7344645960619285,0.05307978076112724,0.6687471109885034,0.6621039699257462,0.6998259823760261,0.31138200730168314,0.09320202552978028,0.9367313123591772,0.3273641674549485,0.4747812870294262,0.8314842383778226,0.6712245763957335,0.48246047392574665,0.010220502115814234,0.6431938319472277,0.6135314857628386,0.1770980726998197,0.6145122178633597,0.5438833917899443,0.9313823409855263,0.43800344709437933,0.9227803464267301,0.79747273847436,0.7004988576822321,0.9081912198963273,0.5547523521673725,0.7825436533668679,0.013333458598763939,0.46821646970055614,0.9991017533491585,0.15105901419592022,0.912307302105765,0.386806141948937,0.11159917308675237,0.005815951084687302,0.4230285268012233,0.12347785968632134,0.9579847632981905,0.32566988711834577,0.2708114988739426,0.49065949038789236,0.5744970355475965,0.8170546185760584,0.8523427137913339,0.967426370472529,0.6824741308683059,0.5489975935829192,0.4089853660944055,0.10407383514695577,0.9386758911793793,0.8067920795771141,0.9932177288080998,0.057284339359014586,0.9385458947916007,0.6989145490628468,0.8473226025099996,0.14684014053027786,0.43039259237220406,0.04839087834011032,0.5475992734013929,0.7657419360432312,0.2037177942655602,0.11254262390050006,0.37702775332066446,0.05322153838304602,0.46687811392627787,0.07709321987543569,0.61619550027305,0.8973234732845953,0.9130500653226973,0.07397990529260148,0.12009327167194556,0.0876140727932303,0.5437812903314426,0.5784359491593534,0.6812903721181913,0.8042606801658276,0.49128248829421267,0.9509807213073107,0.8008771841916187,0.5643713361998257,0.4866241101608958,0.9103474053839076,0.6443564236100292,0.44726352989135687,0.030687046363215623,0.4074089321476157,0.8698257307443067,0.8474558726569849,0.7320190158205728,0.444567482563759,0.9286662857949599,0.19810815334492698,0.7475857567701342,0.6732824582014261,0.5502861597554584,0.040289544309750513,0.03396705641183372,0.43580702103909263,0.14594501642998006,0.36674887032025516,0.558184102244458,0.15503621400365358,0.9812664895448301,0.5764812144008259,0.36903884371811524,0.0029119779219211675,0.639632951765198,0.571086826631676,0.7831242729018779,0.5331636114963588,0.4443961555158814,0.9144447695711804,0.23649588260431384,0.6560558003897169,0.40819313546692304,0.3357993472082328,0.3380758405728884,0.3720363709968631,0.8142394753579745,0.9805361213672517,0.3627518572377786,0.16037898566657846,0.482137669335013,0.4084952864046422,0.897055545668679,0.8841745016592882,0.9573991733393326,0.3361325475844496,0.9291033838351439,0.7842760974747525,0.018967975615163037,0.16841821314730765,0.32502356534810123,0.7459025761990267,0.2006796051749614,0.557926952016848,0.9327774675292111,0.013315624227738371,0.32491706146971033,0.9342188598127017,0.37373021289906716,0.22949285640523506,0.3079715918782311,0.8767683428211246,0.36118380008783507,0.5503312764564022,0.6188813771456003,0.8590602934532477,0.5327710969598419,0.5679226294885266,0.9887479036328458,0.9244452600119318,0.6109040931468848,0.6404587708810531,0.3814584234557586,0.9600280563396634,0.8891884554292041,0.40305498782520355,0.4770374283880066,0.4033978905055063,0.36259559063512287,0.11870164943047125,0.7450588253991299,0.931747155360736,0.5661895381327834,0.875792186914268,0.1793556000107659,0.9268074736582732,0.0021966430428288736,0.7387700327576552,0.9687074531587457,0.03490799064752778,0.15430355904083806,0.7960084891179144,0.31603911824420305,0.048035390166739106,0.28329272577988285,0.4133451774768451,0.08302349761944582,0.8716245699272658,0.729901141130779,0.12796325965932262,0.22465965935578752,0.9870747861646344,0.9542114254555287,0.8358417450001242,0.21452630384161145,0.4608155332704861,0.1487188835512796,0.4284904581452483,0.48409197973559714,0.19792878525914118,0.2074170633435191,0.3526546367035481,0.019666727125508168,0.011961210318828042,0.23481880172162634,0.8357791511792984,0.08433430197183929,0.760320311847257,0.6816544855120592,0.9834015166668426,0.11184359140323863,0.4481769511016175,0.750615009047616,0.36471607086804303,0.10731649105256785,0.31662998084334787,0.2808858987209352,0.7528287057047369,0.8909649369729825,0.521169277979326,0.41626176646204116,0.9373816510379013,0.8827108382481922,0.23434559805718708,0.6615281063278577,0.230229015049792,0.019542100522723992,0.44638051873993556,0.4311445471126048,0.38421550354477885,0.7336487944813213,0.39963387284722757,0.6139499611341035,0.24365845773499836,0.011343522154717938,0.9801420724699825,0.0386168038530329,0.6905773460140517,0.9553884757284952,0.9741035670288772,0.40019225873504793,0.4750744287978551,0.7791015234406755,0.31669754119295174,0.725008740685477,0.2416314646834531,0.7617775352047803,0.3640797640306048,0.9369509803531563,0.986016457912794,0.3890748886476791,0.5413340626737267,0.8627992095544421,0.9173545529532369,0.25289030200744245,0.06560683985371532,0.7287077887359844,0.1734482346579357,0.7626591207116836,0.3818189862432234,0.3963497362213005,0.5493442173026933,0.9842496656266898,0.23246617581161144,0.7095790555707658,0.6763102586241274,0.8422358329016594,0.6780589192133201,0.5101412962576116,0.3461020517717381,0.4159772774571288,0.17439874821899568,0.5984191644329442,0.52421404440881,0.1268914075274888,0.8724845685264219,0.8031124193776542,0.36054055204146124,0.5220697907546715,0.5444401323034965,0.10575369319586254,0.4474059882607781,0.27995000416153415,0.9025987508106694,0.49909991523106817,0.05850870657493328,0.9062728032510706,0.012649503518274648,0.7017436199452065,0.2954784030888601,0.7234460382734547,0.3504350420613718,0.4569263032816939,0.5206142018481905,0.5137798709276635,0.3186676218978857,0.40196625753663934,0.758037980742,0.1406853095686104,0.8394530419193642,0.0147334429791474,0.541481796537657,0.727655077973122,0.9302312286624576,0.27391860245189603,0.24867254779452558,0.8775959300587851,0.8346754824560169,0.07836943019871521,0.8445396178910552,0.7090865278016885,0.5233535715838412,0.769472188144823,0.9427637772645899,0.8661476246266594,0.9332344912600804,0.9895382285945458,0.0901521222849927,0.229939762049471,0.8729525677180204,0.8040999422828274,0.8473405830361046,0.8136217822847707,0.09450138041475825,0.8001268511695544,0.059159627803093,0.02230530565209657,0.3347829893664334,0.35753059803751813,0.32516727100631326,0.5662904413881304,0.2088787974989328,0.781356145485475,0.9577467043263577,0.4446506005657439,0.49155870384684003,0.23482665879623849,0.05544229180117354,0.056966513428383936,0.5197883466595136,0.5888516319515785,0.15597842533188028,0.29735014808557614,0.9494804012295168,0.7937699205608102,0.867605267662978,0.6823390198833066,0.6275719095645016,0.5761083997715561,0.9467037289823897,0.7278532371260239,0.7831404852681272,0.5131606767455507,0.2927854846411969,0.1721955689624628,0.882689090212164,0.2943006744984326,0.35439207151688984,0.020788769394078997,0.21356592440899333,0.030765515419466476,0.4856601938813277,0.9729079660798164,0.6378954041239407,0.637468386953218,0.7333017755762533,0.3068444655808629,0.3650842823549642,0.18243860089639274,0.7841165231480762,0.01879035071504376,0.40446709291754773,0.2939932714813514,0.9269145515286904,0.7706430830711144,0.07201996545798198,0.6443656723788721,0.280446708190384,0.9683900377166689,0.11559918600696477,0.4653969323390606,0.6102221034135832,0.3730983510227006,0.8314882693381791,0.37485428281453526,0.13934622771519187,0.5531136301013706,0.08270406913999179,0.007199084231564665,0.586714568514514,0.2720081990428275,0.9980576632767665,0.805080245417152,0.9332972161779324,0.8355318610252638,0.25682984776177814,0.46533100511344533,0.7487528184564854,0.10464751151398101,0.7231117252691344,0.15674450636590423,0.9973034644508808,0.14274585552147884,0.7482996347153376,0.4770186723286265,0.20223783835148812,0.634252943968349,0.26505600689175385,0.9478280919945329,0.06852405873478568,0.7317894992725923,0.00037550834479097267,0.7864093245899108,0.23581984902044098,0.5215087342779278,0.6613986179703831,0.10941348151106219,0.5151775750419267,0.5244692659461113,0.00481647713819121,0.3839914693624292,0.6053263471109039,0.7752300388071899,0.02445448167585997,0.10887988246351261,0.24464945938409974,0.22350610724133416,0.21945495360426243,0.3308219499123326,0.2908145011013515,0.5314162600651432,0.7756639155382716,0.8131524078748863,0.6273635320236274,0.7274693780854158,0.2752493382699566,0.5023016565438957,0.08972013045389857,0.7691315796375365,0.3831097702835412,0.9549116810879753,0.6294126131605869,0.05521045111886029,0.10710783801577663,0.2610288003109698,0.021417795523354027,0.6732131970953188,0.9278249092624957,0.8799298197023565,0.781625155352813,0.09474885486095769,0.2644269313661691,0.07590521336110316,0.3066179386345476,0.28110953943002304,0.08733576997763595,0.6994163094343685,0.1708483047726999,0.15223622719756325,0.7087154644228649,0.027305463588860428,0.40198250338108477,0.8595969696270133,0.5545628070257479,0.34899769339160863,0.08728283357859488,0.04813296995043161,0.1119498367285223,0.8002787505048364,0.4296432604708535,0.3178216289488704,0.8354800670443281,0.3938770110182501,0.31986550593689816,0.5202112686632411,0.11608729419238097,0.9414162244877617,0.36295363585843027,0.43945834614028734,0.25666062018238106,0.8876616016469423,0.8861948720469474,0.30469637810280115,0.9980281878821342,0.5618224676395966,0.2792654716405067,0.32163712719751775,0.9587904095694464,0.18860028819579647,0.526238960654926,0.8692875646548744,0.5245549451666744,0.4475701952867215,0.793420176648357,0.3101459771834816,0.2931263241663878,0.9160937775365531,0.5165938710471926,0.08529801364422826,0.966275570892064,0.8482187428238127,0.6438071179145217,0.11642974691832388,0.8555109472733627,0.20005239918025267,0.32251158852679773,0.10698757820470035,0.6981757633165863,0.43025104680002435,0.9306820748305118,0.7737084815446443,0.7359578767107039,0.26459883248216187,0.20163173084146505,0.44843312734544405,0.543539289741935,0.21146420124198703,0.1641017866358102,0.20652350003225822,0.12971343043887593,0.3748264055940571,0.9318201338004632,0.7599879181392637,0.5315419919547264,0.3761899298120659,0.035128539600133124,0.2482462072883503,0.8631453696406454,0.38063483805372345,0.28708409193597223,0.8227130987800062,0.06095698608454978,0.770777015264822,0.9224597328281215,0.9014880857867903,0.9112475678755234,0.5314874144792852,0.9686879065606541,0.8500514167066893,0.8983557079234983,0.434693252590737,0.2289916725729282,0.24655772761827396,0.3815821340417064,0.8573699738531626,0.5261050456283888,0.30299393734335517,0.013961600222015442,0.0002779236664379826,0.694294599318416,0.7656943526979538,0.8635577093241493,0.8943159368899894,0.8946250187481487,0.20152205883424665,0.3425031031910697,0.7810820867606219,0.1920380658763415,0.4518424781336632,0.9041098350886121,0.3746027323854511,0.4711511361753661,0.31944499607206855,0.5652764412737414,0.8807009165809708,0.3625545680399915,0.6728995063500193,0.6634307781924766,0.9400707976540963,0.1981304210709389,0.6755222830320168,0.14187427026974758,0.9387873094519436,0.5787422371374072,0.956109391816003,0.5268088454260305,0.5346762854552467,0.07195364340175785,0.08176511947108463,0.5878834657157056,0.8146046132329656,0.4764033152031824,0.635004437030976,0.18005896648113506,0.6968021571577392,0.04723602502884747,0.6926882446846477,0.5479584745698008,0.02154548821890093,0.6795258818966743,0.356490597019287,0.22250610874659302,0.99618814412091,0.7738331560245221,0.47938066241232646,0.216437857818584,0.2637205510119708,0.997916721396489,0.457836378496878,0.8187465259726012,0.5040344840811445,0.8355952894525387,0.6671395881873555,0.43864955339690836,0.8578241572911114,0.5746733751388753,0.47867783672972936,0.18434818668549818,0.05436900861505848,0.39408138917862673,0.5797070249197647,0.10578884582363567,0.6258089661088424,0.6165302280898801,0.5520266498542117,0.4119357055884079,0.23030775515841037,0.237698680271146,0.6855953960037523,0.6137853656656709,0.7070552722292835,0.9155069009186131,0.026950257554114412,0.5592648531023954,0.8809816639375492,0.8328874564221828,0.8645509811505415,0.74560864836514,0.4111807019727429,0.7694534736583308,0.25575910190005624,0.3484182688116166,0.30967929146548023,0.7877299163782978,0.9905200304927237,0.8373308287945536,0.9143713639243543,0.10339668863854168,0.3854533525954334,0.07918087995544862,0.7920933290224683,0.7125224676115572,0.8131396657879911,0.3809334323576511,0.44631501612150104,0.1499692994922285,0.1386869334015369,0.48415255025786347,0.3084559170599875,0.3147949691491633,0.15201686133908124,0.618921796107321,0.9078837832700505,0.9985923094387259,0.6965107150190116,0.8524795672517325,0.13991315503780932,0.7927704345891255,0.21293889681682432,0.3258492567952172,0.695652632995822,0.712823424250032,0.5448610072006759,0.9881780977896075,0.5073795629749795,0.6296623211104988,0.13117638143531127,0.40684951582565776,0.4167732048666277,0.3580410431793869,0.3067989292071132,0.6184416462604336,0.5377593858806915,0.4959956998291612,0.6316917084219504,0.6420073939671145,0.356472813673019,0.9477706110603573,0.9019354111059699,0.09198213218749392,0.7108286726709285,0.4980171286578221,0.299091818893498,0.16748888036656048,0.5879231762264568,0.07715000188208782,0.598601681341353,0.42994232812535627,0.9758590586443289,0.8546282614494883,0.5558141747167735,0.02383254626675311,0.22134013496222943,0.19674547963894984,0.2400517868112408,0.8586642291610505,0.8682904052914261,0.8707806268445584,0.8756896699617369,0.9917645959744081,0.13557291722219858,0.9906627920357036,0.25123449674755954,0.730922529794128,0.40433687119431105,0.9371147791724936,0.36814584347888596,0.22783855959042176,0.013960955276441887,0.2516023642078181,0.42582218746316847,0.8190741304225607,0.10781941159815629,0.48665398537261384,0.7484371813080526,0.8188816177610272,0.33144685283897385,0.4966112879765934,0.9746012415441534,0.7308462152788409,0.03042784432138869,0.019102138659183008,0.9993291653699896,0.0483995426715746,0.20193360359070858,0.36496588556554305,0.2878378565913969,0.8316307753012359,0.8025669944854137,0.46740967848117504,0.362132009334824,0.4223001758821382,0.8789121681028846,0.14738457262486898,0.7775317117668294,0.3433289965700221,0.4558518499135022,0.22796865565465957,0.5680226184484533,0.9334558443454276,0.10451470524759486,0.06766392776192044,0.08769611936768829,0.8365808229826385,0.9395222824764428,0.012409970986991878,0.19977750306212516,0.9559252498377202,0.30083400009330125,0.24938120848146558,0.7074009877154273,0.4347557632015424,0.42265891329526756,0.6468905917185251,0.389485199759286,0.03763008719085161,0.711894463329498,0.3107934612656511,0.21247745424622388,0.7539681113162909,0.9272352547012264,0.7209485141583679,0.4512349760006892,0.46703397026532834,0.16351452321461152,0.981163437363938,0.5827238541135753,0.4527694635113058,0.53966401876695,0.2128666824770497,0.7254526146362663,0.22327976989338427,0.39020099961056715,0.4455829967493666,0.7476972704668743,0.8865374628364687,0.33341061731783606,0.7169261444596577,0.574554192542806,0.5639562349984605,0.807453793421946,0.44445647208512945,0.028914844798533297,0.10387657608921433,0.03926629593653048,0.8959797771116903,0.6180660902447194,0.10360633527283403,0.780526928391168,0.4044642391900959,0.535384974323806,0.20650451218400168,0.848919033622146,0.20698353174659223,0.6701236791948981,0.5367599997323634,0.4922896015206132,0.7588771022629283,0.6188305664796401,0.361936618067231,0.06547743392120048,0.29202053235733005,0.8105231970165734,0.06298654234766951,0.5987534577576721,0.3070079029967278,0.4420639754861468,0.7648808987852705,0.13029911564180338,0.7214677372986789,0.05390863083683341,0.4815861912590396,0.7338033996547187,0.9682609718863633,0.5800669327552529,0.5641109021870732,0.8970873736732408,0.7909200104024493,0.2927426767221122,0.99834644564813,0.6980904823438273,0.028628917068793364,0.4649266800613516,0.35884764665341873,0.8720973426904652,0.6569829253786464,0.4371232330649943,0.8624226018111136,0.39094227303556295,0.48932727574965906,0.11786965303605691,0.47226635874531714,0.486205137376524,0.8904944461937692,0.0775517109321741,0.8461043632697797,0.339541111605939,0.49097395508755803,0.278043813684981,0.5471053902820477,0.805700382583553,0.1402717003576785,0.4953904998399821,0.03984366817506048,0.8764636355842647,0.4022148056470241,0.38191510964320075,0.6691503269017022,0.5117394961094586,0.6998475590508115,0.11476362871793236,0.0210664115703712,0.6088532752829484,0.0691342832373811,0.09811950747682506,0.48183703798923744,0.4299403070682153,0.2039055921753414,0.19269027321467422,0.9449580365294802,0.5292213041826561,0.27296610060693605,0.12142252894541998,0.9058010677655981,0.13356607658529046,0.516004280514627,0.9271850455638933,0.6636332650556914,0.7817311387933409,0.40997764619102184,0.8756520273654805,0.22124659692598575,0.4797675268545993,0.20944471364562162,0.6689048347729393,0.16207831757248506,0.24640767772945152,0.5435188372267455,0.4550798822451604,0.021238641538182255,0.5284913456598718,0.9815483632824497,0.6373423113807525,0.6651170640151207,0.5550415479660652,0.5515077101774158,0.5009744118862386,0.6495653933868744,0.2138332255208208,0.810989923997336,0.667200682772182,0.4765184912439491,0.30279168026421494,0.8526726188001943,0.2866755068166612,0.1546769083890607,0.6778779753571286,0.4204626700440394,0.7171812323727774,0.24486826131710315,0.9527878474608347,0.010561988693713097,0.5473508350846686,0.1583496625893797,0.011630602392797089,0.6110132636267223,0.8958943873744325,0.8093689013952824,0.4387112383950959,0.09377315813263076,0.15138223725341837,0.9156737977544596,0.36302860357275873,0.46442069387270546,0.1778073860719071,0.06278117271331216,0.4194066052937284,0.5499894768319239,0.07409075675139527,0.5141638339896174,0.4306612884578134,0.5048268446818623,0.6139446257908904,0.6597963575645857,0.3014999742338043,0.6682334024732667,0.4039194199785094,0.7034222540599417,0.9123911897254599,0.4420746155673202,0.7716669868485843,0.25252025658169025,0.04564913601645763,0.25292802133701936,0.9652806709990837,0.8376062443263335,0.5467165192865665,0.24734512008505322,0.23577975302621473,0.16328313315263523,0.15824059131003365,0.9015513775736077,0.4900152524157921,0.42280797434790024,0.3854009748803435,0.21242098477096666,0.026584758225125817,0.584698525162613,0.8623994089191466,0.4926224194432707,0.6307056102041775,0.7029927834077949,0.8786151132549509,0.9191132095362687,0.6219792985016532,0.3777965946574692,0.21893437906689034,0.2681098403002806,0.17508886700998127,0.8300119392455136,0.40109533988690654,0.8564418587400114,0.28014597623062704,0.47992326416957665,0.6019292161436743,0.2911465779475347,0.6503936761550227,0.2219624810692417,0.7382521843973605,0.8147732888696188,0.10888288290535125,0.12398487852560311,0.24793741031779098,0.6166239400237872,0.8381283819343823,0.036314209982841454,0.4690074563539003,0.7998530050883423,0.20144153222373365,0.5365327097504786,0.9821731564122277,0.11659235399759399,0.9618050071825293,0.5266087742089628,0.6756171691841045,0.9559364421337343,0.31574124089139044,0.9878008658911717,0.3051666398189257,0.3160036473946601,0.31095314861849005,0.9755571676477309,0.03889775892566315,0.4969646414885529,0.5048636400549713,0.7843909720867932,0.39391700100856564,0.6680964593179967,0.5893283986614826,0.7515471760478323,0.3953863244948736,0.0994689892727555,0.8496521892263297,0.7662640913085856,0.9415771084336315,0.1370498421477762,0.6220866775988004,0.9061929938277892,0.4756759115226604,0.16903235486140147,0.44973491709345526,0.5310964645069857,0.4551204388175795,0.961299612099518,0.33058901531046203,0.3942546674497719,0.9675170166872382,0.7622025265782394,0.6119851345703333,0.7441625803028945,0.08077668239563547,0.23790045151443526,0.05166557861952966,0.543603323992737,0.8290409874671063,0.48964559648208256,0.2843274799035865,0.2957670161180451,0.19069679346343715,0.5164926073691527,0.3456557424223925,0.7197758964002453,0.27332310360077805,0.8043429404938599,0.78209135226154,0.9092031721516551,0.9613181572111301,0.891686578759014,0.23580010974850554,0.7191273440875074,0.6061300215937476,0.6115944625768734,0.07407594109497652,0.956894950630473,0.7487748784162295,0.6855350445984028,0.2670794457273647,0.8938739057318279,0.45655260649402657,0.4991632645761965,0.32504559937609506,0.3796429513110378,0.8832907369965957,0.996656913475875,0.6131995280909369,0.581917374375152,0.7201425779457749,0.0062848131005610085,0.3240910069801741,0.9552453137940339,0.4295737927220252,0.9470328153268428,0.562677037309813,0.3487648535639496,0.7838552325224842,0.7302186041451468,0.9282894621343127,0.1347004981617596,0.854048663169588,0.10430699035609443,0.34559379756344866,0.633516670537387,0.3632331237107822,0.11229530304792809,0.2472458325308414,0.5087979957133905,0.3800912481170157,0.5651174402282788,0.3942624444279188,0.0839103022766655,0.949624566331361,0.9865909380661434,0.025871433481403708,0.7199788867272519,0.5293324251789048,0.23069820347844686,0.9884188632412643,0.8627461576559049,0.8192265546022562,0.23257730995363607,0.8505408921824598,0.9758349428708383,0.22531048326949787,0.3702027430951089,0.1091707173394688,0.13673613629362574,0.60831962422357,0.8338810397497296,0.05346458580198034,0.5943035909838711,0.5064551931615942,0.6334060838416439,0.4562384928926538,0.3970038605733465,0.27764038432074417,0.07161018606317877,0.3367815787226647,0.2846559692479154,0.9054133125958278,0.31755286815678296,0.3766184557773529,0.6135226306900563,0.010443829824400976,0.45442810223105157,0.4214353638026131,0.8582699121822298,0.6721048837601029,0.9218747614350988,0.7621170186619237,0.6167104505066726,0.8675102113519312,0.06328496022943557,0.7822869437013704,0.3358647308791141,0.7332666014431359,0.43975879087412006,0.5211884174975115,0.06036272488433513,0.14568473982438856,0.13549675901714212,0.7941961940188086,0.24432428263268413,0.49676135168969804,0.9806607743494837,0.7717504431067522,0.04446628731633621,0.23542939745985184,0.4790294695942475,0.056603996175308764,0.32350752220964296,0.480001584679442,0.44243194393973195,0.2791219168060768,0.6747560678561723,0.01751904009824027,0.3186297929946505,0.8403860377067625,0.8442537193435358,0.3368582128038128,0.99811952588984,0.3887069715887558,0.8978360687058378,0.35048390713082406,0.010481622851807667,0.5545349766864639,0.38914134976472337,0.06628261376184746,0.23572909227467603,0.6339774904639246,0.3664770991420921,0.47673619913387355,0.8128528826548194,0.0262637057445198,0.7792470397606593,0.46058667193605146,0.7329986594452302,0.8473463842915817,0.7400480033601247,0.19252414639750456,0.8187880210710793,0.40189077404285994,0.11131818068519816,0.8050368893206289,0.22917118432051967,0.4903815887082826,0.8857123452298041,0.9911180298091821,0.47999334963541596,0.5353118246981926,0.46096503764713354,0.7078266549876614,0.9965938504308355,0.1515725380166611,0.6363132293648193,0.24230492015931743,0.08637992245015767,0.2357339123461478,0.572623663123219,0.2303265544742275,0.2538729345658235,0.5272191602107601,0.8544088718210924,0.6763528593361752,0.10051579081216855,0.17932074390224975,0.8915444603704357,0.27650821294601224,0.5758873068813816,0.9966052714933042,0.15608703713777314,0.8548286997653514,0.7793353078508866,0.2783237354114593,0.6219558029813116,0.45215075278836425,0.35294014748801866,0.9159157500951463,0.23773241844412904,0.6728788369918219,0.4905629242637403,0.025766753265320914,0.5292437006225472,0.3182970443474633,0.3619533249915715,0.8748325854280947,0.5576168311840786,0.7090533398124804,0.3114260300217663,0.5365567197759181],"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":"x"},"range":[-1.0000000000000002,1.0000000000000002]},"yaxis":{"title":{"text":"y"},"range":[-1.0000000000000002,1.0000000000000002]},"zaxis":{"title":{"text":"z"},"range":[-1.0000000000000002,1.0000000000000002]}},"coloraxis":{"colorbar":{"title":{"text":"colors"}},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"legend":{"tracegroupgap":0},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('14acb749-ce1c-4720-a034-44189e80e629');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>This needs to be in a dataframe with columns ‘samples’, d1…dn. Samples contains the time indices (in this case 0); x and y contain the numpy arrays.</p>
<div id="cell-75" class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X_cap_of_sphere</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dictionary with column names as keys and corresponding data as values</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> heatgeo.embedding <span class="im">import</span> HeatGeo</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>emb_op <span class="op">=</span> HeatGeo(knn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> emb_op.fit_transform(X)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>D_cap <span class="op">=</span> emb_op.dist</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {<span class="st">'samples'</span>: [<span class="dv">0</span>] <span class="op">*</span> <span class="bu">len</span>(X)}</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]):</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>    data[<span class="ss">f'd</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> X[:, i]</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'dists'</span>] <span class="op">=</span> D_cap[:,<span class="dv">0</span>]</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the dataframe</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the dataframe</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/piriac/mambaforge/envs/zetteldev-diffcurv/lib/python3.11/site-packages/graphtools/base.py:165: RuntimeWarning:

Cannot perform PCA to 40 dimensions on data with min(n_samples, n_features) = 3
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Using s_gd2 for MDS. None
      samples        d1        d2        d3      dists
0           0 -0.352958 -0.043654  0.934620  10.345128
1           0 -0.998347  0.028445  0.049950  38.323773
2           0  0.535247 -0.700063  0.472677  38.387006
3           0  0.092050  0.588340  0.803357  28.981962
4           0  0.508266  0.724136  0.466147  38.387720
...       ...       ...       ...       ...        ...
2020        0  0.426404  0.229886  0.874833  34.029746
2021        0  0.093257  0.824843  0.557617  36.292121
2022        0 -0.480518 -0.516087  0.709053  23.746966
2023        0  0.850412  0.424045  0.311426  38.388207
2024        0  0.576297  0.616432  0.536557  38.386428

[2025 rows x 5 columns]</code></pre>
</div>
</div>
<p>Checking that this works with our sampling function:</p>
<div id="cell-77" class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>X, rdists <span class="op">=</span> sample(df, <span class="dv">0</span>, size<span class="op">=</span>(<span class="dv">100</span>, ), replace<span class="op">=</span><span class="va">False</span>, to_torch<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>plot_3d(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>sampled shape (100, 4)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-52-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="training-machinery" class="level2">
<h2 class="anchored" data-anchor-id="training-machinery">Training Machinery</h2>
<section id="revised-plotting-functions" class="level3">
<h3 class="anchored" data-anchor-id="revised-plotting-functions">Revised Plotting Functions</h3>
<p>::: {#cell-80 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, numpy <span class="im">as</span> np</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_points_flat(</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    model, df, n_points<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    samples_key<span class="op">=</span><span class="st">'samples'</span>, sample_time<span class="op">=</span><span class="va">None</span>, autoencoder<span class="op">=</span><span class="va">None</span>, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="co">            See `MIOFlow.ode` for more.</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="co">        n_points (int): Number of points to generate.</span></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="co">            initial timepoint.</span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_time (list | None): Defaults to `None`. If `None` uses the group numbers in order as the </span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="co">            timepoints as specified in the column `df[samples_key]`.</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.</span></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.</span></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a><span class="co">        generated (float[float[]]): a list with shape `(len(sample_time), n_points, len(df.columns) - 1)`</span></span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated points.</span></span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>    to_torch <span class="op">=</span> <span class="va">True</span> <span class="co">#if use_cuda else False</span></span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> <span class="bu">sorted</span>(df[samples_key].unique())</span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sample_time <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a>        sample_time <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>)</span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>    data_t0, rdists <span class="op">=</span> sample(</span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>        df, np.<span class="bu">min</span>(groups), size<span class="op">=</span>(n_points, ), </span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>        replace<span class="op">=</span>sample_with_replacement, to_torch<span class="op">=</span>to_torch, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>        data_t0 <span class="op">=</span> torch.Tensor(data_t0).<span class="bu">float</span>()</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a>        data_t0 <span class="op">=</span> autoencoder.encoder(data_t0)</span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("initial shape", data_t0.shape)</span></span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a>    time <span class="op">=</span>  torch.Tensor(sample_time).cuda() <span class="cf">if</span> use_cuda <span class="cf">else</span> torch.Tensor(sample_time)</span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> model(data_t0, time, return_whole_sequence<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("point shape", generated.shape)</span></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> autoencoder <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> recon:</span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print("reconstructing points")</span></span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">=</span> autoencoder.decoder(generated)</span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> to_np(generated)</span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_trajectories_flat(</span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a>    model, df, n_trajectories<span class="op">=</span><span class="dv">30</span>, n_bins<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>, samples_key<span class="op">=</span><span class="st">'samples'</span>,autoencoder<span class="op">=</span><span class="va">None</span>, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb61-55"><a href="#cb61-55" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb61-56"><a href="#cb61-56" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb61-57"><a href="#cb61-57" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.</span></span>
<span id="cb61-58"><a href="#cb61-58" aria-hidden="true" tabindex="-1"></a><span class="co">            See `MIOFlow.ode` for more.</span></span>
<span id="cb61-59"><a href="#cb61-59" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.</span></span>
<span id="cb61-60"><a href="#cb61-60" aria-hidden="true" tabindex="-1"></a><span class="co">        n_trajectories (int): Number of trajectories to generate.</span></span>
<span id="cb61-61"><a href="#cb61-61" aria-hidden="true" tabindex="-1"></a><span class="co">        n_bins (int): Number of bins to use for the trajectories. More makes it smoother. Defaults to `100`.</span></span>
<span id="cb61-62"><a href="#cb61-62" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling</span></span>
<span id="cb61-63"><a href="#cb61-63" aria-hidden="true" tabindex="-1"></a><span class="co">            initial timepoint.</span></span>
<span id="cb61-64"><a href="#cb61-64" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.</span></span>
<span id="cb61-65"><a href="#cb61-65" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.</span></span>
<span id="cb61-66"><a href="#cb61-66" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.</span></span>
<span id="cb61-67"><a href="#cb61-67" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.</span></span>
<span id="cb61-68"><a href="#cb61-68" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb61-69"><a href="#cb61-69" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb61-70"><a href="#cb61-70" aria-hidden="true" tabindex="-1"></a><span class="co">        trajectories (float[float[]]): a list with shape `(n_bins, n_points, len(df.columns) - 1)`</span></span>
<span id="cb61-71"><a href="#cb61-71" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated trajectories.</span></span>
<span id="cb61-72"><a href="#cb61-72" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb61-73"><a href="#cb61-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># groups = sorted(df[samples_key].unique())</span></span>
<span id="cb61-74"><a href="#cb61-74" aria-hidden="true" tabindex="-1"></a>    sample_time <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n_bins)</span>
<span id="cb61-75"><a href="#cb61-75" aria-hidden="true" tabindex="-1"></a>    trajectories <span class="op">=</span> generate_points_flat(model, df, n_trajectories, sample_with_replacement, use_cuda, samples_key, sample_time,autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon)</span>
<span id="cb61-76"><a href="#cb61-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectories</span>
<span id="cb61-77"><a href="#cb61-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb61-78"><a href="#cb61-78" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_plot_data_flat(</span>
<span id="cb61-79"><a href="#cb61-79" aria-hidden="true" tabindex="-1"></a>    model, df, n_points, n_trajectories, n_bins, </span>
<span id="cb61-80"><a href="#cb61-80" aria-hidden="true" tabindex="-1"></a>    sample_with_replacement<span class="op">=</span><span class="va">False</span>, use_cuda<span class="op">=</span><span class="va">False</span>, samples_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb61-81"><a href="#cb61-81" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span><span class="va">None</span>, autoencoder<span class="op">=</span><span class="va">None</span>, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb61-82"><a href="#cb61-82" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb61-83"><a href="#cb61-83" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb61-84"><a href="#cb61-84" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb61-85"><a href="#cb61-85" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb61-86"><a href="#cb61-86" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.</span></span>
<span id="cb61-87"><a href="#cb61-87" aria-hidden="true" tabindex="-1"></a><span class="co">            See `MIOFlow.ode` for more.</span></span>
<span id="cb61-88"><a href="#cb61-88" aria-hidden="true" tabindex="-1"></a><span class="co">        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.</span></span>
<span id="cb61-89"><a href="#cb61-89" aria-hidden="true" tabindex="-1"></a><span class="co">        n_points (int): Number of points to generate.</span></span>
<span id="cb61-90"><a href="#cb61-90" aria-hidden="true" tabindex="-1"></a><span class="co">        n_trajectories (int): Number of trajectories to generate.</span></span>
<span id="cb61-91"><a href="#cb61-91" aria-hidden="true" tabindex="-1"></a><span class="co">        n_bins (int): Number of bins to use for the trajectories. More makes it smoother. Defaults to `100`.</span></span>
<span id="cb61-92"><a href="#cb61-92" aria-hidden="true" tabindex="-1"></a><span class="co">        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling</span></span>
<span id="cb61-93"><a href="#cb61-93" aria-hidden="true" tabindex="-1"></a><span class="co">            initial timepoint.</span></span>
<span id="cb61-94"><a href="#cb61-94" aria-hidden="true" tabindex="-1"></a><span class="co">        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.</span></span>
<span id="cb61-95"><a href="#cb61-95" aria-hidden="true" tabindex="-1"></a><span class="co">        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.</span></span>
<span id="cb61-96"><a href="#cb61-96" aria-hidden="true" tabindex="-1"></a><span class="co">        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.</span></span>
<span id="cb61-97"><a href="#cb61-97" aria-hidden="true" tabindex="-1"></a><span class="co">        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.</span></span>
<span id="cb61-98"><a href="#cb61-98" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb61-99"><a href="#cb61-99" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb61-100"><a href="#cb61-100" aria-hidden="true" tabindex="-1"></a><span class="co">        points (float[float[]]): a list with shape `(len(df[sample_key].unique()), n_points, len(df.columns) - 1)`</span></span>
<span id="cb61-101"><a href="#cb61-101" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated points.</span></span>
<span id="cb61-102"><a href="#cb61-102" aria-hidden="true" tabindex="-1"></a><span class="co">        trajectories (float[float[]]): a list with shape `(n_bins, n_points, len(df.columns) - 1)`</span></span>
<span id="cb61-103"><a href="#cb61-103" aria-hidden="true" tabindex="-1"></a><span class="co">            of the generated trajectories.</span></span>
<span id="cb61-104"><a href="#cb61-104" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb61-105"><a href="#cb61-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> logger: logger.info(<span class="ss">f'Generating points'</span>)</span>
<span id="cb61-106"><a href="#cb61-106" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> generate_points_flat(model, df, n_points, sample_with_replacement, use_cuda, samples_key, <span class="va">None</span>, autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon)</span>
<span id="cb61-107"><a href="#cb61-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> logger: logger.info(<span class="ss">f'Generating trajectories'</span>)</span>
<span id="cb61-108"><a href="#cb61-108" aria-hidden="true" tabindex="-1"></a>    trajectories <span class="op">=</span> generate_trajectories_flat(model, df, n_trajectories, n_bins, sample_with_replacement, use_cuda, samples_key, autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span>recon)</span>
<span id="cb61-109"><a href="#cb61-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> points, trajectories</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-81 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, math, numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>sns.color_palette(<span class="st">"bright"</span>)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_comparison_flat(</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    df, generated, trajectories,</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">False</span>, path<span class="op">=</span><span class="st">"../../results/"</span>, <span class="bu">file</span><span class="op">=</span><span class="st">'comparision.png'</span>,</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.isdir(path):</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>        os.makedirs(path)</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> is_3d:</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> new_plot_comparisons_flat(</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>            df, generated, trajectories,</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>            palette<span class="op">=</span>palette, df_time_key<span class="op">=</span>df_time_key,</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>x, y<span class="op">=</span>y, z<span class="op">=</span>z, is_3d<span class="op">=</span>is_3d,            </span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a>            groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>            save<span class="op">=</span>save, path<span class="op">=</span>path, <span class="bu">file</span><span class="op">=</span><span class="bu">file</span>,</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span><span class="op">/</span>s, <span class="dv">8</span><span class="op">/</span>s), dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> <span class="bu">sorted</span>(df[df_time_key].unique())</span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-40"><a href="#cb62-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb62-41"><a href="#cb62-41" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb62-42"><a href="#cb62-42" aria-hidden="true" tabindex="-1"></a>            df[x], df[y], df[z],</span>
<span id="cb62-43"><a href="#cb62-43" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>palette, alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb62-44"><a href="#cb62-44" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>df[df_time_key], </span>
<span id="cb62-45"><a href="#cb62-45" aria-hidden="true" tabindex="-1"></a>            s<span class="op">=</span>df[df_time_key], </span>
<span id="cb62-46"><a href="#cb62-46" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="st">'X'</span>,</span>
<span id="cb62-47"><a href="#cb62-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-48"><a href="#cb62-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb62-49"><a href="#cb62-49" aria-hidden="true" tabindex="-1"></a>        sns.scatterplot(</span>
<span id="cb62-50"><a href="#cb62-50" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>df, x<span class="op">=</span>x, y<span class="op">=</span>y, palette<span class="op">=</span>palette, alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb62-51"><a href="#cb62-51" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span>df_time_key, style<span class="op">=</span>df_time_key, size<span class="op">=</span>df_time_key,</span>
<span id="cb62-52"><a href="#cb62-52" aria-hidden="true" tabindex="-1"></a>            markers<span class="op">=</span>{g: <span class="st">'X'</span> <span class="cf">for</span> g <span class="kw">in</span> states},</span>
<span id="cb62-53"><a href="#cb62-53" aria-hidden="true" tabindex="-1"></a>            sizes<span class="op">=</span>{g: <span class="dv">100</span> <span class="cf">for</span> g <span class="kw">in</span> states}, </span>
<span id="cb62-54"><a href="#cb62-54" aria-hidden="true" tabindex="-1"></a>            ax<span class="op">=</span>ax, legend<span class="op">=</span><span class="va">False</span></span>
<span id="cb62-55"><a href="#cb62-55" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-56"><a href="#cb62-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-57"><a href="#cb62-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-58"><a href="#cb62-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(generated, np.ndarray):</span>
<span id="cb62-59"><a href="#cb62-59" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">=</span> to_np(generated)</span>
<span id="cb62-60"><a href="#cb62-60" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.concatenate(generated, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb62-61"><a href="#cb62-61" aria-hidden="true" tabindex="-1"></a>    n_gen <span class="op">=</span> <span class="bu">int</span>(points.shape[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(states))</span>
<span id="cb62-62"><a href="#cb62-62" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [state <span class="cf">for</span> state <span class="kw">in</span> states <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_gen)]</span>
<span id="cb62-63"><a href="#cb62-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-64"><a href="#cb62-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb62-65"><a href="#cb62-65" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb62-66"><a href="#cb62-66" aria-hidden="true" tabindex="-1"></a>            points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>], points[:, <span class="dv">2</span>],</span>
<span id="cb62-67"><a href="#cb62-67" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>palette,</span>
<span id="cb62-68"><a href="#cb62-68" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>colors, </span>
<span id="cb62-69"><a href="#cb62-69" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-70"><a href="#cb62-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb62-71"><a href="#cb62-71" aria-hidden="true" tabindex="-1"></a>        sns.scatterplot(</span>
<span id="cb62-72"><a href="#cb62-72" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>points[:, <span class="dv">0</span>], y<span class="op">=</span>points[:, <span class="dv">1</span>], palette<span class="op">=</span>palette,</span>
<span id="cb62-73"><a href="#cb62-73" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span>colors, </span>
<span id="cb62-74"><a href="#cb62-74" aria-hidden="true" tabindex="-1"></a>            ax<span class="op">=</span>ax, legend<span class="op">=</span><span class="va">False</span></span>
<span id="cb62-75"><a href="#cb62-75" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-76"><a href="#cb62-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-77"><a href="#cb62-77" aria-hidden="true" tabindex="-1"></a>    ax.legend(title<span class="op">=</span><span class="st">'Timepoint'</span>, loc<span class="op">=</span><span class="st">'upper left'</span>, labels<span class="op">=</span>[<span class="st">'Ground Truth'</span>, <span class="st">'Predicted'</span>])</span>
<span id="cb62-78"><a href="#cb62-78" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'ODE Points compared to Ground Truth'</span>)</span>
<span id="cb62-79"><a href="#cb62-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-80"><a href="#cb62-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_3d:</span>
<span id="cb62-81"><a href="#cb62-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trajectory <span class="kw">in</span> np.transpose(trajectories, axes<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)):</span>
<span id="cb62-82"><a href="#cb62-82" aria-hidden="true" tabindex="-1"></a>            plt.plot(trajectory[:, <span class="dv">0</span>], trajectory[:, <span class="dv">1</span>], trajectory[:, <span class="dv">2</span>], alpha<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">'Black'</span>)<span class="op">;</span></span>
<span id="cb62-83"><a href="#cb62-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb62-84"><a href="#cb62-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trajectory <span class="kw">in</span> np.transpose(trajectories, axes<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)):</span>
<span id="cb62-85"><a href="#cb62-85" aria-hidden="true" tabindex="-1"></a>            plt.plot(trajectory[:, <span class="dv">0</span>], trajectory[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">'Black'</span>)<span class="op">;</span></span>
<span id="cb62-86"><a href="#cb62-86" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-87"><a href="#cb62-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> save:</span>
<span id="cb62-88"><a href="#cb62-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: savefig complains image is too large but saves it anyway. </span></span>
<span id="cb62-89"><a href="#cb62-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb62-90"><a href="#cb62-90" aria-hidden="true" tabindex="-1"></a>            fig.savefig(os.path.expanduser(os.path.join(path, <span class="bu">file</span>)))</span>
<span id="cb62-91"><a href="#cb62-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb62-92"><a href="#cb62-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span> </span>
<span id="cb62-93"><a href="#cb62-93" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb62-94"><a href="#cb62-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>::: {#cell-82 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Patch</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.lines <span class="im">import</span> Line2D</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rcParams, cycler</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> new_plot_comparisons_flat(</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    df, generated, trajectories,</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    palette <span class="op">=</span> <span class="st">'viridis'</span>,</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, </span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">False</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'comparision.png'</span>,</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>    is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> groups <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        groups <span class="op">=</span> <span class="bu">sorted</span>(df[df_time_key].unique())</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    cmap <span class="op">=</span> plt.cm.viridis</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    sns.set_palette(palette)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    plt.rcParams.update({</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.prop_cycle'</span>: plt.cycler(color<span class="op">=</span>cmap(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(groups) <span class="op">+</span> <span class="dv">1</span>))),</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.axisbelow'</span>: <span class="va">False</span>,</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.edgecolor'</span>: <span class="st">'lightgrey'</span>,</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.facecolor'</span>: <span class="st">'None'</span>,</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.grid'</span>: <span class="va">False</span>,</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.labelcolor'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.spines.right'</span>: <span class="va">False</span>,</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.spines.top'</span>: <span class="va">False</span>,</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">'figure.facecolor'</span>: <span class="st">'white'</span>,</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lines.solid_capstyle'</span>: <span class="st">'round'</span>,</span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">'patch.edgecolor'</span>: <span class="st">'w'</span>,</span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">'patch.force_edgecolor'</span>: <span class="va">True</span>,</span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">'text.color'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.bottom'</span>: <span class="va">False</span>,</span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.color'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.direction'</span>: <span class="st">'out'</span>,</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">'xtick.top'</span>: <span class="va">False</span>,</span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.color'</span>: <span class="st">'dimgrey'</span>,</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.direction'</span>: <span class="st">'out'</span>,</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.left'</span>: <span class="va">False</span>,</span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">'ytick.right'</span>: <span class="va">False</span>, </span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">'font.size'</span>:<span class="dv">12</span>, </span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.titlesize'</span>:<span class="dv">10</span>,</span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">'axes.labelsize'</span>:<span class="dv">12</span></span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a>    n_cols <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a>    n_rols <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a>    grid_figsize <span class="op">=</span> [<span class="dv">12</span>, <span class="dv">8</span>]</span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a>    dpi <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>    grid_figsize <span class="op">=</span> (grid_figsize[<span class="dv">0</span>] <span class="op">*</span> n_cols, grid_figsize[<span class="dv">1</span>] <span class="op">*</span> n_rols)</span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(<span class="va">None</span>, grid_figsize, dpi<span class="op">=</span>dpi)</span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a>    hspace <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a>    wspace <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-56"><a href="#cb63-56" aria-hidden="true" tabindex="-1"></a>    gspec <span class="op">=</span> plt.GridSpec(n_rols, n_cols, fig, hspace<span class="op">=</span>hspace, wspace<span class="op">=</span>wspace)</span>
<span id="cb63-57"><a href="#cb63-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-58"><a href="#cb63-58" aria-hidden="true" tabindex="-1"></a>    outline_width <span class="op">=</span> (<span class="fl">0.3</span>, <span class="fl">0.05</span>)</span>
<span id="cb63-59"><a href="#cb63-59" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb63-60"><a href="#cb63-60" aria-hidden="true" tabindex="-1"></a>    bg_width, gap_width <span class="op">=</span> outline_width</span>
<span id="cb63-61"><a href="#cb63-61" aria-hidden="true" tabindex="-1"></a>    point <span class="op">=</span> np.sqrt(size)</span>
<span id="cb63-62"><a href="#cb63-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-63"><a href="#cb63-63" aria-hidden="true" tabindex="-1"></a>    gap_size <span class="op">=</span> (point <span class="op">+</span> (point <span class="op">*</span> gap_width) <span class="op">*</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb63-64"><a href="#cb63-64" aria-hidden="true" tabindex="-1"></a>    bg_size <span class="op">=</span> (np.sqrt(gap_size) <span class="op">+</span> (point <span class="op">*</span> bg_width) <span class="op">*</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb63-65"><a href="#cb63-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-66"><a href="#cb63-66" aria-hidden="true" tabindex="-1"></a>    plt.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-67"><a href="#cb63-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-68"><a href="#cb63-68" aria-hidden="true" tabindex="-1"></a>    is_3d <span class="op">=</span> <span class="va">False</span></span>
<span id="cb63-69"><a href="#cb63-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-70"><a href="#cb63-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if is_3d:        </span></span>
<span id="cb63-71"><a href="#cb63-71" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     ax = fig.add_subplot(1,1,1,projection='3d')</span></span>
<span id="cb63-72"><a href="#cb63-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># else:</span></span>
<span id="cb63-73"><a href="#cb63-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     ax = fig.add_subplot(1,1,1)</span></span>
<span id="cb63-74"><a href="#cb63-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-75"><a href="#cb63-75" aria-hidden="true" tabindex="-1"></a>    axs <span class="op">=</span> []</span>
<span id="cb63-76"><a href="#cb63-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, gs <span class="kw">in</span> <span class="bu">enumerate</span>(gspec):        </span>
<span id="cb63-77"><a href="#cb63-77" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(gs)</span>
<span id="cb63-78"><a href="#cb63-78" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-79"><a href="#cb63-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-80"><a href="#cb63-80" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="fl">0.3</span>   </span>
<span id="cb63-81"><a href="#cb63-81" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb63-82"><a href="#cb63-82" aria-hidden="true" tabindex="-1"></a>                df[x], df[y],</span>
<span id="cb63-83"><a href="#cb63-83" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span>df[df_time_key],</span>
<span id="cb63-84"><a href="#cb63-84" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>size,</span>
<span id="cb63-85"><a href="#cb63-85" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.7</span> <span class="op">*</span> n,</span>
<span id="cb63-86"><a href="#cb63-86" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'X'</span>,</span>
<span id="cb63-87"><a href="#cb63-87" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb63-88"><a href="#cb63-88" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-89"><a href="#cb63-89" aria-hidden="true" tabindex="-1"></a>                cmap<span class="op">=</span>cmap</span>
<span id="cb63-90"><a href="#cb63-90" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb63-91"><a href="#cb63-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-92"><a href="#cb63-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trajectory <span class="kw">in</span> np.transpose(trajectories, axes<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)):</span>
<span id="cb63-93"><a href="#cb63-93" aria-hidden="true" tabindex="-1"></a>                plt.plot(trajectory[:, <span class="dv">0</span>], trajectory[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'Black'</span>)<span class="op">;</span></span>
<span id="cb63-94"><a href="#cb63-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-95"><a href="#cb63-95" aria-hidden="true" tabindex="-1"></a>        states <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, generated.shape[<span class="dv">0</span>]) <span class="co">#sorted(df[df_time_key].unique())</span></span>
<span id="cb63-96"><a href="#cb63-96" aria-hidden="true" tabindex="-1"></a>        points <span class="op">=</span> np.concatenate(generated, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb63-97"><a href="#cb63-97" aria-hidden="true" tabindex="-1"></a>        n_gen <span class="op">=</span> <span class="bu">int</span>(points.shape[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(states))</span>
<span id="cb63-98"><a href="#cb63-98" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> [state <span class="cf">for</span> state <span class="kw">in</span> states <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_gen)]</span>
<span id="cb63-99"><a href="#cb63-99" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb63-100"><a href="#cb63-100" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> <span class="st">'.'</span></span>
<span id="cb63-101"><a href="#cb63-101" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb63-102"><a href="#cb63-102" aria-hidden="true" tabindex="-1"></a>                points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>],</span>
<span id="cb63-103"><a href="#cb63-103" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span><span class="st">'black'</span>,</span>
<span id="cb63-104"><a href="#cb63-104" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>bg_size,</span>
<span id="cb63-105"><a href="#cb63-105" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="dv">1</span> <span class="op">*</span> n,</span>
<span id="cb63-106"><a href="#cb63-106" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span>o,</span>
<span id="cb63-107"><a href="#cb63-107" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb63-108"><a href="#cb63-108" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span></span>
<span id="cb63-109"><a href="#cb63-109" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb63-110"><a href="#cb63-110" aria-hidden="true" tabindex="-1"></a>        ax.scatter(</span>
<span id="cb63-111"><a href="#cb63-111" aria-hidden="true" tabindex="-1"></a>                points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>],</span>
<span id="cb63-112"><a href="#cb63-112" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb63-113"><a href="#cb63-113" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>gap_size,</span>
<span id="cb63-114"><a href="#cb63-114" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="dv">1</span> <span class="op">*</span> n,</span>
<span id="cb63-115"><a href="#cb63-115" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span>o,</span>
<span id="cb63-116"><a href="#cb63-116" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb63-117"><a href="#cb63-117" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span></span>
<span id="cb63-118"><a href="#cb63-118" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb63-119"><a href="#cb63-119" aria-hidden="true" tabindex="-1"></a>        pnts <span class="op">=</span> ax.scatter(</span>
<span id="cb63-120"><a href="#cb63-120" aria-hidden="true" tabindex="-1"></a>                points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>],</span>
<span id="cb63-121"><a href="#cb63-121" aria-hidden="true" tabindex="-1"></a>                c<span class="op">=</span>colors,</span>
<span id="cb63-122"><a href="#cb63-122" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span>size,</span>
<span id="cb63-123"><a href="#cb63-123" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.7</span> <span class="op">*</span> n,</span>
<span id="cb63-124"><a href="#cb63-124" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span>o,</span>
<span id="cb63-125"><a href="#cb63-125" aria-hidden="true" tabindex="-1"></a>                linewidths<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb63-126"><a href="#cb63-126" aria-hidden="true" tabindex="-1"></a>                edgecolors<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-127"><a href="#cb63-127" aria-hidden="true" tabindex="-1"></a>                cmap<span class="op">=</span>cmap</span>
<span id="cb63-128"><a href="#cb63-128" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb63-129"><a href="#cb63-129" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb63-130"><a href="#cb63-130" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cbar = plt.colorbar(pnts, pad=0.05)</span></span>
<span id="cb63-131"><a href="#cb63-131" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cbar.set_label('States')</span></span>
<span id="cb63-132"><a href="#cb63-132" aria-hidden="true" tabindex="-1"></a>        legend_elements <span class="op">=</span> [        </span>
<span id="cb63-133"><a href="#cb63-133" aria-hidden="true" tabindex="-1"></a>            Line2D(</span>
<span id="cb63-134"><a href="#cb63-134" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'X'</span>, color<span class="op">=</span><span class="st">'w'</span>, </span>
<span id="cb63-135"><a href="#cb63-135" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="st">'Ground Truth'</span>, markerfacecolor<span class="op">=</span>cmap(<span class="dv">0</span>), markersize<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.3</span></span>
<span id="cb63-136"><a href="#cb63-136" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb63-137"><a href="#cb63-137" aria-hidden="true" tabindex="-1"></a>            Line2D([<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'w'</span>, label<span class="op">=</span><span class="st">'Predicted'</span>, markerfacecolor<span class="op">=</span>cmap(<span class="fl">.999</span>), markersize<span class="op">=</span><span class="dv">15</span>),</span>
<span id="cb63-138"><a href="#cb63-138" aria-hidden="true" tabindex="-1"></a>            Line2D([<span class="dv">0</span>], [<span class="dv">0</span>], color<span class="op">=</span><span class="st">'black'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Trajectory'</span>)</span>
<span id="cb63-139"><a href="#cb63-139" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb63-140"><a href="#cb63-140" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-141"><a href="#cb63-141" aria-hidden="true" tabindex="-1"></a>        leg <span class="op">=</span> plt.legend(handles<span class="op">=</span>legend_elements, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb63-142"><a href="#cb63-142" aria-hidden="true" tabindex="-1"></a>        ax.add_artist(leg)</span>
<span id="cb63-143"><a href="#cb63-143" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-144"><a href="#cb63-144" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">""</span>)</span>
<span id="cb63-145"><a href="#cb63-145" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="st">""</span>)</span>
<span id="cb63-146"><a href="#cb63-146" aria-hidden="true" tabindex="-1"></a>        ax.get_xaxis().get_major_formatter().set_scientific(<span class="va">False</span>)</span>
<span id="cb63-147"><a href="#cb63-147" aria-hidden="true" tabindex="-1"></a>        ax.get_yaxis().get_major_formatter().set_scientific(<span class="va">False</span>)</span>
<span id="cb63-148"><a href="#cb63-148" aria-hidden="true" tabindex="-1"></a>        kwargs <span class="op">=</span> <span class="bu">dict</span>(bottom<span class="op">=</span><span class="va">False</span>, left<span class="op">=</span><span class="va">False</span>, labelbottom<span class="op">=</span><span class="va">False</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-149"><a href="#cb63-149" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(which<span class="op">=</span><span class="st">"both"</span>, <span class="op">**</span>kwargs)</span>
<span id="cb63-150"><a href="#cb63-150" aria-hidden="true" tabindex="-1"></a>        ax.set_frame_on(<span class="va">False</span>)</span>
<span id="cb63-151"><a href="#cb63-151" aria-hidden="true" tabindex="-1"></a>        ax.patch.set_alpha(<span class="dv">0</span>)</span>
<span id="cb63-152"><a href="#cb63-152" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-153"><a href="#cb63-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-154"><a href="#cb63-154" aria-hidden="true" tabindex="-1"></a>        axs.append(ax)</span>
<span id="cb63-155"><a href="#cb63-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-156"><a href="#cb63-156" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> save:</span>
<span id="cb63-157"><a href="#cb63-157" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: savefig complains image is too large but saves it anyway. </span></span>
<span id="cb63-158"><a href="#cb63-158" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb63-159"><a href="#cb63-159" aria-hidden="true" tabindex="-1"></a>            fig.savefig(os.path.expanduser(os.path.join(path, <span class="bu">file</span>)))</span>
<span id="cb63-160"><a href="#cb63-160" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb63-161"><a href="#cb63-161" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span> </span>
<span id="cb63-162"><a href="#cb63-162" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb63-163"><a href="#cb63-163" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
<section id="train-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="train-autoencoder">Train Autoencoder</h3>
<div id="cell-84" class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>set_seeds(<span class="dv">0</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>use_cuda <span class="op">=</span> torch.cuda.is_available() <span class="co"># This is True if we want to holdout (or skip) one timepoint during training. It is used to test the accuracy of the trajectories on unseen data.</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>hold_one_out <span class="op">=</span> <span class="va">False</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co"># It can be a group number or 'random', works in tandem with hold_one_out</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>hold_out <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The dimensions in the input space, it is columns - 2 because we assume one column is equal to "samples", and one to </span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>model_features <span class="op">=</span> <span class="bu">len</span>(df.columns) <span class="op">-</span> <span class="dv">2</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>groups <span class="op">=</span> <span class="bu">sorted</span>(df.samples.unique()) <span class="co"># These determine the logic flow for training: </span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=True use_gae=False is only the encoder to match the approximation of the geodesic.</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=False use_gae=True the full Geodesic Autoencoder (GAE), i.e. matching the geodesic and a reconstruction loss.</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=False use_gae=False Is not using the GAE.</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co">#   use_emb=True use_gae=True, is redundant and should raise an error. </span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>use_emb <span class="op">=</span> <span class="va">False</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>use_gae <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>need_to_train_gae <span class="op">=</span> (use_emb <span class="kw">or</span> use_gae) <span class="kw">and</span> use_emb <span class="op">!=</span> use_gae</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="co"># If the reconstruction loss needs to be computed.</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>recon <span class="op">=</span> use_gae <span class="kw">and</span> <span class="kw">not</span> use_emb </span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="co"># These are training GAE hyperparameters needed for training</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Distance_type in ['gaussian', 'alpha_decay'], and Gaussian scale</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>distance_type <span class="op">=</span> <span class="st">'gaussian'</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>rbf_length_scale<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> setup_distance(distance_type, rbf_length_scale<span class="op">=</span>rbf_length_scale)</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="co">#Can be changed depending on the dataset</span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>n_epochs_emb <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>samples_size_emb <span class="op">=</span> (<span class="dv">30</span>, )</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Layers for the Geodesic Autoencoder</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>gae_embedded_dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>encoder_layers <span class="op">=</span> [model_features, <span class="dv">8</span>, gae_embedded_dim]</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>gae <span class="op">=</span> Autoencoder(</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>    encoder_layers <span class="op">=</span> encoder_layers,</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>    decoder_layers <span class="op">=</span> encoder_layers[::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">'ReLU'</span>, use_cuda <span class="op">=</span> use_cuda</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(gae.parameters()) <span class="co"># Added in extra cell just for iterative programming / running of code</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a><span class="co">#   but could be added to code block above</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> need_to_train_gae:</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>    start_time_geo <span class="op">=</span> time.time()</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> train_ae(</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>        gae, df, groups, optimizer, </span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>        n_epochs<span class="op">=</span>n_epochs_emb, sample_size<span class="op">=</span>samples_size_emb,</span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>        noise_min_scale<span class="op">=</span><span class="fl">0.09</span>, noise_max_scale<span class="op">=</span><span class="fl">0.15</span>, </span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>        dist<span class="op">=</span>dist, recon<span class="op">=</span>recon, use_cuda<span class="op">=</span>use_cuda,</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>        hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>    run_time_geo <span class="op">=</span> time.time() <span class="op">-</span> start_time_geo</span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'run time:'</span>,run_time_geo)</span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>    autoencoder <span class="op">=</span> gae</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>    autoencoder <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e342557144524e76b2dd0b0844010287","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train loss recon: 0.29978
Train loss dist: 14.21898
Train loss recon: 0.1959
Train loss dist: 13.8237
Train loss recon: 0.14264
Train loss dist: 12.44598
Train loss recon: 0.14028
Train loss dist: 10.7756
Train loss recon: 0.10549
Train loss dist: 6.65429
Train loss recon: 0.07052
Train loss dist: 6.06167
Train loss recon: 0.06131
Train loss dist: 6.42952
Train loss recon: 0.03826
Train loss dist: 5.49068
Train loss recon: 0.04637
Train loss dist: 5.00403
Train loss recon: 0.04955
Train loss dist: 5.21386
Train loss recon: 0.03427
Train loss dist: 4.99325
Train loss recon: 0.04833
Train loss dist: 5.24775
Train loss recon: 0.0556
Train loss dist: 5.33601
Train loss recon: 0.04755
Train loss dist: 5.42864
Train loss recon: 0.03774
Train loss dist: 4.66094
Train loss recon: 0.04395
Train loss dist: 5.45364
Train loss recon: 0.04013
Train loss dist: 5.06383
Train loss recon: 0.05005
Train loss dist: 5.3801
Train loss recon: 0.03372
Train loss dist: 5.51523
Train loss recon: 0.04161
Train loss dist: 5.20342
Train loss recon: 0.03844
Train loss dist: 4.64227
Train loss recon: 0.04762
Train loss dist: 5.00493
Train loss recon: 0.04017
Train loss dist: 4.84484
Train loss recon: 0.03416
Train loss dist: 5.44035
Train loss recon: 0.03585
Train loss dist: 4.843
Train loss recon: 0.03061
Train loss dist: 4.73335
Train loss recon: 0.02642
Train loss dist: 5.05248
Train loss recon: 0.0285
Train loss dist: 4.80238
Train loss recon: 0.04175
Train loss dist: 4.77051
Train loss recon: 0.04292
Train loss dist: 5.47804
Train loss recon: 0.04829
Train loss dist: 5.31583
Train loss recon: 0.03592
Train loss dist: 5.05193
Train loss recon: 0.03385
Train loss dist: 4.89765
Train loss recon: 0.04207
Train loss dist: 4.77998
Train loss recon: 0.03104
Train loss dist: 5.03058
Train loss recon: 0.03242
Train loss dist: 4.90398
Train loss recon: 0.03317
Train loss dist: 5.12947
Train loss recon: 0.03357
Train loss dist: 4.55094
Train loss recon: 0.03463
Train loss dist: 4.91162
Train loss recon: 0.03648
Train loss dist: 4.92186
Train loss recon: 0.03123
Train loss dist: 5.04462
Train loss recon: 0.04179
Train loss dist: 5.05214
Train loss recon: 0.03139
Train loss dist: 4.6634
Train loss recon: 0.04303
Train loss dist: 4.85329
Train loss recon: 0.02498
Train loss dist: 4.95734
Train loss recon: 0.03313
Train loss dist: 5.16042
Train loss recon: 0.03663
Train loss dist: 4.92924
Train loss recon: 0.03867
Train loss dist: 4.97585
Train loss recon: 0.02341
Train loss dist: 4.83968
Train loss recon: 0.03044
Train loss dist: 5.08849
Train loss recon: 0.03829
Train loss dist: 4.62376
Train loss recon: 0.04586
Train loss dist: 5.29302
Train loss recon: 0.03759
Train loss dist: 4.91762
Train loss recon: 0.03614
Train loss dist: 4.6513
Train loss recon: 0.03544
Train loss dist: 5.28495
Train loss recon: 0.04198
Train loss dist: 4.95934
Train loss recon: 0.04378
Train loss dist: 4.92449
Train loss recon: 0.03921
Train loss dist: 4.62219
Train loss recon: 0.02299
Train loss dist: 4.58913
Train loss recon: 0.02894
Train loss dist: 4.98756
Train loss recon: 0.02403
Train loss dist: 4.62241
Train loss recon: 0.05574
Train loss dist: 5.50261
Train loss recon: 0.05465
Train loss dist: 4.88215
Train loss recon: 0.03533
Train loss dist: 4.63413
Train loss recon: 0.0389
Train loss dist: 5.11089
Train loss recon: 0.03945
Train loss dist: 4.68378
Train loss recon: 0.04565
Train loss dist: 4.64126
Train loss recon: 0.03496
Train loss dist: 4.87198
Train loss recon: 0.03932
Train loss dist: 4.90814
Train loss recon: 0.02484
Train loss dist: 4.66099
Train loss recon: 0.02889
Train loss dist: 5.12584
Train loss recon: 0.0449
Train loss dist: 4.81695
Train loss recon: 0.03801
Train loss dist: 5.20867
Train loss recon: 0.03469
Train loss dist: 4.92989
Train loss recon: 0.0352
Train loss dist: 4.62843
Train loss recon: 0.04315
Train loss dist: 4.53723
Train loss recon: 0.03891
Train loss dist: 4.93336
Train loss recon: 0.04784
Train loss dist: 4.89815
Train loss recon: 0.03113
Train loss dist: 4.59524
Train loss recon: 0.04102
Train loss dist: 4.7057
Train loss recon: 0.03461
Train loss dist: 4.56004
Train loss recon: 0.03051
Train loss dist: 4.74329
Train loss recon: 0.03336
Train loss dist: 4.61661
Train loss recon: 0.03774
Train loss dist: 4.76561
Train loss recon: 0.03035
Train loss dist: 4.53095
Train loss recon: 0.02833
Train loss dist: 4.91449
Train loss recon: 0.03495
Train loss dist: 4.53804
Train loss recon: 0.03218
Train loss dist: 4.9197
Train loss recon: 0.03763
Train loss dist: 5.36351
Train loss recon: 0.02813
Train loss dist: 4.59649
Train loss recon: 0.03336
Train loss dist: 4.58902
Train loss recon: 0.03551
Train loss dist: 4.65735
Train loss recon: 0.04721
Train loss dist: 5.10036
Train loss recon: 0.0425
Train loss dist: 4.89055
Train loss recon: 0.03863
Train loss dist: 5.03523
Train loss recon: 0.03383
Train loss dist: 4.56323
Train loss recon: 0.03148
Train loss dist: 4.77757
Train loss recon: 0.03495
Train loss dist: 5.10973
Train loss recon: 0.03588
Train loss dist: 4.52369
Train loss recon: 0.04815
Train loss dist: 5.252
Train loss recon: 0.032
Train loss dist: 5.43188
Train loss recon: 0.02708
Train loss dist: 4.99387
Train loss recon: 0.03373
Train loss dist: 5.3603
Train loss recon: 0.03797
Train loss dist: 4.78494
Train loss recon: 0.06023
Train loss dist: 5.12181
Train loss recon: 0.04541
Train loss dist: 4.93475
Train loss recon: 0.03657
Train loss dist: 4.71971
Train loss recon: 0.03165
Train loss dist: 4.6597
Train loss recon: 0.03777
Train loss dist: 5.05777
Train loss recon: 0.03905
Train loss dist: 4.47953
Train loss recon: 0.0318
Train loss dist: 4.48638
Train loss recon: 0.04863
Train loss dist: 4.50907
Train loss recon: 0.03232
Train loss dist: 4.80382
Train loss recon: 0.03784
Train loss dist: 4.66494
Train loss recon: 0.03513
Train loss dist: 4.41659
Train loss recon: 0.03552
Train loss dist: 5.23656
Train loss recon: 0.03677
Train loss dist: 4.81617
Train loss recon: 0.02703
Train loss dist: 5.05139
Train loss recon: 0.0459
Train loss dist: 4.97646
Train loss recon: 0.04949
Train loss dist: 4.91825
Train loss recon: 0.02877
Train loss dist: 5.31773
Train loss recon: 0.04028
Train loss dist: 5.20704
Train loss recon: 0.03129
Train loss dist: 4.48452
Train loss recon: 0.03757
Train loss dist: 4.66998
Train loss recon: 0.03637
Train loss dist: 5.66429
Train loss recon: 0.04459
Train loss dist: 4.25673
Train loss recon: 0.03087
Train loss dist: 4.68505
Train loss recon: 0.04594
Train loss dist: 4.72121
Train loss recon: 0.03911
Train loss dist: 4.38591
Train loss recon: 0.03184
Train loss dist: 5.24584
Train loss recon: 0.03437
Train loss dist: 4.48447
Train loss recon: 0.01866
Train loss dist: 4.93018
Train loss recon: 0.04442
Train loss dist: 5.02048
Train loss recon: 0.03625
Train loss dist: 5.37211
Train loss recon: 0.03446
Train loss dist: 4.54933
Train loss recon: 0.04181
Train loss dist: 4.70711
Train loss recon: 0.03667
Train loss dist: 4.49791
Train loss recon: 0.04025
Train loss dist: 5.14559
Train loss recon: 0.0356
Train loss dist: 4.69952
Train loss recon: 0.03212
Train loss dist: 4.65852
Train loss recon: 0.03657
Train loss dist: 4.45978
Train loss recon: 0.03633
Train loss dist: 4.32306
Train loss recon: 0.0304
Train loss dist: 5.19078
Train loss recon: 0.02643
Train loss dist: 5.30683
Train loss recon: 0.03263
Train loss dist: 4.76333
Train loss recon: 0.03918
Train loss dist: 5.10087
Train loss recon: 0.03423
Train loss dist: 4.72177
Train loss recon: 0.03457
Train loss dist: 4.83049
Train loss recon: 0.03893
Train loss dist: 5.06144
Train loss recon: 0.05301
Train loss dist: 4.6376
Train loss recon: 0.02876
Train loss dist: 4.94981
Train loss recon: 0.02965
Train loss dist: 4.78324
Train loss recon: 0.03808
Train loss dist: 4.45806
Train loss recon: 0.03086
Train loss dist: 4.62747
Train loss recon: 0.03325
Train loss dist: 4.52188
Train loss recon: 0.04068
Train loss dist: 4.45452
Train loss recon: 0.04172
Train loss dist: 4.88524
Train loss recon: 0.03011
Train loss dist: 4.92795
Train loss recon: 0.03486
Train loss dist: 4.61232
Train loss recon: 0.04566
Train loss dist: 5.10318
Train loss recon: 0.04133
Train loss dist: 4.82648
Train loss recon: 0.03685
Train loss dist: 5.00526
Train loss recon: 0.02854
Train loss dist: 5.10366
Train loss recon: 0.01746
Train loss dist: 4.48784
Train loss recon: 0.0387
Train loss dist: 4.47901
Train loss recon: 0.02234
Train loss dist: 4.5681
Train loss recon: 0.04388
Train loss dist: 4.5321
Train loss recon: 0.03841
Train loss dist: 4.60829
Train loss recon: 0.02424
Train loss dist: 4.35806
Train loss recon: 0.04771
Train loss dist: 4.57201
Train loss recon: 0.04066
Train loss dist: 4.59126
Train loss recon: 0.04438
Train loss dist: 5.71862
Train loss recon: 0.04799
Train loss dist: 4.926
Train loss recon: 0.03189
Train loss dist: 4.49375
Train loss recon: 0.03754
Train loss dist: 4.95665
Train loss recon: 0.03989
Train loss dist: 4.64207
Train loss recon: 0.03883
Train loss dist: 4.79211
Train loss recon: 0.03034
Train loss dist: 4.34837
Train loss recon: 0.03731
Train loss dist: 4.72959
Train loss recon: 0.03997
Train loss dist: 4.59091
Train loss recon: 0.04113
Train loss dist: 4.76118
Train loss recon: 0.0249
Train loss dist: 4.78832
Train loss recon: 0.04246
Train loss dist: 4.73066
Train loss recon: 0.04525
Train loss dist: 4.9849
Train loss recon: 0.04058
Train loss dist: 4.91371
Train loss recon: 0.03287
Train loss dist: 5.59119
Train loss recon: 0.03446
Train loss dist: 4.46752
Train loss recon: 0.03879
Train loss dist: 4.82013
Train loss recon: 0.04239
Train loss dist: 5.45407
Train loss recon: 0.02635
Train loss dist: 4.77129
Train loss recon: 0.04241
Train loss dist: 5.14577
Train loss recon: 0.03045
Train loss dist: 5.29046
Train loss recon: 0.04095
Train loss dist: 4.98065
Train loss recon: 0.03377
Train loss dist: 4.62637
Train loss recon: 0.04511
Train loss dist: 5.23424
Train loss recon: 0.03415
Train loss dist: 5.00579
Train loss recon: 0.03624
Train loss dist: 5.14518
Train loss recon: 0.04281
Train loss dist: 4.3338
Train loss recon: 0.03126
Train loss dist: 4.63608
Train loss recon: 0.02928
Train loss dist: 4.51689
run time: 28.55097770690918</code></pre>
</div>
</div>
<div id="cell-85" class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>sample_X <span class="op">=</span> sample(df,<span class="dv">0</span>,size<span class="op">=</span>(<span class="dv">1000</span>,),replace<span class="op">=</span><span class="va">False</span>,to_torch<span class="op">=</span><span class="va">True</span>,use_cuda<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>encoded_X <span class="op">=</span> gae.encode(sample_X).cpu().detach().numpy()</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(encoded_X[:,<span class="dv">0</span>],encoded_X[:,<span class="dv">1</span>],c<span class="op">=</span>sample_X[:,<span class="dv">2</span>].cpu())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-57-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This would appear as the best way of approximating the spherical distances within euclidean space…</p>
</section>
<section id="train-ode" class="level3">
<h3 class="anchored" data-anchor-id="train-ode">Train ODE</h3>
<div id="cell-88" class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>set_seeds(<span class="dv">10</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Directory where results are saved</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>exp_name <span class="op">=</span> <span class="st">'test'</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># density loss knn</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>use_density_loss <span class="op">=</span> <span class="va">False</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Weight of density (not percentage of total loss)</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>lambda_density <span class="op">=</span> <span class="dv">35</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="co"># For petal=LeakyReLU / dyngen=CELU</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> <span class="st">'LeakyReLU'</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> MMD_loss_to_uniform(use_cuda<span class="op">=</span>use_cuda)</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Can change but we never really do, mostly depends on the dataset.</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">16</span>]</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale of the noise in the trajectories. Either len(groups)*[float] or None. Should be None if using an adaptative ODE solver.</span></span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>sde_scales <span class="op">=</span> <span class="bu">len</span>(groups)<span class="op">*</span>[<span class="fl">0.1</span>] </span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> recon:    </span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    model_features <span class="op">=</span> gae_embedded_dim</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_model(</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    model_features, layers, </span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span>activation, scales<span class="op">=</span>sde_scales, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>) <span class="co"># Basically "batch size"</span></span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>sample_size<span class="op">=</span>(<span class="dv">60</span>, )</span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Training specification</span></span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>n_local_epochs <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>n_post_local_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the reverse trajectories to train</span></span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a>reverse_schema <span class="op">=</span> <span class="va">False</span></span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a><span class="co"># each reverse_n epoch</span></span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>reverse_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion_name = 'ot'</span></span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = config_criterion(criterion_name)</span></span>
<span id="cb67-34"><a href="#cb67-34" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters())</span>
<span id="cb67-35"><a href="#cb67-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-36"><a href="#cb67-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Bookkeeping variables</span></span>
<span id="cb67-37"><a href="#cb67-37" aria-hidden="true" tabindex="-1"></a>batch_losses <span class="op">=</span> []</span>
<span id="cb67-38"><a href="#cb67-38" aria-hidden="true" tabindex="-1"></a>globe_losses <span class="op">=</span> []</span>
<span id="cb67-39"><a href="#cb67-39" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> hold_one_out <span class="kw">and</span> hold_out <span class="kw">in</span> groups:</span>
<span id="cb67-40"><a href="#cb67-40" aria-hidden="true" tabindex="-1"></a>    local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups) <span class="cf">if</span> hold_out <span class="kw">not</span> <span class="kw">in</span> [t0, t1]}</span>
<span id="cb67-41"><a href="#cb67-41" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb67-42"><a href="#cb67-42" aria-hidden="true" tabindex="-1"></a>    local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups)}</span>
<span id="cb67-43"><a href="#cb67-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-44"><a href="#cb67-44" aria-hidden="true" tabindex="-1"></a><span class="co"># For creating output.</span></span>
<span id="cb67-45"><a href="#cb67-45" aria-hidden="true" tabindex="-1"></a>n_points <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb67-46"><a href="#cb67-46" aria-hidden="true" tabindex="-1"></a>n_trajectories <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb67-47"><a href="#cb67-47" aria-hidden="true" tabindex="-1"></a>n_bins <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb67-48"><a href="#cb67-48" aria-hidden="true" tabindex="-1"></a>opts <span class="op">=</span> {</span>
<span id="cb67-49"><a href="#cb67-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">'phate_dims'</span>: <span class="dv">3</span>,</span>
<span id="cb67-50"><a href="#cb67-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_cuda'</span>: use_cuda,</span>
<span id="cb67-51"><a href="#cb67-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">'model_features'</span>: model_features,</span>
<span id="cb67-52"><a href="#cb67-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">'exp_name'</span>: exp_name,</span>
<span id="cb67-53"><a href="#cb67-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">'groups'</span>: groups,</span>
<span id="cb67-54"><a href="#cb67-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sample_size'</span>: sample_size,</span>
<span id="cb67-55"><a href="#cb67-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_emb'</span>: use_emb,</span>
<span id="cb67-56"><a href="#cb67-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_local_epochs'</span>: n_local_epochs,</span>
<span id="cb67-57"><a href="#cb67-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_epochs'</span>: n_epochs,</span>
<span id="cb67-58"><a href="#cb67-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_post_local_epochs'</span>: n_post_local_epochs,</span>
<span id="cb67-59"><a href="#cb67-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">'criterion_name'</span>: <span class="st">'MMD'</span>,</span>
<span id="cb67-60"><a href="#cb67-60" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hold_one_out'</span>: hold_one_out,</span>
<span id="cb67-61"><a href="#cb67-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_density_loss'</span>: use_density_loss,</span>
<span id="cb67-62"><a href="#cb67-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_points'</span>: n_points,</span>
<span id="cb67-63"><a href="#cb67-63" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_trajectories'</span>: n_trajectories,</span>
<span id="cb67-64"><a href="#cb67-64" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_bins'</span>: n_bins,</span>
<span id="cb67-65"><a href="#cb67-65" aria-hidden="true" tabindex="-1"></a>    <span class="st">'autoencoder'</span>: autoencoder,</span>
<span id="cb67-66"><a href="#cb67-66" aria-hidden="true" tabindex="-1"></a>    <span class="st">'activation_ode'</span>: activation,</span>
<span id="cb67-67"><a href="#cb67-67" aria-hidden="true" tabindex="-1"></a>    <span class="st">'layer'</span>: layers,</span>
<span id="cb67-68"><a href="#cb67-68" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lambda_density'</span>:lambda_density,</span>
<span id="cb67-69"><a href="#cb67-69" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_gae'</span>: use_gae,</span>
<span id="cb67-70"><a href="#cb67-70" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sde_scales'</span>: sde_scales,</span>
<span id="cb67-71"><a href="#cb67-71" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hold_out'</span>:hold_out,</span>
<span id="cb67-72"><a href="#cb67-72" aria-hidden="true" tabindex="-1"></a>    <span class="st">'encoder_layers'</span>: encoder_layers,</span>
<span id="cb67-73"><a href="#cb67-73" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_epochs_emb'</span>: n_epochs_emb,</span>
<span id="cb67-74"><a href="#cb67-74" aria-hidden="true" tabindex="-1"></a>    <span class="st">'samples_size_emb'</span>: samples_size_emb,</span>
<span id="cb67-75"><a href="#cb67-75" aria-hidden="true" tabindex="-1"></a>    <span class="st">'recon'</span>: recon,</span>
<span id="cb67-76"><a href="#cb67-76" aria-hidden="true" tabindex="-1"></a>    <span class="st">'distance_type'</span>:distance_type,</span>
<span id="cb67-77"><a href="#cb67-77" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rbf_length_scale'</span>:rbf_length_scale,</span>
<span id="cb67-78"><a href="#cb67-78" aria-hidden="true" tabindex="-1"></a>    <span class="st">'reverse_schema'</span>: reverse_schema,</span>
<span id="cb67-79"><a href="#cb67-79" aria-hidden="true" tabindex="-1"></a>    <span class="st">'reverse_n'</span>: reverse_n</span>
<span id="cb67-80"><a href="#cb67-80" aria-hidden="true" tabindex="-1"></a>} </span>
<span id="cb67-81"><a href="#cb67-81" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb67-82"><a href="#cb67-82" aria-hidden="true" tabindex="-1"></a>local_losses, batch_losses, globe_losses <span class="op">=</span> training_regimen_neural_flattener(</span>
<span id="cb67-83"><a href="#cb67-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># local, global, local train structure</span></span>
<span id="cb67-84"><a href="#cb67-84" aria-hidden="true" tabindex="-1"></a>    n_local_epochs<span class="op">=</span>n_local_epochs,</span>
<span id="cb67-85"><a href="#cb67-85" aria-hidden="true" tabindex="-1"></a>    n_epochs<span class="op">=</span>n_epochs, </span>
<span id="cb67-86"><a href="#cb67-86" aria-hidden="true" tabindex="-1"></a>    n_post_local_epochs<span class="op">=</span>n_post_local_epochs,</span>
<span id="cb67-87"><a href="#cb67-87" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span>criterion,</span>
<span id="cb67-88"><a href="#cb67-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># where results are stored</span></span>
<span id="cb67-89"><a href="#cb67-89" aria-hidden="true" tabindex="-1"></a>    exp_dir<span class="op">=</span><span class="st">"../../results/"</span>, </span>
<span id="cb67-90"><a href="#cb67-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-91"><a href="#cb67-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">BEGIN</span><span class="co">: train params</span></span>
<span id="cb67-92"><a href="#cb67-92" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model, df<span class="op">=</span>df, groups<span class="op">=</span>groups, optimizer<span class="op">=</span>optimizer, </span>
<span id="cb67-93"><a href="#cb67-93" aria-hidden="true" tabindex="-1"></a>    use_cuda<span class="op">=</span>use_cuda,</span>
<span id="cb67-94"><a href="#cb67-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-95"><a href="#cb67-95" aria-hidden="true" tabindex="-1"></a>    hold_one_out<span class="op">=</span>hold_one_out, hold_out<span class="op">=</span>hold_out,</span>
<span id="cb67-96"><a href="#cb67-96" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-97"><a href="#cb67-97" aria-hidden="true" tabindex="-1"></a>    use_density_loss<span class="op">=</span>use_density_loss, </span>
<span id="cb67-98"><a href="#cb67-98" aria-hidden="true" tabindex="-1"></a>    lambda_density<span class="op">=</span>lambda_density,</span>
<span id="cb67-99"><a href="#cb67-99" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-100"><a href="#cb67-100" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span>autoencoder, use_emb<span class="op">=</span>use_emb, use_gae<span class="op">=</span>use_gae, </span>
<span id="cb67-101"><a href="#cb67-101" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-102"><a href="#cb67-102" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>sample_size, logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-103"><a href="#cb67-103" aria-hidden="true" tabindex="-1"></a>    reverse_schema<span class="op">=</span>reverse_schema, reverse_n<span class="op">=</span>reverse_n,</span>
<span id="cb67-104"><a href="#cb67-104" aria-hidden="true" tabindex="-1"></a>    use_penalty<span class="op">=</span><span class="va">True</span>, lambda_energy<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb67-105"><a href="#cb67-105" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="re">END</span><span class="co">: train params</span></span>
<span id="cb67-106"><a href="#cb67-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-107"><a href="#cb67-107" aria-hidden="true" tabindex="-1"></a>    plot_every<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-108"><a href="#cb67-108" aria-hidden="true" tabindex="-1"></a>    n_points<span class="op">=</span>n_points, n_trajectories<span class="op">=</span>n_trajectories, n_bins<span class="op">=</span>n_bins, </span>
<span id="cb67-109"><a href="#cb67-109" aria-hidden="true" tabindex="-1"></a>    <span class="co">#local_losses=local_losses, batch_losses=batch_losses, globe_losses=globe_losses</span></span>
<span id="cb67-110"><a href="#cb67-110" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb67-111"><a href="#cb67-111" aria-hidden="true" tabindex="-1"></a>run_time <span class="op">=</span> time.time() <span class="op">-</span> start_time <span class="op">+</span> run_time_geo <span class="cf">if</span> use_emb <span class="kw">or</span> use_gae <span class="cf">else</span> time.time() <span class="op">-</span> start_time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ToyModel(
  (func): ToyODE(
    (seq): Sequential(
      (0): Linear(in_features=5, out_features=16, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=16, out_features=32, bias=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Linear(in_features=32, out_features=16, bias=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=16, out_features=2, bias=True)
    )
  )
)
Train loss: 1.63637
Train loss: 1.45043
Train loss: 1.24333
Train loss: 0.88386
Train loss: 0.59404
Train loss: 0.20402
Train loss: 0.08042
Train loss: 0.07429
Train loss: 0.06613
Train loss: 0.06634
Train loss: 0.05296
Train loss: 0.06773
Train loss: 0.05758
Train loss: 0.07926
Train loss: 0.0679
Train loss: 0.06263
Train loss: 0.05523
Train loss: 0.05386
Train loss: 0.06468
Train loss: 0.0544
Train loss: 0.05573
Train loss: 0.05195
Train loss: 0.05308
Train loss: 0.06659
Train loss: 0.04848
Train loss: 0.04667
Train loss: 0.06537
Train loss: 0.0493
Train loss: 0.07418
Train loss: 0.05231
Train loss: 0.05318
Train loss: 0.05452
Train loss: 0.06104
Train loss: 0.05249
Train loss: 0.06916
Train loss: 0.04959
Train loss: 0.06246
Train loss: 0.05964
Train loss: 0.04859
Train loss: 0.06186
Train loss: 0.05649
Train loss: 0.05605
Train loss: 0.05349
Train loss: 0.066
Train loss: 0.0448
Train loss: 0.04979
Train loss: 0.04919
Train loss: 0.05362
Train loss: 0.05508
Train loss: 0.06019</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d907a5dd233240b38ece635a2720a973","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"45ff163fb32f4248811c8bc9c514b1fe","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"10818d55460643e99c794fdb0adfcac4","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0416469950334b94b5a4c337cc773438","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b63461897ae548728632eeb42c102473","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"15cd531b61f44051abcdc680ec2f19e0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"1fd5c7c22ece4a84a9377568cca1ba32","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"aff353d6777a466f9af168d1cd75c1f0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b1c5e02c302a4dacac563761b696848a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ddc0d0037c6546d98ff707beaf4b08c7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f049b32351d5438bbde3cc903fe647e2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9cb1948e9934dd3aaa02266c4ea057d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0e5b25478135494c9d7dee952923d0d9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"694099d5d9fd4cb59b6a9950d2878534","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4c6e5e240b7b4ef3af5d8e8195324498","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"aa80f66a3e9f4805ad16d4429065afca","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"45210e84c1fa41c8ab734ecd76501fe0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"af21b3a695c0402c964043d8ff5493c0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"129a9642cd4c4cf698a7c0793d234f82","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"992a7afb45ae4ac4b850937115f4166d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"1beb6f68e62642cfba9151ff3e7c9e39","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2a24fdb2c75c4b79a0344c8e98d268ed","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b7753c8d65d94110b720b86ae7b34e99","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b1122abba9d84d6e99a56ed0c1779e5d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9409ac0d6b1c4ef9b31b89a874ac1cf9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dd221232aafa49b6be39d66618e8da30","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"03c0612de5c64d2d9fec48fdee2e663a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b49b0e304ee843a296fedefbecf18b3a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"593a9b9a70ee4c31866cf48e242d137a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"46ae4872ee2e4ca9b731de72af0384fd","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4dda691e54884f47871d8203b4426002","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3503aa6febfe423cb205f5e612f08771","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3747fdcfbd474bea90ea8e65fe2f285e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"09baa16f73244be6a86a4083330c525a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3bae3facf2d44b4a967b7c8cf076608b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"05b4eef9181a4993a36a93f5f23d7837","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"09e9cf45d18f4f92a79849836266628e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"35667f6cc5a849899ab70613235662f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c94f586ce1c84709b7c272eae26c0e0d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"33b4f3c9bfdd42a19c2989d8214b6932","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"64f13911b4ea4dbda45ae1e4122c7b74","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7100dab5ad0047a3a750e217b4229164","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4ea66cb319ae426c897a7960c3c3f6fa","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f432c5ca1eb040da8fa2f18ac60306fc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9dd9a2937da149f0928e28f6171da273","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5e209da80d774e198a621dc2c7235e56","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f7ea61a424f34888acc5636d861e523b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b374f5b9d61040089f4dbdf3078da0f8","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b751971a941042e883190211b8e1dc7b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6a70fc9efb94f76b2bfa4169face39e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"796735206232484ab2a94a79b0445b17","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dda0d0fb35824a3f9a7b6718ddcaa7c7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2bbb5d1af80d41f1b44a4b0329cc6432","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
<section id="all-in-one-trainer" class="level2">
<h2 class="anchored" data-anchor-id="all-in-one-trainer">All in One Trainer</h2>
<p>Let’s distill the above into an easy-to-call function:</p>
<p>::: {#cell-90 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> heatgeo.embedding <span class="im">import</span> HeatGeo</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusion_curvature.radial_flattening_ae <span class="im">import</span> radially_flatten_with_ae</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flatten_with_mioflow(</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    X, <span class="co"># Pointcloud in question</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    intrinsic_dim, <span class="co"># X's manifold dimension</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="va">None</span>, <span class="co"># Geodesic Distance matrix of X; if not supplied will compute with HeatGeo</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    central_idx <span class="op">=</span> <span class="dv">0</span>, <span class="co"># Will flatten with respect to this point</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    return_trajectories <span class="op">=</span> <span class="va">False</span>, <span class="co"># by default only returns flattened data; can optionally also return each timestep and the trajectories between them</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    use_cuda <span class="op">=</span> torch.cuda.is_available(),</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    trained_autoencoder <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Put pointcloud into dictionary</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> {<span class="st">'samples'</span>: [<span class="dv">0</span>] <span class="op">*</span> <span class="bu">len</span>(X)} <span class="co"># all at time 0</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]):</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>        data[<span class="ss">f'd</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> X[:, i]</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> D <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>        emb_op <span class="op">=</span> HeatGeo(knn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>        emb <span class="op">=</span> emb_op.fit_transform(X)</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> emb_op.dist</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the dataframe</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'radial_dists'</span>] <span class="op">=</span> D[central_idx]</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#### Train Autoencoder #####</span></span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> trained_autoencoder <span class="kw">is</span> <span class="va">None</span>: </span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a>        flattened_X, trained_autoencoder <span class="op">=</span> radially_flatten_with_ae(X, D, intrinsic_dim <span class="op">=</span> intrinsic_dim)</span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_cuda: trained_autoencoder <span class="op">=</span> trained_autoencoder.cuda()</span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a>    n_epochs_emb <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a>    samples_size_emb <span class="op">=</span> (<span class="dv">30</span>, )</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a>    distance_type <span class="op">=</span> <span class="st">'gaussian'</span></span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a>    rbf_length_scale<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>    dist <span class="op">=</span> setup_distance(distance_type, rbf_length_scale<span class="op">=</span>rbf_length_scale)</span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">##### Train MIOFLOW #####</span></span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a>    set_seeds(<span class="dv">10</span>)</span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Directory where results are saved</span></span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a>    exp_name <span class="op">=</span> <span class="st">'test'</span></span>
<span id="cb69-42"><a href="#cb69-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># density loss knn</span></span>
<span id="cb69-43"><a href="#cb69-43" aria-hidden="true" tabindex="-1"></a>    use_density_loss <span class="op">=</span> <span class="va">False</span></span>
<span id="cb69-44"><a href="#cb69-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Weight of density (not percentage of total loss)</span></span>
<span id="cb69-45"><a href="#cb69-45" aria-hidden="true" tabindex="-1"></a>    lambda_density <span class="op">=</span> <span class="dv">35</span></span>
<span id="cb69-46"><a href="#cb69-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For petal=LeakyReLU / dyngen=CELU</span></span>
<span id="cb69-47"><a href="#cb69-47" aria-hidden="true" tabindex="-1"></a>    activation <span class="op">=</span> <span class="st">'LeakyReLU'</span></span>
<span id="cb69-48"><a href="#cb69-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-49"><a href="#cb69-49" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> MMD_loss_to_uniform()</span>
<span id="cb69-50"><a href="#cb69-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-51"><a href="#cb69-51" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb69-52"><a href="#cb69-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Can change but we never really do, mostly depends on the dataset.</span></span>
<span id="cb69-53"><a href="#cb69-53" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">16</span>]</span>
<span id="cb69-54"><a href="#cb69-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale of the noise in the trajectories. Either len(groups)*[float] or None. Should be None if using an adaptative ODE solver.</span></span>
<span id="cb69-55"><a href="#cb69-55" aria-hidden="true" tabindex="-1"></a>    sde_scales <span class="op">=</span> <span class="dv">1</span><span class="op">*</span>[<span class="fl">0.1</span>] </span>
<span id="cb69-56"><a href="#cb69-56" aria-hidden="true" tabindex="-1"></a>    model_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb69-57"><a href="#cb69-57" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_model(</span>
<span id="cb69-58"><a href="#cb69-58" aria-hidden="true" tabindex="-1"></a>        model_features, layers, </span>
<span id="cb69-59"><a href="#cb69-59" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span>activation, scales<span class="op">=</span>sde_scales, use_cuda<span class="op">=</span>use_cuda</span>
<span id="cb69-60"><a href="#cb69-60" aria-hidden="true" tabindex="-1"></a>    ) </span>
<span id="cb69-61"><a href="#cb69-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basically "batch size"</span></span>
<span id="cb69-62"><a href="#cb69-62" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>(<span class="dv">60</span>, )</span>
<span id="cb69-63"><a href="#cb69-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training specification</span></span>
<span id="cb69-64"><a href="#cb69-64" aria-hidden="true" tabindex="-1"></a>    n_local_epochs <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb69-65"><a href="#cb69-65" aria-hidden="true" tabindex="-1"></a>    n_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb69-66"><a href="#cb69-66" aria-hidden="true" tabindex="-1"></a>    n_post_local_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb69-67"><a href="#cb69-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Using the reverse trajectories to train</span></span>
<span id="cb69-68"><a href="#cb69-68" aria-hidden="true" tabindex="-1"></a>    reverse_schema <span class="op">=</span> <span class="va">False</span></span>
<span id="cb69-69"><a href="#cb69-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># each reverse_n epoch</span></span>
<span id="cb69-70"><a href="#cb69-70" aria-hidden="true" tabindex="-1"></a>    reverse_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb69-71"><a href="#cb69-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># criterion_name = 'ot'</span></span>
<span id="cb69-72"><a href="#cb69-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># criterion = config_criterion(criterion_name)</span></span>
<span id="cb69-73"><a href="#cb69-73" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters())</span>
<span id="cb69-74"><a href="#cb69-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-75"><a href="#cb69-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bookkeeping variables</span></span>
<span id="cb69-76"><a href="#cb69-76" aria-hidden="true" tabindex="-1"></a>    batch_losses <span class="op">=</span> []</span>
<span id="cb69-77"><a href="#cb69-77" aria-hidden="true" tabindex="-1"></a>    globe_losses <span class="op">=</span> []</span>
<span id="cb69-78"><a href="#cb69-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if hold_one_out and hold_out in groups:</span></span>
<span id="cb69-79"><a href="#cb69-79" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     local_losses = {f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups) if hold_out not in [t0, t1]}</span></span>
<span id="cb69-80"><a href="#cb69-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># else:</span></span>
<span id="cb69-81"><a href="#cb69-81" aria-hidden="true" tabindex="-1"></a>    local_losses <span class="op">=</span> {<span class="ss">f'</span><span class="sc">{</span>t0<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>t1<span class="sc">}</span><span class="ss">'</span>:[] <span class="cf">for</span> (t0, t1) <span class="kw">in</span> generate_steps(groups)}</span>
<span id="cb69-82"><a href="#cb69-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-83"><a href="#cb69-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For creating output.</span></span>
<span id="cb69-84"><a href="#cb69-84" aria-hidden="true" tabindex="-1"></a>    n_points <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb69-85"><a href="#cb69-85" aria-hidden="true" tabindex="-1"></a>    n_trajectories <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb69-86"><a href="#cb69-86" aria-hidden="true" tabindex="-1"></a>    n_bins <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb69-87"><a href="#cb69-87" aria-hidden="true" tabindex="-1"></a>    opts <span class="op">=</span> {</span>
<span id="cb69-88"><a href="#cb69-88" aria-hidden="true" tabindex="-1"></a>        <span class="st">'phate_dims'</span>: <span class="dv">3</span>,</span>
<span id="cb69-89"><a href="#cb69-89" aria-hidden="true" tabindex="-1"></a>        <span class="st">'use_cuda'</span>: use_cuda,</span>
<span id="cb69-90"><a href="#cb69-90" aria-hidden="true" tabindex="-1"></a>        <span class="st">'model_features'</span>: model_features,</span>
<span id="cb69-91"><a href="#cb69-91" aria-hidden="true" tabindex="-1"></a>        <span class="st">'exp_name'</span>: exp_name,</span>
<span id="cb69-92"><a href="#cb69-92" aria-hidden="true" tabindex="-1"></a>        <span class="st">'groups'</span>: groups,</span>
<span id="cb69-93"><a href="#cb69-93" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sample_size'</span>: sample_size,</span>
<span id="cb69-94"><a href="#cb69-94" aria-hidden="true" tabindex="-1"></a>        <span class="st">'use_emb'</span>: <span class="va">True</span>,</span>
<span id="cb69-95"><a href="#cb69-95" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_local_epochs'</span>: n_local_epochs,</span>
<span id="cb69-96"><a href="#cb69-96" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_epochs'</span>: n_epochs,</span>
<span id="cb69-97"><a href="#cb69-97" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_post_local_epochs'</span>: n_post_local_epochs,</span>
<span id="cb69-98"><a href="#cb69-98" aria-hidden="true" tabindex="-1"></a>        <span class="st">'criterion_name'</span>: <span class="st">'MMD'</span>,</span>
<span id="cb69-99"><a href="#cb69-99" aria-hidden="true" tabindex="-1"></a>        <span class="st">'hold_one_out'</span>: <span class="va">False</span>,</span>
<span id="cb69-100"><a href="#cb69-100" aria-hidden="true" tabindex="-1"></a>        <span class="st">'use_density_loss'</span>: use_density_loss,</span>
<span id="cb69-101"><a href="#cb69-101" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_points'</span>: n_points,</span>
<span id="cb69-102"><a href="#cb69-102" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_trajectories'</span>: n_trajectories,</span>
<span id="cb69-103"><a href="#cb69-103" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_bins'</span>: n_bins,</span>
<span id="cb69-104"><a href="#cb69-104" aria-hidden="true" tabindex="-1"></a>        <span class="st">'autoencoder'</span>: trained_autoencoder,</span>
<span id="cb69-105"><a href="#cb69-105" aria-hidden="true" tabindex="-1"></a>        <span class="st">'activation_ode'</span>: activation,</span>
<span id="cb69-106"><a href="#cb69-106" aria-hidden="true" tabindex="-1"></a>        <span class="st">'layer'</span>: layers,</span>
<span id="cb69-107"><a href="#cb69-107" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lambda_density'</span>:lambda_density,</span>
<span id="cb69-108"><a href="#cb69-108" aria-hidden="true" tabindex="-1"></a>        <span class="st">'use_gae'</span>: <span class="va">False</span>,</span>
<span id="cb69-109"><a href="#cb69-109" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sde_scales'</span>: sde_scales,</span>
<span id="cb69-110"><a href="#cb69-110" aria-hidden="true" tabindex="-1"></a>        <span class="st">'hold_out'</span>:<span class="dv">10</span>,</span>
<span id="cb69-111"><a href="#cb69-111" aria-hidden="true" tabindex="-1"></a>        <span class="st">'encoder_layers'</span>: [X.shape[<span class="dv">1</span>],<span class="dv">5</span>,<span class="dv">5</span>,intrinsic_dim],</span>
<span id="cb69-112"><a href="#cb69-112" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_epochs_emb'</span>: n_epochs_emb,</span>
<span id="cb69-113"><a href="#cb69-113" aria-hidden="true" tabindex="-1"></a>        <span class="st">'samples_size_emb'</span>: samples_size_emb,</span>
<span id="cb69-114"><a href="#cb69-114" aria-hidden="true" tabindex="-1"></a>        <span class="st">'recon'</span>: <span class="va">False</span>,</span>
<span id="cb69-115"><a href="#cb69-115" aria-hidden="true" tabindex="-1"></a>        <span class="st">'distance_type'</span>:distance_type,</span>
<span id="cb69-116"><a href="#cb69-116" aria-hidden="true" tabindex="-1"></a>        <span class="st">'rbf_length_scale'</span>:rbf_length_scale,</span>
<span id="cb69-117"><a href="#cb69-117" aria-hidden="true" tabindex="-1"></a>        <span class="st">'reverse_schema'</span>: reverse_schema,</span>
<span id="cb69-118"><a href="#cb69-118" aria-hidden="true" tabindex="-1"></a>        <span class="st">'reverse_n'</span>: reverse_n</span>
<span id="cb69-119"><a href="#cb69-119" aria-hidden="true" tabindex="-1"></a>    } </span>
<span id="cb69-120"><a href="#cb69-120" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb69-121"><a href="#cb69-121" aria-hidden="true" tabindex="-1"></a>    local_losses, batch_losses, globe_losses <span class="op">=</span> training_regimen_neural_flattener(</span>
<span id="cb69-122"><a href="#cb69-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># local, global, local train structure</span></span>
<span id="cb69-123"><a href="#cb69-123" aria-hidden="true" tabindex="-1"></a>        n_local_epochs<span class="op">=</span>n_local_epochs,</span>
<span id="cb69-124"><a href="#cb69-124" aria-hidden="true" tabindex="-1"></a>        n_epochs<span class="op">=</span>n_epochs, </span>
<span id="cb69-125"><a href="#cb69-125" aria-hidden="true" tabindex="-1"></a>        n_post_local_epochs<span class="op">=</span>n_post_local_epochs,</span>
<span id="cb69-126"><a href="#cb69-126" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb69-127"><a href="#cb69-127" aria-hidden="true" tabindex="-1"></a>        <span class="co"># where results are stored</span></span>
<span id="cb69-128"><a href="#cb69-128" aria-hidden="true" tabindex="-1"></a>        exp_dir<span class="op">=</span><span class="st">"../../results/"</span>, </span>
<span id="cb69-129"><a href="#cb69-129" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-130"><a href="#cb69-130" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="re">BEGIN</span><span class="co">: train params</span></span>
<span id="cb69-131"><a href="#cb69-131" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model, df<span class="op">=</span>df, groups<span class="op">=</span>groups, optimizer<span class="op">=</span>optimizer, </span>
<span id="cb69-132"><a href="#cb69-132" aria-hidden="true" tabindex="-1"></a>        use_cuda<span class="op">=</span>use_cuda,</span>
<span id="cb69-133"><a href="#cb69-133" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-134"><a href="#cb69-134" aria-hidden="true" tabindex="-1"></a>        hold_one_out<span class="op">=</span><span class="va">False</span>, hold_out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb69-135"><a href="#cb69-135" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-136"><a href="#cb69-136" aria-hidden="true" tabindex="-1"></a>        use_density_loss<span class="op">=</span>use_density_loss, </span>
<span id="cb69-137"><a href="#cb69-137" aria-hidden="true" tabindex="-1"></a>        lambda_density<span class="op">=</span>lambda_density,</span>
<span id="cb69-138"><a href="#cb69-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-139"><a href="#cb69-139" aria-hidden="true" tabindex="-1"></a>        autoencoder<span class="op">=</span>trained_autoencoder, use_emb<span class="op">=</span><span class="va">False</span>, use_gae<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb69-140"><a href="#cb69-140" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-141"><a href="#cb69-141" aria-hidden="true" tabindex="-1"></a>        sample_size<span class="op">=</span>sample_size, logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb69-142"><a href="#cb69-142" aria-hidden="true" tabindex="-1"></a>        reverse_schema<span class="op">=</span>reverse_schema, reverse_n<span class="op">=</span>reverse_n,</span>
<span id="cb69-143"><a href="#cb69-143" aria-hidden="true" tabindex="-1"></a>        use_penalty<span class="op">=</span><span class="va">True</span>, lambda_energy<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb69-144"><a href="#cb69-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="re">END</span><span class="co">: train params</span></span>
<span id="cb69-145"><a href="#cb69-145" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-146"><a href="#cb69-146" aria-hidden="true" tabindex="-1"></a>        plot_every<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb69-147"><a href="#cb69-147" aria-hidden="true" tabindex="-1"></a>        n_points<span class="op">=</span>n_points, n_trajectories<span class="op">=</span>n_trajectories, n_bins<span class="op">=</span>n_bins, </span>
<span id="cb69-148"><a href="#cb69-148" aria-hidden="true" tabindex="-1"></a>        <span class="co">#local_losses=local_losses, batch_losses=batch_losses, globe_losses=globe_losses</span></span>
<span id="cb69-149"><a href="#cb69-149" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb69-150"><a href="#cb69-150" aria-hidden="true" tabindex="-1"></a>    run_time <span class="op">=</span> time.time() <span class="op">-</span> start_time <span class="op">+</span> run_time_geo <span class="cf">if</span> use_emb <span class="kw">or</span> use_gae <span class="cf">else</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb69-151"><a href="#cb69-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-152"><a href="#cb69-152" aria-hidden="true" tabindex="-1"></a>    generated, trajectories <span class="op">=</span> generate_plot_data_flat(</span>
<span id="cb69-153"><a href="#cb69-153" aria-hidden="true" tabindex="-1"></a>        model, df, <span class="dv">1000</span>, n_trajectories, <span class="dv">100</span>, use_cuda<span class="op">=</span>use_cuda, samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb69-154"><a href="#cb69-154" aria-hidden="true" tabindex="-1"></a>        autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb69-155"><a href="#cb69-155" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb69-156"><a href="#cb69-156" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> return_trajectories:</span>
<span id="cb69-157"><a href="#cb69-157" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> generated, trajectories, model</span>
<span id="cb69-158"><a href="#cb69-158" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb69-159"><a href="#cb69-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> generated[<span class="op">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
<p>What does it look like?</p>
<p>In action:</p>
<div id="cell-93" class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusion_curvature.radial_flattening_ae <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-94" class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>emb_op <span class="op">=</span> HeatGeo(knn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> emb_op.fit_transform(X_cap_of_sphere)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>D_cap_of_sphere <span class="op">=</span> emb_op.dist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/piriac/mambaforge/envs/zetteldev-diffcurv/lib/python3.11/site-packages/graphtools/base.py:165: RuntimeWarning:

Cannot perform PCA to 40 dimensions on data with min(n_samples, n_features) = 3
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Using s_gd2 for MDS. None</code></pre>
</div>
</div>
<div id="cell-95" class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>flattened_X, trained_ae <span class="op">=</span> radially_flatten_with_ae(X_cap_of_sphere, D_cap_of_sphere, intrinsic_dim <span class="op">=</span> <span class="dv">2</span>, return_model<span class="op">=</span><span class="va">True</span>, max_epochs<span class="op">=</span><span class="dv">25</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/piriac/mambaforge/envs/zetteldev-diffcurv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
/home/piriac/mambaforge/envs/zetteldev-diffcurv/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/piriac/mambaforge/envs/zetteldev-diffcurv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
/home/piriac/mambaforge/envs/zetteldev-diffcurv/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning:

Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.

`Trainer.fit` stopped: `max_epochs=25` reached.</code></pre>
</div>
</div>
<div id="cell-96" class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(flattened_X[:,<span class="dv">0</span>], flattened_X[:,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-63-output-1.png" class="img-fluid"></p>
</div>
</div>
<div id="cell-97" class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>flattened_X, trajs, model <span class="op">=</span> flatten_with_mioflow(</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    X_cap_of_sphere, <span class="dv">2</span>, return_trajectories<span class="op">=</span><span class="va">True</span>, D<span class="op">=</span>D_cap_of_sphere, trained_autoencoder <span class="op">=</span> trained_ae,</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ToyModel(
  (func): ToyODE(
    (seq): Sequential(
      (0): Linear(in_features=6, out_features=16, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=16, out_features=32, bias=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Linear(in_features=32, out_features=16, bias=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=16, out_features=3, bias=True)
    )
  )
)
samples shape torch.Size([59, 4])</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"44328988124a4661b5cbcc0696daf14e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4432c4985e034e39a2f552ae42cafc5a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-error">
<pre><code>RuntimeError: The size of tensor a (59) must match the size of tensor b (4) at non-singleton dimension 0</code></pre>
</div>
</div>
<div id="cell-98" class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>generated, trajectories <span class="op">=</span> generate_plot_data_flat(</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    model, df, <span class="dv">1000</span>, n_trajectories, <span class="dv">100</span>, use_cuda<span class="op">=</span>use_cuda, samples_key<span class="op">=</span><span class="st">'samples'</span>, logger<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    autoencoder<span class="op">=</span>autoencoder, recon<span class="op">=</span><span class="va">False</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-99" class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>generated.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(20, 1000, 2)</code></pre>
</div>
</div>
<div id="cell-100" class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> imageio</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the frames</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>frames <span class="op">=</span> []</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over each array in the generated variable</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> array <span class="kw">in</span> generated:</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a scatter plot of the array</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>    plt.scatter(array[:, <span class="dv">0</span>], array[:, <span class="dv">1</span>])</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="bu">min</span>(array[:, <span class="dv">0</span>]), <span class="bu">max</span>(array[:, <span class="dv">0</span>]))</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="bu">min</span>(array[:, <span class="dv">1</span>]), <span class="bu">max</span>(array[:, <span class="dv">1</span>]))</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)  <span class="co"># Turn off the axis</span></span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>    plt.gca().set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)  <span class="co"># Set aspect ratio to equal</span></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()  <span class="co"># Adjust layout</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the scatter plot as an image</span></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="st">'frame.png'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read the saved image</span></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> imageio.imread(<span class="st">'frame.png'</span>)</span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append the image to the frames list</span></span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a>    frames.append(image)</span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the frames as a GIF</span></span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a>imageio.mimsave(<span class="st">'generated.gif'</span>, frames, duration<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_315027/2159275057.py:20: DeprecationWarning:

Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.
</code></pre>
</div>
</div>
<div id="cell-101" class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">19</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(generated[i][:,<span class="dv">0</span>],generated[i][:,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-68-output-1.png" class="img-fluid"></p>
</div>
</div>
<div id="cell-102" class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>plot_comparison_flat(</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    df, generated, trajectories,</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>    palette <span class="op">=</span> <span class="st">'viridis'</span>, df_time_key<span class="op">=</span><span class="st">'samples'</span>,</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'2d_comparision.png'</span>,</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'d1'</span>, y<span class="op">=</span><span class="st">'d2'</span>, z<span class="op">=</span><span class="st">'d3'</span>, is_3d<span class="op">=</span><span class="va">False</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="MIOFlow for Neural Flattening_files/figure-html/cell-69-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<div id="cell-104" class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>plot_losses(</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    globe_losses, </span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>    save<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>IMGS_DIR, <span class="bu">file</span><span class="op">=</span><span class="st">'losses.png'</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>PermissionError: [Errno 13] Permission denied: '/results'</code></pre>
</div>
</div>
<div id="cell-105" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sync changes to the library</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Javascript</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>display(Javascript(<span class="st">'IPython.notebook.save_checkpoint();'</span>))</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>time.sleep(<span class="dv">2</span>)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pixi run nbsync</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/javascript">
IPython.notebook.save_checkpoint();
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> WARN pixi::project::manifest: BETA feature `[pypi-dependencies]` enabled!

Please report any and all issues here:

    https://github.com/prefix-dev/pixi.

Turn this warning off by setting the environment variable `PIXI_BETA_WARNING_OFF` to `true`.

✨ Pixi task: nbdev_export                                           
/home/piriac/Pumberton/Workshop/21-SUMRY-Curvature/diffusion-curvature/.pixi/env/lib/python3.11/site-packages/nbdev/export.py:54: UserWarning: Notebook '/home/piriac/Pumberton/Workshop/21-SUMRY-Curvature/diffusion-curvature/nbs/experiments/Curvature from Flattening.ipynb' uses `#|export` without `#|default_exp` cell.
Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.
See https://nbdev.fast.ai/getting_started.html for more information.
  warn(f"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\n"</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        console.log("RESIZE");
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>