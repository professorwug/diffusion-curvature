{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp pyg.diffusion\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from fastcore.all import *\n",
    "from nbdev.showdoc import *\n",
    "# Configure environment\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false' # Tells Jax not to hog all of the memory to this process.\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## project specifics\n",
    "import diffusion_curvature\n",
    "\n",
    "from diffusion_curvature.graphs import *\n",
    "from diffusion_curvature.datasets import *\n",
    "# from diffusion_curvature.core import *\n",
    "from diffusion_curvature.utils import *\n",
    "# from diffusion_curvature.comparison_space import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a PyG Diffusion Layer\n",
    "> Differentiable, Sparse, PyTorch Native Graph Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Adapted from Alex Tong and Kincaid MacDonald's implementation in LEGSNet\n",
    "# [ML4Molecules_2020_paper_63.pdf](https://ml4molecules.github.io/papers2020/ML4Molecules_2020_paper_63.pdf)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "class LazyLayer(torch.nn.Module):\n",
    "    \"\"\" Currently a single elementwise multiplication with one laziness parameter per\n",
    "    channel. this is run through a softmax so that this is a real laziness parameter\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.Tensor(2, n))\n",
    "\n",
    "    def forward(self, x, propogated):\n",
    "        inp = torch.stack((x, propogated), dim=1)\n",
    "        s_weights = torch.nn.functional.softmax(self.weights, dim=0)\n",
    "        return torch.sum(inp * s_weights, dim=-2)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.ones_(self.weights)\n",
    "\n",
    "\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None,\n",
    "             add_self_loops=False, dtype=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    if add_self_loops:\n",
    "        edge_index, tmp_edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, 1, num_nodes)\n",
    "        assert tmp_edge_weight is not None\n",
    "        edge_weight = tmp_edge_weight\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow_(-1)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "    return edge_index, deg_inv_sqrt[row] * edge_weight\n",
    "\n",
    "class Diffuse(MessagePassing):\n",
    "    \"\"\" Implements low pass walk with optional weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, trainable_laziness=False, fixed_weights=True\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\", node_dim=-3)  # \"Add\" aggregation.\n",
    "        assert in_channels == out_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.fixed_weights = fixed_weights\n",
    "        if trainable_laziness:\n",
    "            self.lazy_layer = LazyLayer(in_channels)\n",
    "        if not self.fixed_weights:\n",
    "            self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        # turn off this step for simplicity\n",
    "        if not self.fixed_weights:\n",
    "            x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization\n",
    "        edge_index, edge_weight = gcn_norm(edge_index, edge_weight, x.size(self.node_dim), dtype=x.dtype)\n",
    "\n",
    "        # Step 4-6: Start propagating messages.\n",
    "        propogated = self.propagate(\n",
    "            edge_index, edge_weight=edge_weight, size=None, x=x,\n",
    "        )\n",
    "\n",
    "        if not self.trainable_laziness:\n",
    "            return 0.5 * (x + propogated)\n",
    "        return self.lazy_layer(x, propogated)\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # Step 4: Normalize node features.\n",
    "        return edge_weight.view(-1, 1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # Step 6: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync changes to the library\n",
    "from IPython.display import display, Javascript\n",
    "import time\n",
    "display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "time.sleep(2)\n",
    "!pixi run nbsync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
