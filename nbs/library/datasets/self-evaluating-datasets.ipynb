{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0264fcef-53bb-424e-a794-1048aa2eb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp self_evaluating_datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643ecbd-a522-4e2e-9b57-d430d9d3a9c1",
   "metadata": {},
   "source": [
    "# Self Evaluating Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839626f4-d294-4295-92f1-d2c51f55e25b",
   "metadata": {},
   "source": [
    "This is a handy wrapper that allows you to supply a list of data objects and metrics, which you can then turn into an iterable and evaluate through a variety of methods, while storing the results in the iterable dataset class. The below is meant to be used as a parent class for actual self-evaluating datasets, which supply the data list and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fb551f5-6651-4c46-b2ac-735ee7545285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.all import *\n",
    "import inspect\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def metric(func):\n",
    "    setattr(func, 'tag', 'metric')\n",
    "    return func\n",
    "\n",
    "class Wrapper:\n",
    "    def __init__(self, obj, **kwargs):\n",
    "        self.obj = obj\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "class SelfEvaluatingDataset():\n",
    "    def __init__(self,\n",
    "                 datalist:List, # list of objects to be evaluated in the dataset. Usually includes multiple examples, e.g. a torus, sphere, saddle; multiple images, multiple validation datasets.\n",
    "                 names:List, # names of the datasets in datalist.\n",
    "                 result_names:List, # quantities to be computed (e.g. curvature, predictions). Usually just one per dataset.\n",
    "                ):\n",
    "        store_attr()\n",
    "\n",
    "        self.DS = [ # list of datasets\n",
    "            Wrapper(obj, results={rn:{} for rn in result_names}, name=name) for obj, name in zip(datalist, names)\n",
    "        ]\n",
    "        self.idx = -1\n",
    "        for i in range(self.__len__()):\n",
    "            # aggregate ground truth values\n",
    "            for rn in self.result_names:\n",
    "                self._store_truth(rn, i)\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.DS)\n",
    "    \n",
    "    def preprocess(self, unprocessed_data_object):\n",
    "        return unprocessed_data_object # override\n",
    "    \n",
    "    def get_item(self, idx):\n",
    "        return self.DS[idx]\n",
    "\n",
    "    def __next__(self):\n",
    "        self.idx += 1\n",
    "        if self.idx >= self.__len__():\n",
    "            raise StopIteration\n",
    "        result = self.get_item(self.idx)\n",
    "        return result\n",
    "\n",
    "    def update(self,\n",
    "               result,\n",
    "               idx = None,\n",
    "               result_name = 'default',\n",
    "               method_name='computed',\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Store the result of the curvature computation by passing the computed curvature of the center (first) point.\n",
    "        \"\"\"\n",
    "        if idx is None: idx = self.idx\n",
    "        if result_name == 'default': \n",
    "            if len(self.result_names) == 1:\n",
    "                result_name = self.result_names[0]\n",
    "        self.DS[idx].results[result_name][method_name] = result\n",
    "\n",
    "    def get_truth(self, result_name, idx):\n",
    "        \"\"\"Compute the ground truth for each of your targets, and assign to a method. Usually this involves accessing some attribute of the input data and calling the update function\"\"\"\n",
    "        truth = None\n",
    "        return truth\n",
    "\n",
    "    \n",
    "    def _store_truth(self, result_name, idx):\n",
    "        truth = self.get_truth(result_name, idx)\n",
    "        self.update(\n",
    "            truth, idx, method_name = \"ground truth\", result_name=result_name\n",
    "        )\n",
    "\n",
    "\n",
    "    def compute_metrics(self, filter = None):\n",
    "        self._aggregate_labels()\n",
    "        metrics = self._get_metrics()\n",
    "        self.metric_tables = {rn : {} for rn in self.result_names}\n",
    "        for rn in self.result_names:\n",
    "            for metric in metrics:\n",
    "                self.metric_tables[rn][metric.__name__] = {}\n",
    "                for method_name in self.method_names:\n",
    "                    self.metric_tables[rn][metric.__name__][method_name] = self.compute(metric=metric, method_name=method_name, result_name=rn, filter = None)\n",
    "            self.metric_tables[rn] = pd.DataFrame(self.metric_tables[rn])\n",
    "            \n",
    "    def compute(self, metric, result_name, method_name, filter=None):\n",
    "        # Overwrite this class with your logic. It implements the computation of a single metric for a single method\n",
    "        return metric(self.labels[result_name][method_name], self.labels[result_name]['ground truth'])\n",
    "    \n",
    "\n",
    "    def _aggregate_labels(self):\n",
    "        # returns a dictionary whose keys are method names, paired with a list of each of the results given by the metrics.\n",
    "        # Just a more convenient data format for comparing method outputs.\n",
    "        self.method_names = list(self.DS[0].results[self.result_names[0]].keys())\n",
    "        self.labels = {}\n",
    "        for rn in self.result_names:\n",
    "            self.labels[rn] = {}\n",
    "            for m in self.method_names:\n",
    "                self.labels[rn][m] = [self.DS[i].results[rn][m] for i in range(self.__len__())]\n",
    "\n",
    "\n",
    "    def plot(self, title = None):\n",
    "        if title is None: title = f\"In dimension {self.dimension}\"\n",
    "        # for each computed method on this dataset, we plot the histogram of saddles vs spheres\n",
    "        self._aggregate_labels()\n",
    "        # get the idxs for each type of dataset\n",
    "        dataset_names = [self.DS.data_vars[i].attrs['name'] for i in range(len(self.DS))]\n",
    "        unique_names = list(set(dataset_names))\n",
    "        idxs_by_name = {n: [i for i, name in enumerate(dataset_names) if name == n] for n in unique_names}        \n",
    "        for m in self.method_names: \n",
    "            if m != 'ks' and m != 'name':\n",
    "                for dname in unique_names:\n",
    "                    plt.hist(self.labels[m][idxs_by_name[dname]], bins=50, label = dname, edgecolor='none', linewidth=5)\n",
    "                plt.legend()\n",
    "                plt.xlabel(m)\n",
    "                plt.title(title)\n",
    "                plt.show()\n",
    "\n",
    "    def table(self, filter=None):\n",
    "        self.compute_metrics(filter=filter)\n",
    "        for k in self.metric_tables.keys():\n",
    "            print(k)\n",
    "            print(self.metric_tables[k])\n",
    "        return self.metric_tables\n",
    "\n",
    "    def _get_metrics(self):\n",
    "        tagged_functions = []\n",
    "        methods = [method for method in dir(self) if callable(getattr(self, method))]\n",
    "        for method_name in methods:\n",
    "            member = getattr(self, method_name)\n",
    "            if hasattr(member, 'tag') and getattr(member, 'tag') == 'metric':\n",
    "                tagged_functions.append(member)\n",
    "        return tagged_functions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a19ca58d-3748-42b1-9c36-e51b2f90c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SED = SelfEvaluatingDataset(\n",
    "    [1,2,3,4],\n",
    "    ['bob', 'dave', 'dan', 'ike'],\n",
    "    ['ks'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a9f062c-012f-467f-94c6-4b939e3a5d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SED._get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c761f2d0-80c5-4760-97ac-b827775c98b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ \u001b[1mPixi task (\u001b[0m\u001b[35m\u001b[1mdefault\u001b[0m\u001b[1m): \u001b[0m\u001b[34m\u001b[1mnbdev_export\u001b[0m\n",
      "\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                                 "
     ]
    }
   ],
   "source": [
    "# sync changes to the library\n",
    "from IPython.display import display, Javascript\n",
    "import time\n",
    "display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "time.sleep(2)\n",
    "!pixi run nbsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176fe26-3f53-4143-b231-4d0d89a4e8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zetteldev-diffcurv]",
   "language": "python",
   "name": "conda-env-zetteldev-diffcurv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
