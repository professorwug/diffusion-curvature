# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/library/core-pyg/3a-Diffusion-Rays.ipynb.

# %% auto 0
__all__ = ['DiffusionRays']

# %% ../../../nbs/library/core-pyg/3a-Diffusion-Rays.ipynb 4
from sklearn.preprocessing import normalize
import warnings

class DiffusionRays:
    """
    """

    def __init__(
        self,
        G, # Graph with W for affinity matrix
        t=25, # Scaling parameter for diffusion maps
        knn=5, # Number of rays to construct around each point, based in k nearest neighbors
        num_steps=20, # Num points within each ray #TODO: would be best to discern this dynamically
        percent_manifold_to_cover=0.3, # Size of each ray
        # n_evecs=10, #Dimension of diffusion map space
        # radius=0.3,
    ):
        self.t = t
        self.knn = knn
        self.num_steps = num_steps
        # self.percent_of_manifold_to_cover = percent_of_manifold_to_cover
        self.name = "Diffusion Ray Curvature"  # to be programmatically accessed by printing functions, e.g.
        self.radius = percent_manifold_to_cover
        # dmap = diffusion_map.DiffusionMap.from_sklearn(
        # epsilon=0.15, alpha=0.5, n_evecs=n_evecs,
        # )
        # self.diffusion_coordinates = 
        # dmap.fit_transform(X)
        self.graph = G
        self.A = G.W.toarray()
        self.num_points = len(self.A)
        D = np.diag(1 / np.sum(self.A, axis=1) ** 0.5)
        # # Compute symmetric diffusion operator
        # M = normalize(G.W, norm="l1", axis=1)
        self.Ms = D @ self.A @ D # TODO: can we keep things sparse for longer
        # # eigendecompose # TODO: DEMD already does eigendecomposition with fast algorithms. Can we reuse that?
        # # Create diffusion map and diffusion coordinates (basis of diffusion distance)
        print("Eigendecomposing diffusion matrix")
        self.E, self.V = np.linalg.eigh(self.Ms)
        # # correct eigenvecs of Ms to M
        self.V = D @ self.V
        # print("Building diffusion coordinates")
        self.diffusion_coordinates = self.V * (self.E ** self.t)

    def diffusion_distances_to(self, i):
        return np.linalg.norm(
            self.diffusion_coordinates
            - (
                np.ones_like(self.diffusion_coordinates)
                @ np.diag(self.diffusion_coordinates[i])
            ),
            axis=1,
        )

    def rays(self, i):
        """
        Returns indxs of diffusion rays around point i
        """
        # Find max diffusion distance from i
        distances_to_i = self.diffusion_distances_to(i)
        max_dist_to_i = np.max(distances_to_i)
        print(max_dist_to_i, 'max dist')
        # Convert this distance to a radius of inclusion
        radius = self.radius * max_dist_to_i
        print("radius", radius)
        # find k nearest neighbors for i.
        nn = np.argsort(distances_to_i)  # sorts the adjacency matrix
        knn = nn[1 : self.knn + 1]  # takes the k values with highest affinity
        # Simple proof of concept: can be heavily optimized
        # Loop through nearest neighbors and assemble rays for each
        rays = np.zeros((self.knn, self.num_steps, self.diffusion_coordinates.shape[1]))
        ray_coords = np.zeros((self.knn, self.num_steps), dtype=np.int)  # for debugging
        x = self.diffusion_coordinates[i]
        for m, k in enumerate(knn):
            # Loop through all points in the dataset and compute the distance from $p$ to the ray that passes through x and y
            point_dists = []
            y = self.diffusion_coordinates[k]
            normalized_ray_direction = (y - x) / np.linalg.norm(y - x)
            # Only consider coordinates within a specified radius from the central point
            points_within_radius = (distances_to_i <= radius).nonzero()[
                0
            ]  # gives the indices of points within the radius from i
            # TODO: This step could be heavily optimized
            print(points_within_radius.shape)
            for n, j in enumerate(points_within_radius):
                p = self.diffusion_coordinates[j]
                # length of the hypotenuse from x to p
                c2 = np.linalg.norm(p - x) ** 2
                # length of ray to p's projection onto y-x
                a2 = (np.dot(normalized_ray_direction, p - x)) ** 2
                # length of shortest side from p to closest point on ray y-x
                b2 = c2 - a2
                point_dists.append(b2)
            # take the closest num_step points
            # print("Sorted point dists",np.sort(point_dists))
            ray_coords[m] = points_within_radius[
                np.argsort(point_dists)[: self.num_steps]
            ]
            # print("have ray coords",ray_coords[m])
        return ray_coords

    def pointwise_curvature(self):
        # returns an [n_points] sized array of pointwise curvatures.
        # TODO: How can we speed up redundant distance calculations?
        # TODO: We probably don't need to compute the curvature of every point. Can we sample points, and then average the curvatures around them?
        curvatures = np.empty(self.num_points)
        for i in trange(self.num_points):
            curvatures[i] = self.curvature(i)[0]
        return curvatures
